[MUSIC] Welcome to Introduction to
Systematic Review of Meta-Analysis. I am Tianjing Li, Assistant Professor
in the Department of Epidemiology at Johns Hopkins Bloomberg School
of Public Health. I am here today with my colleagues Dr.
Kay Dickerson and Ms. Claire Twose. >> Hi, my name is Kay Dickerson, I'm professor of epidemiology at Johns
Hopkins Bloomberg School of Public Health. I'm also Director of the US
Conference Center here in Baltimore. I'd like to go over the objectives
of the course with you. We aim for three basic things. The first is that you learn the steps
that comprise a systematic review and meta-analysis. Second, we want you to be able to
read critically in the future, read the literature when you come across
a systematic review and meta-analysis. And finally we want you
to be able to integrate the results of systematic
reviews into the work you do. Whether you're in research or clinical
practice or you're just interested, for now, in systematic reviews and
meta-analysis. Hello.
>> My name is Claire Twose. I'm the Associate Director for
Informationist Services in Public Health and Basic Science at the Welch Medical
Library of Johns Hopkins University. I serve the Johns Hopkins Bloomberg School
of Public Health. I've been doing searching for
systematic reviews for over ten years and co-teaching with Doctors Dickerson and
Li in their courses here. In the middle modules, you're going to
see me giving you an introduction to the best practices for
doing searches for systematic reviews. And introducing you to the literature
about publication bias. Additionally, in those middle modules
you will be introduced to metabias and qualitative synthesis. >> In the first two modules, we will teach
you what a systematic review is, and why you would like to conduct one. And then we will show
you how to develop and refine a clinical question that can
be addressed by a systematic review.
[MUSIC] Hi, everybody. Welcome to the first lecture,
Introduction to Systematic Reviews. In this lecture, we're going to introduce you what is
the systematic review in meta analysis. And I hope you will take away
from this lecture that systematic review uses explicit methods to identify,
select, appraise and synthesize results from similar but
separate studies. Not all systematic reviews
would have a meta analysis. And meta-analysis is the statistical
method of analyzing a large collection of results
from individual studies. By the end of it, I will introduce you the
Cochrane Library which is the main product of the Cochrane collaboration and, is
the single best place to find independent, high quality evidence for
health care decision making. Let me introduce to you
the concept of systematic review. And why it's important that
we keep track of evidence and information, using systematic reviews. Information about
healthcare is everywhere. But how do you know if one healthcare
intervention works better than another or if it will do more harm than good? Here, I'm going to give you some
examples of a typical clinical question. For example,
you might be standing in a drugstore and wondering, are antioxidant
supplements effective for preventing mortality in healthy
participants like yourself? Do I need to go to annual checkups? Will that reduce illness and mortality? And for women
who are in labor,
is early epidural as effective and safe and late epidural? These are typical clinical questions
you may have in your daily life. But where will you look for
information to answer those questions? Well, first of all there's
too much information. Information is everywhere. You see them on newspapers, on Internet,
from your friends, from your relatives. You may hear from your neighbors
they're taking antioxidants. You may wonder why they are doing it and
should I take antioxidants as well. But are antioxidants effective at all? And this cartoon illustrates very
well the Science News Cycle. The research studies show
that there is a correlation, then it gets publicized through
different media channels. And your may hear from your grandparents
that, well maybe I should wear the hat to prevent A or
the units you're interested in. But how do you find
trustworthy information, how will you make that
healthcare decision? Let's come back to the example of
the timing of epidural for women in labor. A Cochrane systematic review has summarized information
from over 15,000 women. And those women were randomized
to early or late epidural groups. And they found that, when a woman is
in labor, the appropriate time to give an epidural during child
birth is when she asks for it. So the C-section rate which is
the outcome the authors look at does not differ between the two groups,
which is the early or the late group. An early epidurals made no difference to
the likelihood of needing an assisted birth involving forceps or suction, or to the amount of time spent in
the second pushing stage of labor. So we could rely on systematic reviews and find trustworthy answers from
these systematic reviews. Here is another example where you may
ask should I skip my annual physicals. Probably, that's agood idea. So from a health perspective, the annual
physical exam is basically worthless, shown in the Cochran Systematic Review. And this systematic review searched
the
words biomedical literature on this topic, and analyzed 14 randomized controlled
trials with over 182,000 people. And they found that the annual physicals
did not reduce morbidity or mortality. Neither the overraw morbidity or mortality
or cardiovascular or cancer courses. Although the number of new
diagnosis was increased, there are important harmful outcomes
that need to be considered as well. For example the number of followup
diagnostic procedures or short term, psychological effects. Those are important harm outcomes for
annual physical exam. In summary, if we're going to rely
on a Cochrane systematic review, then probably skipping your annual
physical is not a bad idea. Now we have to look at two
systematic reviews and the findings from these two
systematic reviews are ready. Let's talk about what
is a systematic review. A systematic review focuses
on a specific questions. That's were we started. Remember the question on
annual physical exam and on the epidural, early versus
late epidural for woman in labor. And a systematic review uses explicit,
pre-planned scientific methods to identify, select, appraise, and
summarize similar but separate studies. So reviews summarize knowledge, and here are the four steps that
differentiate a systematic review from a traditional narrative review which
is to identify all evidence on the topic, select them, appraise the quality of
these evidence and summarize them. Not all review articles
are systematic reviews. As I said, only a subset of them uses these three planned scientific
methods to summarize information. And the reviews that are not
systematic reviews are the traditional narrative reviews. Only a subset of systematic reviews
will include meta-analysis, which is statistical methods that
people can use to quantify and combine the results from
several independent studies. Let's take a look at what
a traditional narrative review is and how is it different from
a systematic review. Almost all of us started writing
about review articles very early on, probably in elementary school. And reviews are important because we
cannot read everything and digest them. There are more than 23,000 medical
journals and if you wan to keep up to date, you have to read about
90 journals, in full, every day. However, most of us were not taught
how to write review articles. So, we have double standards here. Try to remember how you
write your review articles. Not your systematic reviews, but
the narrative review articles. Probably you're going to the library and look for
few papers that support your hypothesis. So, in doing narrative reviews,
most people are highlighting and cherry picking what they like and
what fit their hypothesis. There is no standard format. There is no clearly specified methods
of identifying, selecting, and validating including information and
there's really quantitative synthesis to integrate
the information from multiple studies. Here is an early study that showed
how the traditional narrative reviews are different from systematic reviews and
meta analysis. Here the authors look at the medical
reviews published in the medical literature from the 1985 to 1986,
and then 1996, as well as reviews published on epidemiological
topics from the 1997 to 1999. They compared these three types of reviews
to meta-analysis, identifying 1996. If we look at the parameters, or the
domains, that the authors have evaluated, whether that review has addressed
a focused question, described methods for locating evidence, or
used explicit criteria to select studies, you will see that meta-analysis
which is shown on the last column have a higher likelihood of
addressing a focused question and having explicit methods for
selecting and locating evidence. So meta analysis,
95% of them has met that criteria, comparing to less than half
of all the other reviews. So that shows you a systematic review is
really different from a narrative review. It has to focus on a clinical question or
a research question and use pre-specified methods to
identify all evidence on the topic. Critically appraise the evidence and the
synthesized evidence in a coherent way. How do you do a systematic review? First of all, you have to
establish your research team and it's important to have content and
methods experts on your team. And then you're going to develop your
process of gathering stakeholder input; formulating your research question,
minimize bias and conflicts of interest. And then as the next step,
you would develop your protocol. In the protocol you're going to outline
the steps you're going to use for doing a systematic review. And this is very important step, because again it differentiate
a
systematic review from a narrative review. In other words,
we're using a very transparent and replicable process of
doing a systematic review. Like you would work on a bench signs or doing an experiment on animals,
you will have a protocol. Here, we will have a protocol for
doing systematic reviews. After you have your protocol written,
you can start your review process by collecting your data, locating
the studies, screening the results, and then abstract the data and appraise the
risk of bias in the individual studies. After that, which is now on step five,
you will synthesize your findings, interpret, and
assess the overall body of evidence. Your report writing was summarized
everything you have done so far and a good thing about systematic
reviews is that it can be updated. Meaning that, let's say a few more studies
published after your systematic review is done, a few years later down the road,
you could always update your systematic review based on
the same protocol that you have started. Again, systematic review is just
a way how we summarize evidence. And it is a transparent,
reproducible way of summarizing evidence. And the methods could be
applied to any field. You probably have heard of
evidence based healthcare, evidence based medicine,
public health policy. Apply it to your own field. It's not simply oriented to clinical
trials or clinical practices, but that said, most methods we're going
to learn come from clinical trials. That's where the methods come from. What is Evidence-Based Health Care?
Evidence-based health care
emphasize three components. It is integration of best research
evidence with clinical expertise and patient values. So it's evidence, clinical expertise and patient values and
the term was coined in 1991. Why is evidence-based
health care important? Let me use the United States
as an example. So the healthcare spending in the US,
we overspent. According to
the World Health Organization, the United States spent more on healthcare
per capital than any other country. Of every $1 GDP in the US,
$0.18 went to healthcare. And if we look at the data,
how are we're overspending health care, you can see that there's a whole bunch
of sources for unnecessary spending that range from inefficient services to
excess services and administrative cost. So actually about more than
half of this overspending can be saved if we use
evidence-based healthcare. If we get evidence-based
healthcare into play, we can save a lot of money in making
the healthcare better and more efficient. You probably have heard of
comparative effectiveness research. Comparative Effectiveness Research is a
new lingo for evidence based health care. It's taking one step further,
emphasizing not comparing an intervention against nothing but
compare modible interventions, maybe competing interventions
against each other. It was defined as the generation and
synthesis of evidence that compares the benefits and the harms of alternative
methods to prevent, diagnosis, treat, and monitor a clinical condition or
to improve the delivery of care. And the purpose of comparative effect in
this research is to assist the consumers, which are patients,
clinicians, purchasers, and policy makers to make informed decisions that will improve healthcare at both
the individual and population levels. The major founder for
comparative effectiveness research in the United States is called the patient
center outcome research institute, PCORI. PCORI is a non-profit,
non-governmental organization and the congress authorized the establishment
of PCORI in the patient protection and affordable care act of 2010. And the PCORI's mandate is
to improve the quality and relevance of evidence
available to help patients. Again, the caregivers,
clinicians, employers, insurers, policy makers to make
informed healthcare decision. As you can see, the comparative
effect in this research, and what PCORI is interested in funding,
and improve the quality and relevance of health care is really the practice of
evidence-based medicine and health care. And PCORI found comparative
effectiveness research. As well as support work that will improve
the methods used to conduct such research. So, how is PCORI different from
traditional ways of doing research? I think these summarize it very well. We think it starts by
listening to patients. That research agenda is driven by
what patients say is important. And patient-centered-outcomes
research is putting useful, practical information in the hands
of patients and their clinicians. Again, the emphasis on
the patient's values, on the clinical expertise and evidence. That ends our discussion
on systematic review. Let's move on to our next topic,
what is a meta-analysis?
So meta-analysis, a lot of people
equate it with systematic reviews, but it's actually different. Not all systematic reviews
would have a meta-analysis, and meta-analysis is really
a component of a systematic review where you have enough data to
combine them statistically. And here are two classic definitions for
systematic review. It is the statistical analysis of a large
collection of analysis results from individual studies for
the purpose of integrating the findings. Or alternatively, a statistical
analysis which combines the results of several independent studies considered
by the analyst to be combineable. Personally, I like the second definition
better because you have to decide, as a systematic reviewer,
whether the studies are similar enough, such that you can combine
them in your meta-analysis. Most meta-analysis
are presented in forest plot. And here is one example of a forest plot. Here we have five studies. And each line and the
square in the center
represent the results from one study. And the size of the square, is proportional to the weight that each
study is taken in the meta-analysis. The larger the square,
the more weight the study is taking. And we also have the two sticks
around the square which shows you the confidence interval for each study. If you look down on the plot,
you will see a diamond, a blue diamond. That's where the meta-analytical effect
lies and where the point estimate is, and the confidence interval for
the meta-analysis. Depending on the measure of
association you're going to use, you may have different scale on the xx. For example, here we're using risk ratio. A risk
ratio of one is the now effect. And you can label your figure such that,
if the diamond lies on the left of the line of no effect,
it favors the treatment. Or if the diamond lies to
the right-hand side of the line of no effect it favors the control. So you can actually show the direction
of effect on the same plot. This is called a forest plot, and
it shows you the meta-analysis results, as well as the results from individual
studies you put into your meta-analysis. Meta-analysis provides us
statistical methods for answering what is the direction
of effect or association? What is the size of effect? And is the effect
consistent across studies? You may also want to ask the question,
what is the strength of evidence for the effect. Assessment of the strength of evidence
relies additionally on the judgement of the study quality, study design, as well
as the statistical measure of uncertainty. Again, a general framework for
synthesis or for your meta-analysis is that you want to answer the question
of what is the direction of effect? What is the size of effect? And whether the effects
are consistent across studies. What Meta-Analysis Can Help you to Do? If you have several studies included
in your systematic review, and they're similar enough, when you put
them together in a meta-analysis, you can determine whether an effect
exists in a particular direction. It helps you to combine
the results quantitatively and obtain a single summary result which is
shown as a diamond on the forest plot. You can also use systematic
reviews to invest heterogeneity, to examine reasons for
different results among studies. Again, very unlikely you will get
identical results from studies on a research question. But you will be able to look at why they
are different using meta-analysis and related methods. As I mentioned earlier, I really like the
definition where meta-analysis is defined as by the analysts of whether the studies
are combinable, and here are why. The justifications for combining results. You have to decide whether studies
are estimating in whole, or in part a common effect. This is very important, because as I said, very unlikely you will
have
two identical studies. They are similar in some way, and you have
to decide if they are similar enough. When you decide to combine
the studies together, hopefully the studies are addressing the
same fundamental, biological, clinical or mechanistic question. Let's look at one example. The research question is what
is
the effect of interferon therapy in Hepatitis C? In the size of the effect
might be higher or lower when the participants are older,
more educated or healthier than others. So for some of your studies, the studies might be conducted in older participants
while others in younger participants. Some conducted in North America,
and others in Africa. There are different forms of interferon,
as well as different concentrations, dosages, and usages. And there are also different
viral sub types of Hepatitis C. So when you have a handful of
studies that address this question, what is the effect of interferon
therapy in Hepatitis C, there will be characteristics that
the studies differ from each other. There could be different designs,
different participant characteristics, different intervention characteristics,
and even different outcomes and you have to decide whether these studies
are addressing the same question. When we decide to incorporate a group
of studies in the meta-analysis, we assume that the studies
have enough in common that it makes sense to
synthesize information. When it doesn't make sense
to synthesize information, you don't have to do a meta analysis. So when to do a meta analysis? When more than one
study has estimated
a treatment effect or association. That means when you have two or more studies that have estimated the same
treatment effect or association. And when the differences in study
characteristics are unlikely to affect the treatment effect. Again, as I said,
if the study characteristics, let's say, studies conducted in older participants
are likely to affect the treatment effect ,saying that the treatment
is less effective in older adults. Then perhaps it's not a good
idea to combine the studies done in older adults with the studies
done in younger adults. When the treatment effect
has been measured and reported in similar ways, that says
that when you have data available, that's when you could do a meta-analysis. There are also cases that you do
not want to do a meta-analysis. For example, if the information presented in individual
study is really not that great. So a meta-analysis is only as
good as the studies in it. You may get a very precise estimate by
combining all the studies together. But the combined estimate is worse
than the bias study on it's own. You have to be aware of reporting biases. That means the published information may
be different from what's all out there, including those unpublished information. You don't want to mix apples with
oranges,
although it's not useful for learning about apples,
it's useful for learning about fruit. Again the studies must address the same
question, though the question can and usually must be broader. So when you do a meta-analysis
on a set of studies, unlikely that all the studies
will be identical. They will be similar. And as a data analyst, you have to
decide how similar is similar, and how different is different, and
when to do a meta-analysis. I'm going to end this section by
showing you a very good example of the importance of synthesizing
what we know in an ongoing fashion. And this is a classic example. And there are many more
examples that like this. And this is is a cumulative meta-analysis. So here, each line and
the circle on the plot is no longer representing
the results from one single study. Each line in the circle
represents a meta-analysis. And here we have a series of meta
analysis and they were done cumulatively. And this show us how important it
is we synthesize what we know in ongoing fashion. And the topic is now thrombolytic
therapy in preventing death, in people who have already
had a heart attack. And the way you're going to read this
plot, or the accumulated meta-analysis, is like we're adding or
throwing a study into the pot. If we had been keeping
track of the information, what the meta-analysis results would show,
by each time point. So let's take a look. The first study on the thrombolytic
therapy in preventing death in people who already had a heart attack,
was done in the early 1960s. And the first randomized control
trial included 23 participants. It has a point estimate around 0.5. Odds ratio 0.5, favors the treatment. Although the
confidence interval is very
wide, crossing the null value of one. And a second analysis was done,
again in the early 1960s, so the second study was
added to the first study. So combining the two studies together,
there were 65 participants. As you can see,
the point estimate stays about the same, although the confidence
interval gets tighter. So, people kept doing
the same experiment or randomized control trials over and
over again. By the early 1970s, by the time we
had ten randomized control trials, 2544 participants were
randomized in those ten trials. It shows that thrombolytic
therapy was effective. And the confidence interval, the upper bound of the confidence
interval, does not cross the null value. And the P value is less than 0.1. So by the early 1970s,
if we were keeping track of the evidence, we would have known that
thrombolytic therapy is effective. Yet, no one were keeping track of
the information or the evidence. Investigators kept randomizing patients
to thrombolytic therapy versus placebo, or nothing. And look how many more
patients were randomized. So by the 1990s,
70 randomized control trials were done with over 48,000 participants. If we were keeping track
of the information, we would have known the answer
by the early of the 70s. Yet we randomized another
45,000 participants, telling patients, well, we don't know
whether thrombolytic therapy is effective. That's why we're randomizing you to
thrombolytic therapy or placebo. Imagining the patients who are randomized
to placebo therapy, those patients were wasted, because we would have known
the answer as early as the 70's. So if you look at all of these estimates, the effect size stays the same,
it's just getting more precise. It wasn't until meta-analysis
was done that thrombotic therapy was mentioned
beneficially in a textbook. So on the right-hand side of this
slide you see a grade which shows that whether the textbook or review articles
recommended thrombolytic therapy. It was not until med analysis was done that thrombolytic therapy was
mentioned in the textbook. So this example shows
you how behind we were, in terms of not keeping track of
the evidence, and the potential harm that could happen to our patients if
we were not keeping track of our evidence. Where does systematic review, or evidence synthesis fit in in
the knowledge translation process? We have the evidence generation, which is the clinical trial
observational of studies. Those are the primary research. And then systematic reviews
summarize those information. And the systematic reviews,
are then feeding to clinical policies. For example, into the practice guidelines. When the evidence is integrated
with clinical expertise, and patient value, that's how we
practice evidence based health care. We talked about what is a systematic
review in meta-analysis. And by using a lot of examples, I hope I
have convinced you that it's important to synthesize information
in an ongoing fashion. It is also important that the way
your synthesize information follows a prespecified methods, and the methods
are transparent, are reproducible and will help you to reach valued conclusions.
Now let's move on to our next session. We're going to talk about producers and
users of systematic reviews. Who is doing systematic reviews? More than 60% of the systematic reviews
are done by independent authors, independent investigators, researches, clinicians, who are interested
in a particular topic. And the Cochrane Collaboration produces
less that 40% of the systematic reviews, and it's the largest international
organization that does systematic reviews. They're also grouped interested in policy. For example, the professional
societies,
governments payers. They usually can't check out
their systematic reviews. For example, in the United States,
the Agency for Healthcare and Research and
Quality Evidence-based Practice Centers. They do a lot of systematic
reviews in the U.S. In the U.K. the National Institute for Health and
Care Excellence, the NIHCE. They sponsor a lot of systematic reviews
to support their practice guidelines. And similarly,
there are institutes in Germany, and other countries,
that do systematic reviews. Business does systematic reviews, too. For example, Hayes and
ECRI are the two companies that have a lot of contract from the pharmaceutical
companies on systematic reviews. Who are using systematic reviews? Well, all types of decision makers. There could
be individual doctors and the researchers, patients and
consumers, guidelines producers. Policy makers, for example, purchasers,
payers, and regulatory authorities. You may have seen the pyramid
of the level of evidence. On top of the pyramid
is systematic reviews. And for clinical questions on
the intervention effectiveness, systematic reviews of
randomized control trials are usually regarded as
the highest level of evidence. And why is that? Here are the features
of a systematic review. A systematic review facilitates
efficient integration of information for rational decision making. It provides a clear and
transparent process, and that process is documented in
the protocol of your systematic review. It helps to demonstrate where the effects
of healthcare are consistent, and where they vary. Again, that relates back to the idea of
whether the effect size are homogeneous or heterogeneous. Because you're using a transparent and
clear process, you minimize the bias and
systematic errors in summarizing evidence. Meta-analysis can provide more precise
estimates than individual studies. And systematic reviews can be readily
updated as needed with new information, new evidence, coming to the literature. And they allow decisions based on
the totality of the available evidence. Having said a lot about what systematic
review and meta-analysis are. And here are some useful references,
of recommended readings for you, to learn about how to do it. The first book is
the Finding What Works in Health Care. It's put together by
the Institute of Medicine. And they outline the standards for
doing systematic reviews. And this book is freely available online. And you can Google the book name and
you will find it. Another useful reference is
the Cochrane Handbook for Systematic Reviews of Interventions. Again, this book is
freely available online. And it's the handbook used by
the Cochrane Review authors and recommended by the Cochrane Collaboration. Cochrane is a global independent
network of researchers, professionals, patients, caregivers and
people interested in healthcare. And the Cochrane collaboration
produces systematic reviews. As I said, it's the largest organization, international organization,
that produces systematic reviews. And they study all of the best available
evidence generated through research, and make it easier to inform
decisions about health. It is a not for profit organization with collaborations
from more than 120 countries. Working together to produce credible,
accessible health information that is free from commercial sponsorship
and other conflicts of interest. So if you Google Cochrane and it will take you to
the Cochrane Collaboration's home page. You may have recognized already that
the Cochrane collaboration's logo looks like a forest plot. Which we talked about early
on that a meta analysis is typically showing a forest plot. The circle formed by two C shapes
represents our global collaboration. And then in the center of the circle,
you see a forest plot. This forest plot when in our logo illustrates an example of
the potential for systematic reviews. To improve health care and it shows
that corticosteroids given to women who are about to give birth prematurely,
can save the life of the new born child. So this is one of our early
systematic reviews and was influential in increasing
the use of this treatment. And probably this single systematic review has saved thousands of babies
lives around the world. Again, the Cochrane Collaboration
prepares, maintains, and promotes the accessibility of systematic reviews of
the effects of healthcare interventions. There are 14 centers around the world. In the United States Cochrane Center,
is one of the fourteen centers that helps to facilitate the work
of the Cochrane collaboration. And U.S. Cochrane Center is located at Johns
Hopkins Bloomberg School of Public Health. In the U.S. Cochrane Center's dedicated to promoting
awareness of the Cochrane collaboration. And its objectives and access to
the Cochrane library in the United States. The Cochrane Library is the main
product of the Cochrane collaboration. It's published daily by John Wiley and it contains by issue 4 of 2015
Cochrane's systematic reviews. There are over 6,000 completed
Cochrane Systematic Reviews. It also has a collection of non
Cochrane Systematic Reviews, and there are over 36,000 of them. One of the most accessed of
resources of the Cochrane library is the central registrar
of controlled trials. And all the controlled trials
gathered from medline, and these, and other databases were gathered together,
and there are over 848,000 of them. And if you're browsing the Cochran Library
there are many useful resources, including the reviews, the trials,
editorials, and highlights. And I would encourage
you to take a look and see if there are reviews answering
your own research question. The key messages for
today include the following. I hope you will be able to describe what
is a systematic review and meta-analysis. Again, systematic review uses
explicit methods to identify, select, appraise, and synthesize results
from similar but separate studies. And meta-analysis refers to the statistic
analysis of the results from individual studies, and is an optional
component of a systematic review. The Cochrane Library is the main
product of the Cochrane collaboration. And is the single best place to find
independent, high quality evidence for health care decision making. Thank you for listening, we will continue
our discussion, we will talk about where to start, which is how to
formulate your research question. Thank you very much. [MUSIC]
When I was a newly minted PhD I
had two very small children and I was looking for a job where I
would not be under-utilized but that wasn't a full time job and
I found a job with the US Office Of Personnel Management
that let me work two days a week. I was originally hired to
develop selection tests. But there was a fellow there
named Franz Schmidt who was busy developing the Schmidt-Hunter
form of meta-analysis and it was so much more interesting
than the work I'd been hired to do. I figured out a way to get
into that shop instead and I just fell in love with
meta-analysis because of its power, both statistically and
in terms of theory building. And because it was an opportunity to
change a paradigm, so that was just fun. Well, I think that people who
are interested in science and who are interested in evidence really
need to work together to make sure that science is taken into account in policy. If it's not going to be taken into
account by the top government officials, then we have to find other
policymakers who we can lobby. And I do think that the results
are evident syntheses, are the most important form of scientific
evidence that we have and that's what we should be using when we talk to
anybody in a policy making position.
[MUSIC] Hi this is Kay Dickersin and
its good to see you again. In this lecture we'll be covering framing
the question that your going to address in your systematic review. This is a very important topic
because if we get the question right, everything else will follow. We're going to start with section A,
and talk about resources that you can use when you're trying to decide
how to frame your question. You'll work as a group to
frame your question and it will take you at least one
session to get it right, maybe more. But there are resources you can use. That is books and
other resources to help you along the way. Let's look again at
the steps of the systematic review that we talked about last time. Our very first thing that we need to do,
after we form our group, is frame the research question
we're going to address. And I'll talk about how that's
described as PICO, P-I-C-O, or P-E-C-O, depending on whether you're
looking at an intervention question. Usually a clinical Type intervention,
but it could be behavioral or an ideology question such as we
would look at in epidemiology.. Some of the resources that you can
access follow, the first is this book. You can also get it for free online
called, Finding What Works in Healthcare. It came out in about 2010. And it's put out by
the Institute of Medicine. And it's what's called standards for
doing a systematic review. Here's an example of the second sample
which is initiating a systematic review. In this standard, and I'm not going to go through all
the details that's on this slide. You can either read the book
which is online for free. You can download it or
you could purchase it. A few copies are in the bookstore and
we have a copy on reserve in the library. The first step is, establish your team,
which is what we are doing right now. The second is to manage the bias and
the conflict of interest within your team. And ensure that you
have stakeholder input. Who are the important people
who might have input into the issues that you're discussing? It might be an environmental
epidemiologist, if you're looking at
that type of question. Or it might be a clinician. It would probably definitely
include a consumer or a patient. And then you have to decide what
to do about managing that bias and conflict of interest. If people do have a potential conflict of
interest, then you need to ask about it. How do you actually manage it from
those who are providing the input? And then finally you would get to
what I'm going to talk about today. Which is formulating the topic or
framing the question. In the IOM book on how to
do a systematic review there's a little more detail
on formulating the topic. This includes confirming that
you need a new review and that one hasn't already
been done very recently. Either done recently or updated and
therefore you don't need to do a new one. You need to develop an analytic
framework if you're going to do that. And we'll talk about that briefly
at the end of this lecture. It's an optional part of our course. But it is one of the IOM standards. aYou need to use a
standard format
to articulate each question. And we will spend quite
a bit of time on how to use the PICO format to
articulate your question. State your rationale for each question,
and then go back and forth and refine it. And that's when you'll see,
working with your group, just how long it takes to come to a final
question that you are going to address. And to be honest, this is something we work on throughout
the class with the small groups. Because once you find literature, you will
see some refinement is probably needed. Another resource for refining your
question or framing your question, is the Cochrane handbook For
Systematic Reviews of Interventions. Again, you can get this for
free online, or you can purchase a hardcopy of the book. I have to confess, I like books,
and so I have purchased one. You can get them at the bookstore,
we have some copies for sale there. Or you can get it at
the library on reserve. Or you can download it chapter by chapter,
the way it shows here. Browse the handbook online. And you can go in chapter by chapter,
depending on what you need. So, let's say you decide
to look at the handbook. This is version 1.1 which, I just checked,
is the latest version that's up online. It's from 2011. And defining the review question
is found in chapter five. And you can see there separate
sections under it for how to define the question and
the eligibility criteria. What type of participants will you have, what type of interventions
will you have and so forth. And we'll go through all of these
topics in a little more detail. But I am just showing you here about
the resource that you can use. If you want to go back to
the Cochrane handbook to look to see about framing your question. You do not need to use
both of these sources. It's just, if you've chosen one to be
your major resource for this class. Here's where you find
the information you need. And then finally in chapter five,
there are some criteria and key points. That you might want to take
into consideration when your defining your question that
your asking with framing it. So for example, I'm not going to go
through this in a lot of detail. You start in the key points
with a well framed question. You need to specify the PICO. And with a Cochrane handbook,
there's a focus on interventions. But in this class,
we're also focusing on etiology, or environmental, in born
characteristics of people. So that would be peco, as in P-E-C-O. There is a focus within
the Cochrane views on outcomes, and we're going to spend some time
on outcomes in this class. Because this is an emerging
area in systematic reviews and you'll hear us talk about it a lot more. Than maybe we have in past years because
people have realized just how important a well-defined outcome is. So that's it for section A,
talking about resources you'll use for framing your question. Our next section will be section B,
where we decide the type and scope of our question.
Welcome back. This is Kay Dickerson and Section B of
the lecture on framing the question for the Systematic Reviews and
Meta-analysis course. Section B covers deciding the type and scope of the question that you're going to
be addressing in your systematic review. This is a very important part
of your systematic review, but I have to confess it's probably the one
where people have the most problems. And so, maybe I shouldn't tell you that
ahead of time, because I don't want you to be frightened, but I want to say you
really need to pay attention to this. It's not as easy as it looks. So the first thing I'm going to tell you
about is that there all different kinds of questions we have about health care,
about epidemiology, about interventions. And they are all different types
of questions as it turns out, and each type of question requires
a different type of research. And don't worry, I'll get into that. But let's start first by just thinking
about the different types of questions we might have. So you might say, what proportion of the population is newly
diagnosed with this problem each year. That's whats called an incidence question,
and there's a certain type of study design that you would use if you
had a incidence question. And that means when you're
looking at the literature and doing a systematic review, you don't
just take any old paper, or article, or research that says that
it's looking at incidents. You want to make sure it's the type of
design that actually can address properly, with minimum bias and incidence question. You might have a prevalence
question. What proportion of the population is
currently living with this problem? Both of those are standard
epidemiology questions. The questions that kind of
belong together are therapy, screening and prevention,
and sometimes harm. Those are all intervention questions. So a therapy question is,
what should be done to treat this problem? A screening question is,
will detecting this problem early before I get symptoms make
a difference in my health? And health is the outcome there for
a screening question. Another type of question that's like that
because it's an intervention question, is how can this problem be prevented? That's at the bottom of the slide, but
that's also an intervention type question. You might have a question, how good is
this test at detecting this problem? And that's a diagnostic accuracy question. That's a different type of question
from a screening question. So the screening question is saying, will detecting this problem early
make a difference in my health? So health is an outcome. In a diagnostic accuracy question,
you're just comparing one test to another, and seeing how good that test
is at detecting the problem. So we're looking at things like
sensitivity and specificity. A prognosis question is a type of question
that doctors are interested in but also people in public health. What is the likely
outcome of this problem? What's likely if I have
a baby two months early, what's the likely outcome for that baby? A harm question is, will there be any
negative effects of this intervention? And so often when we're looking at
the effectiveness of a therapy, we also want to look at harms, and
the possible safety questions. And then finally, an ideology question
which you see in epidemiology all the time, what causes this problem? If I have glaucoma, is it because I
have a family history of glaucoma? Or how likely is it that
the family history contributed? So think about your question and
then try classifying it. Because it's not until you classify it
that you actually can decide what type of research is the best research
to examine the question. Let's look at some examples. I've already given you a few. But here they are written down so
you can think about them in that context. Incidence and prevalence. Here are some examples. What's the incidence of
low birth weight
of minority populations compared to the white population? A therapy question. Is exercise effective in improving
quality of life in persons with COPD? A screening question. Is PSA to detect prostate
cancer effective? And that means effective
in terms of saving lives or some sort of health outcome. In this case, we've said is it
effective in reducing mortality? Diagnostic accuracy. How effective is MRI at
detecting new breast cancers and follow up of women with breast
cancer who had lumpectomy. Here's a prognosis question. What is the effect of pregnancy
on exacerbating the symptoms of multiple sclerosis. Here's a harm question. What proportion of post menopausal
women receiving calcium and vitamin D can expect
to have kidney stones? That's a common side effect
of this intervention. And etiology, is coffee consumption casually associated
with developing pancreatic cancer? Probably not, I'll say as an aside, but we're always worried
about coffee consumption and I'm happy to tell you that so
far as we know, it's not that bad for us. So now you can see there
are different types of questions and the type of things that are being
addressed with those types of questions. Let's look now at the study designs that
you will use to study those questions. So if we have an incidence question or a prevalence question, we look for
surveys or cohort studies. If we have a therapy or
a screening question, we're looking for clinical trials,
randomized clinical trials. Because it's only be comparing one
intervention to another that we'll be able to tell if they have similar outcomes,
or if one is superior to the other. For questions of diagnostic accuracy, we would love to have randomized clinical
trials, but you hardly ever find them. More likely you'll find
cross sectional studies. And we say okay, it would be great
if those studies were randomized or had a random start but we rarely see them. For prognosis, most commonly
you'll see a cohort study but a clinical trial can also
tell you about prognosis. That would be great. For harm, also,
clinical trials will be wonderful. Randomized clinical trials. But usually, they aren't big enough
to be able to detect rare outcomes. Nor are the conducted long enough. And so, we have to look, instead, at cohort
studies or case control
studies when we have questions of harm. Etiology or an epidemiology type question. We typically use observational
studies,
cohort studies, or case control studies. So now you see why it's so necessary that we classify the question
before we begin our systematic review. That's because each type of question
requires a different type of study to minimize bias, or
at least that's what we would expect. So if we have a question about
how well an intervention works, we probably won't look at case
control studies because they have too high a likelihood of bias
associated with them. Instead, we'll look for
randomized clinical trials. However, when it comes to looking at harm,
we probably will have to turn to case control or cohort studies because
the randomized clinical trials just either aren't long enough or big
enough to be able to deduct harm reliably. Now many of you have probably seen this,
what's called levels of evidence, or pyramid of evidence. It's called a Hierarchy of Evidence. And often if I'm asked to
talk to a group,
they'll say, oh please, would you review the Hierarchy of Evidence
with us, because we want to understand it. The first thing I want to emphasize and most importantly is this Hierarchy
of Evidence really only works for those intervention questions
that I mentioned before. Questions of therapy, questions of harm, although I want to put a caveat on there
because we know from the beginning that we are unlikely to find this type of
study for harm, intervention studies, studies of screening, and
also studies of prevention. So now we can look at this hierarchy of
evidence, which doesn't work so well when we're taking about studies of etiology,
prognosis, incidents or prevalence. So if you look at this
hierarchy of evidence, what you can see is that at the top
the highest form of evidence is a systematic review of
randomized clinical trials. There aren't as many of these as there are
unsystematic clinical observations, but they are the highest form of evidence for
determining whether an intervention works, whether it's treatment, prevention,
screening, or detecting harm. A single randomized trial, that is,
where there is fewer than two randomized trials in a systematic review
available is better than a systematic review of observational studies
in terms of minimizing bias. But if you have to, you may be
turning to observational studies. For example, as I mentioned for
detecting harm. And so you can see what this
hierarchy of evidence represents, is that the studies you find least often,
in most cases the systematic reviews of randomized trials,
are also the highest form of evidence. What you'll find a lot of
is very low evidence, and evidence we probably would not
consider in a typical situation for giving standard of care treatment based
on just that low form of evidence. There are exceptions, however,
in very rare diseases, where it's a desperate situation, but
we're not going to talk about that here. So John Tukey, who was a well known
statistician at Princeton said this many years ago, in the 1960s, and
I think it's a really great maxim for us who are doing systematic reviews. The most important maxim for
data analysis to heed, and one which many statisticians have shunned,
is this. Far better an approximate answer to the
right question, which is often vague, than an exact answer to the wrong question,
which can always be made precise. Now, this is really a good beginning for
us to talk about how to formulate that question that we're
going to try to address in this class. That is, in doing systematic reviews, people tend to ask pretty broad
questions rather than very narrow ones. I'll give an example. One might ask whether it prevents another heart attack if
you take
aspirin after your first heart attack. So a very broad question might be, is secondary prevention of heart
attack with aspirin effective? Now, one might say well when would
you start taking that Aspirin? Should I start taking it as soon
as I have the first heart attack? What if I start taking it ten years later? How much Aspirin should I take? Should I take a
baby Aspirin every day? Should I take two Aspirin
every four hours? What's the right amount? And for how long should I take it? Do I take it the rest of my life? Do I take
it a short amount of time? So you can see that that question that
I mentioned, is Aspirin effective in secondary prevention of heart attack,
can be made very precise. But the trouble is,
when you make that question very precise, there are going to be fewer and fewer
studies that can address the question. And you'll be making it so precise that, in fact, maybe there
are no studies that answer the question. And in fact, maybe it's okay if
the question was very broad. Maybe any amount of Aspirin is helpful. Maybe taking it anytime after that
first heat attack is helpful. Maybe both men and
women can be helped by taking Aspirin. So, you want to be careful when you
narrow your question down a lot, because maybe that's not how
systematic reviews are best conducted. Now studies that are out there,
and we've already touched on this, can differ in many, many different ways. The types of population that
they're studying, the inclusion and exclusion criteria,
if we use the heart attack example. Maybe some studies say you can't have had
diabetes, and other studies say you can. Maybe some say you can only
have had one heart attack. Some say you can have had infinite number. So they can differ in all kinds of things. How
you define how much Aspirin is taken. What the comparison group is. Is it taking no aspirin or
maybe a vitamin pill? What you do if people want to take
other types of drugs, for example. That might help prevent heart attack. How you define the definition? Well did you
measure heart attack at
2 years, at 4 years, at 10 years, at 20 years? How did you define heart attack? And finally the quality
of the study design and how the study was conducted,
how the analysis was conducted. Whether missing data influenced
that analysis and how it was done. So there are all different ways similar
studies can differ and this effects how you define your systematic review and
what your question is. I am not going to go into a lot of detail
here on ways that studies can differ, you're about to find it out in
your own systematic reviews. However, I've already touched on the fact
that you're going to make your question and you have to decide whether
it's a broad or a narrow question. The trouble with narrow questions, is that they may not be applicable
to multiple groups or populations. So if I say I'm only going to look
at Aspirin studies to prevent heart attacks in men ages 35 to 40, whatever results I get, they're only
going to apply to men 35 to 40. And that may not be the question
you really have in mind. You can also get spurious findings
because you'll have fewer studies. And that can be a problem when
you have a smaller sample size. And there are many examples of spurious
findings in situations like that. For example, the efficacy of Aspirin
in preventing strokes in women. There was an association
that was seen incorrectly between dysfunctional uterine bleeding and
BMI, in African American women. And this is something that
just ended up not true, because the questions were too narrow. If you want to learn more about, or
see a comparison of the pros and cons of broad and narrow questions,
I'll refer you to the Cochrane Handbook table 5.6.a and
you can look at extensive tables there. Here we're just going to
talk about in general terms. What's the downside or the upside
of looking at narrow questions or broad questions? When you have very broad questions,
such as is Aspirin effective in preventing a second heart attack, you might get, as I
mentioned, all different kinds of studies. And you have the criticism I'm sure you've
heard before that systematic reviews and meta-analysis compare apples and oranges. And this can happen. So, you're
goal is really to compare
different kinds of apples not apples and oranges, because that really doesn't work. So you have to make the decision
whether
men and women are similar enough that you're going to include them both in
your systematic review and meta-analysis. I would tend to do it, and then if you
think there's likely to be a difference you might plan ahead of time
to do a sub-group analysis. But I think men and
women are probably similar enough that in the example I gave you,
you could use both of them. However, the dose might be something
where you would draw the line, or the timing since that first heart attack. So you really have to decide how much
of a difference it makes in terms of the validity of the answer
that you're likely to get. Another problem with broad questions is,
how do you search the literature? It could take you forever because
you're likely to find a large number of hits when you do searches
of multiple databases. Now that's just real life and that's one of the side effects
of a systematic review. But you do have to think
about that ahead of time. Certainly if you find more studies,
it'll make your synthesis more difficult. But again,
you might be willing to take that on, because that's the question
people really have. So again, if you want to compare the pros
and cons of broad or narrow questions, do look at the Cochrane Handbook, where
there's a very nice table doing this. And we'll just talk about
it in generalities here. So that finishes up section B. And we'll move on next to Section C,
Elements of the Question.
Welcome back to section C. In this section, we're going to talk
about elements of the question. We've been talking about how to
classify your question so that you know what type of study design you're going
to be using in your systematic review. And then we talked about thinking about is
it a broad question or narrow question? How do I want to frame it exactly? And now we're going to talk in little bit
more detail about that framing process. What are the elements of the question? And by that I mean, what types
of patients are we interested in including?What types of interventions or
comparisons do we want to make? And what outcome are we interested in? So, here,
we're talking about the steps and the practice of
evidence-based health care. So, if you're providing health care
as a health care practitioner or as a public health worker,
it's not enough just to do the research or to synthesize the existing research. You have to take the next step and
actually implement what you find. So the first step is
to frame the question. First we have to have a question. Then we have to find the best evidence
that supports the answer to that question. You want to critically appraise it, and
then you want to apply that evidence. Either make a new policy, or encourage people not to do what they're
doing already, or something like that. And so that's a very important
part of this course. We'll teach you how to do
the systematic review, but unless somebody's applying that evidence,
it does no good to do all this research. Well, we're just going to be
focusing on number one here, framing the question, but if you don't get
this right, you can't apply the evidence. So again, I emphasize how
important it is that we get this part of the whole process done right. So when you frame the question,
you consider the patients and the interventions and
comparisons outcomes you want to address. You want to classify the question, and you want to then identify the
appropriate
study designs for addressing the question. And we've talked about those things so
far. Here's a little more
detail on those things. Figure out what the question is, so
do it in a very, sort of broad way. Sit with your group. Talk about in very sort of. You didn't realize it before this,
but in very broad terms. And now you realize that the question
is actually harder to develop than just that quick one sentence
that you're throwing out. But then think, well,
what type of question is this? And so what type of evidence will we
look for to address that question? So here's an example. We have a patient who has amblyopia. And you're a doctor, and
that's basically with a lazy eye. It's usually a child. Sometimes it can be an adult,
and they have a lazy eye. And let's say that patient is coming to
you, or the parent's bringing the child to you, or you've seen this detected in
a school screening, something like that. You as a health care provider might have
many questions going on in your head. What's the cause of this condition? Were there environmental exposures? Is it
because of the parents and
something the kid inherited? What's the likelihood this
amblyopia's going to get worse? What if I do nothing? Maybe it'll get better on its own. What if we put a patch over the
eye that's
functioning better than the lazy eye? Will that get the lazy
eye functioning better if the person is dependent
only on that lazy eye? That's a therapy type question. Maybe there are harms of patching,
and in fact there are. We know there are harms of patching. If you're a child, and
you're wearing a patch on your good eye. Let's call it the good eye. It's not the lazy eye. You might get teased on the
schoolyard. That doesn't sound like a harm maybe
to those of us who are adults, but when you're a kid,
getting teased is really a bad harm. And you don't want that
to happen to your child. So, that is a potential harm, and
you have to think about, well, maybe we should put skeletons on the patch
or something that makes it look cool. And not just like a Band-aid. So, what potential differences in
outcome are there if a patient's treated earlier rather than later? Should I just wait for this kid to see
if the lazy eye will fix on its own? Should we do screening in preschool
to try to catch it much earlier so that we have better outcomes over
a larger proportion of the kids? So, that's both a screening study and
a prognosis study. You want to know what happens if you
don't patch, what happens naturally, a natural history type study. And then also you might
have a screening question. If I catch this earlier, if I start doing
preschool screening to catch amblyopia early, will this make a difference
in the outcome for the children? Will more kids be helped? And what kind of change am I looking for overall once we
decide that
treatment's necessary? What type of outcome am I looking for? Can these kids be 100% cured, or am I just looking for
sort of
a middle of the road half lazy eye? What am I expecting? And this encompasses several questions,
but it might be the question that you
would have as a parent or a doctor. So, as you can see there are a lot of
questions in there, and you have to decide which question you're trying to
address in your systematic review. You can only address one at a time because
they require different types of study designs. So once you have your well-formulated
question, and we aren't quite there yet, but we're starting to narrow it down,
then this can be used for your whole systematic review process. It will help you determine which criteria
you want to select your studies, your eligibility criteria. It will help you to develop your
search strategy when you search bibliographic databases when
you do hand searching, and it will tell you what types of
data do you need to abstract. So, you're not just going to find studies
that are relevant to your question. You're going to be abstracting data so that you can combine the data
in what we call a meta-analysis. All right, so what are the components
of a well-constructed and answerable clinical question? Remember, we want something answerable. And I've mentioned
before PICO. People, interventions or exposures,
comparisons and outcomes. Here it is. Patients or populations. Interventions or exposures. Comparison groups and
Outcome. And that spells PICO. And it could be P-E-C-O if
we're talking Exposure. I never knew whether to pronounce
it pie-co or pee-co, but since we're talking about
epidemiology type studies and exposure as well as intervention,
I think pee-co is probably the better pronunciation since it encompasses
many different types of studies. And I'll just mention here that you can do
a systematic review of any type of study. Systematic reviews aren't only of
randomized trials or intervention studies. A systematic review is for
any type of question you come up with. However, there will probably be
a limitation on which study designs you'll include based on the type
question you're asking. So, let's talk about the types of people,
or populations, or patients. What we mean is, we wanted to find
the condition or the disease that we're interested in, and we want to include
explicit diagnostic criteria. So if we are looking at
people who had heart attack, how do we define heart attack? Often what you'll see is that systematic
reviewers define their population or their disease or
condition the way the authors of the original studies that they include
in their review have defined it. So, if a study defines heart attack
using specific criteria from, let's say, I'm making this up, New York, and you might define
a heart attack slightly differently. Say in a different time period. That may be all right if you as
the systematic reviewer say, however the authors defined heart attack,
we're considering that okay. Or you might say, we're only including studies that
defined heart attack this way. So you have to decide again whether
to be broad and more inclusive or whether to really define your disease or
condition in a specific way so that your audience and the question you actually
want addressed are properly addressed. You would probably say at this point what
type of setting you're interested in, although some people separate out setting. So, are you looking at people
who are based in the community? Are you looking at hospitalized people? Are you just looking at one age group,
one race or ethnicity, sex, outpatients, and so forth? So again, if we're talking about
secondary prevention of heart attack, do you want people who are still in
the hospital when they're given aspirin, or they could be community dwelling? Are you going to accept people who
are in nursing homes or intensive care? So, what setting and
population are you interested in? When we think about the exposure or
interventions, we're also thinking about risk factors. That is, what's the timing of
the exposure or the intervention? How long after that first heart attack,
for example. What route of administration? What dose are we talking about? And how long should people be treated or
exposed? So, for example, if you're looking
at the effective exercise on falls. How long should people
have to have exercise? That is, is going to exercise class
once a week for half an hour okay? Or do you define exercise as half
an hour of at least walking every day? So, you have to define your exposure or
your intervention very carefully as part of your PICO and
ultimately your inclusion criteria. When you're thinking about your
comparison group that's a comparison with the intervention of interests or
the exposure of interests. So let's say we're interested in people
who work at a chicken processing plant and wondering if they have
increased adverse outcomes. Let's see, I'm going to make this up, increased infection with
certain type of bacteria. And so you'd have to think, well,
how do we choose our comparison group? People not working at
a chicken plant at all? Or are we interested in just the people
who are exposed directly to the chickens? And maybe their comparison group
would be people who also work for the company but might work in an office. Or are they the appropriate
comparison group? Maybe it's people in the same
region who don't work for a chicken processing plant but
work manufacturing wire. So, I'm making up these ideas, but you
have to think about, just as you would in any epidemiologic study for example,
what makes a good control group. And what you'll find is that in systematic
reviews of epidemiologic studies, you will often see more than
one type of control group or comparison group because you can get
different results, as you might imagine, depending on the comparison group used. For clinical trials, comparison
groups are typically placebos or standard therapy or no treatment depending
on what type of intervention is given. There are ethical issues, of course, for clinical trials because you have
to make sure that it's okay that your comparison group either receives
no treatment or a placebo. For example, treating people
with severe depression with placebo might be considered unethical,
and you might have to compare your test intervention to some
sort of standard therapy. You'll find more out about this
as you do your systematic review. It's extremely challenging especially for
epidemiologic or etiology studies. Now let's finally talk about
the outcomes that you select. Outcomes are getting harder and
harder to define, and I will touch on this a little bit. But the things that you need to
think about as you define them is, first of all, is this outcome important? Is it important to the patient or
the consumer, or is it just important to doctors? So for example, lab values
are viewed as important to doctors, but perhaps less important to patients. Patients might be interested in,
how soon can I go back to work? Do I have to leave work early? Do I have nausea? That might be extremely important
to
a patient and less important to a doctor. Probably everyone's concerned about
death though, so that's a good bet that it's patient important as
well as important to a caregiver. If you're thinking about outcomes,
you might also want to know, when are they measured? Does it make sense to test
the effectiveness of a class for teenagers teaching them about
using condoms to prevent sexually transmitted diseases and
only follow them for two weeks? That probably doesn't make too much sense. You probably want to follow them for
about a year to see whether the class did any good at all in getting
them to use condoms, but I have seen studies that
followed them just for two weeks. They also looked at knowledge
retention as an outcome, and I'm not sure that knowledge retention is
really an outcome that makes much sense. And this is the type of issue you
might want to consider when you're deciding your outcomes. Is knowledge retention as
important as say, using a condom? For preventing sexually
transmitted diseases? Probably not. And knowledge retention is a typical
outcome that you'll see in all different kinds of
intervention studies. So you want to know, when was it measured? What was the outcome? Was it important to the
person? Does it make sense? What was used as your measure? Did you use mean? Or did you use change from base
line? And, what sort of test was used to
actually determine your outcome? So if you're doing a vision test,
did you use that E chart or did you use some other type of chart for
measuring how well somebody sees? And if it's a baby, the E chart sure won't work because
they don't know their letters. So, deciding on which
type of chart was used is also part of deciding on your outcome and
how you're going to measure it. So, here's a well-constructed and
answerable clinical question. And I'm interested here in amblyopia. As we mentioned before
that is the lazy eye. So, for preschool children with mild
to moderate visual acuity impairment, are glasses or spectacles and
patching effective in improving visual acuity compared
with glasses alone or no treatment? And so this is a well formulated question. You can see that the preschool children
with mild to moderate visual
impairment is the population. Your intervention is glasses and patching, and your comparison group
is glasses alone or no treatment. Your outcome is visual acuity. Now this question says nothing about the
timing of measurement of visual acuity or how long the glasses and
patching are given, but that would be given in more detail form
later on and not in the question itself. And I'll go over that so that you can
see exactly what I'm talking about. We'll cover examples
a little bit later on, and that's where you can see how we actually
define the details of the question even though the question itself
is about one sentence long. Well that's it for section C, and next we'll talk in section D
about refining the question.
Hi, welcome back. This is Kate Dickerson. We're doing Section D, Refining the
Question for the framing the question for systematic reviews and
meta analysis class. So, if we refine the question,
how do we go about doing this? And I'm going to talk more about this and
then we'll go through some examples. Again, you can go back to chapter
five in the Cochrane Handbook and look at that box,
box 52a I'm giving here as an example. I'll give another one a little later on. That says what factors you should consider
when you're developing criteria for p, the types of participants or
patients or people. P works for any of those. So how is the disease defined? What are the most
important characteristics? So, the wonderful thing about this chapter
is it gives you little crib notes. And you can just check through and make sure you've done all those
things as you define your question. Similarly, box 53A covers
the factors to consider when you're developing your criteria for
types of intervention. And 54C is types of outcomes. Now a comparison group was
probably under intervention. Often comparison group is hard to
separate from the intervention itself. Now, I'll mention once more that Cochrane is really concerned
with intervention studies. So, you won't see anything in
the handbook about exposures. However, the same things apply for
exposures as apply for interventions. You have to change intervention to
exposure, but it's the same, basic idea. So, don't get too worried about the fact that Cochrane is just
dealing with interventions. You're going to have to stretch
your imagination a little bit. But we'll help you along if you
find that difficult making that extrapolation from Cochrane. I said I was going to talk
more about outcomes and I'm just going to have this slide. And I will say that these examples
are courtesy of Ian Saldana who has been for
many years a TA in this class. And he is a Doctoral
student at Johns Hopkins or by the time you listen to
this he may have graduated. And you can also read more
about how to define an outcome. In a New England Journal of
Medicine article from 2011 that Deborah Zarin wrote. She's the director of what is
called clinicaltrials.gov. That's a trial registration website where
trials are registered at inception. And they Critical factors, the PICO and some other factors are put into
a database on clinicaltrials.gov. But I'm just going to be
talking about outcomes. Now clinicaltrials.gov defines
five elements to every outcome. And we think these are pretty good
because if you come across an entry on, clinicaltrials.gov, and
it has all five of these elements defined. It's pretty easy to decide
what they did in their study. But you won't often find that people have
provided you with all five elements. The elements I've mentioned briefly, but you might not have recognized that
they fit into this particular context. The first is what's called the domain. And that's what we call an outcome. So for
example, anxiety's an outcome. Heart attack's an outcome. Visual acuity is an outcome. It's what we talk about when
we talk about outcomes. It's the domain or name of the outcome. Now you have to measure that outcome
somehow if you're talking about anxiety. It can be measured using
the Hamilton Anxiety Rating Scale. If you're talking about visual acuity,
it can be measured using that E-Chart I .mentioned or
the Snellen Visual Acuity, etc. So there are specific measurements and
measurement scales that are used. And you need to specify that,
when you specify the outcome in your particular clinical trial and
your systematic review. Then specific metrics are used for
measuring an outcome. That is,
is it a value at a certain time point? That is,
what is the hemoglobin A1C at six months? That might be a specific metric. What method of aggregation do you use for
the data. So for example, if you're looking at
100 patients in each treatment group. You can't look at each of the hundred
people's values at a specific time point one by one, instead,
you're probably getting a mean. Overall across the 100 people. So it might be the mean
change from baseline or might be the mean value at a particular
time point of hemoglobin A1C. That's the measurement that you're doing. And the time point is
when did you measure it? Is it one month? Three months? Six months? So, all five of those elements
are actually the proper way to describe an outcome for a clinical trial,
or an epidemiologic study. It's complicated, I know. And you'll probably have to return to
this as a reference again and again. But this is the proper way,
we believe at this time. Who knows what's going to change in
the future to measure an outcome. So let's go back again to PI/ECO,
patients or populations, interventions or exposures,
comparison groups or outcome. And keep that in mind because it's so
important for everything we do. I can't emphasize that enough. Now I just want to say that sometimes
you'll see people calling this picots or picos. And I mentioned sort of off to the side
some people put settingiIn here. Well they also put timing,
and that's the T and the S. So timing might include how
long is the minimum follow up you want in your systematic review. For example, we're looking at
education of teenagers for prevention of sexually
transmitted disease. What is our minimum duration of
follow-up that we think makes sense for looking to see whether this
education was effective? I've already said that I don't
think two weeks makes much sense. Knowing that kids have changed their
behavior at two weeks in time. That's fine, but that doesn't mean that's
what they're going to continue to do. So I would say if I'm looking at condom
use after an educational program. That I would want to probably
follow those kids at least a year. Maybe I'd look at them at monthly
intervals, and ask them what they're doing, but I'd probably want
to follow them at least a year. Similarly, if I'm looking
at the effects of patching in preschoolers who have amblyopia. I'd probably want to look at
whether their amblyopia's reverted after they stopped the patching. Or whether this is a longterm
fix that we can expect. I mentioned also, we might be
interested just in looking at people. For example,
in a community living situation. If we're talking about falls, we might not
be interested in falls in nursing homes. We might only be interested in falls
of people living on their own. We might be interested only in
inpatients or only in outpatients. So often people say that PICO,
PICO or PECO isn't enough. That we need to add timing and setting,
and it's fine if you want to do that. We tend to just use PICO without the T and
S and incorporate that somewhere else. We don't forget about it entirely,
but if you want to make it PICOTS, that's fine with me. So, sometimes, as I mentioned,
you have a combined effectiveness and harm question that you're addressing. And as I mentioned, effectiveness
questions are best answered with an RCT. While harm questions are best
answered with a randomized trial, but this often isn't practical. Because the follow up's too short or
the studies are too small. And so
you may need to use observational data. So I just throw that in as a reminder. Let's go back then to
the refinement of our question. We make a big deal in this
class about deciding ahead of time what you're going to do, and
trying not to be influenced by the data. However, I will say that to
formulate your answerable question, you generally need to know a little
bit about what's out there. It really helps to know what outcomes
people who've done trials think is important. So for example, you might believe that quality of
life is the most important outcome. In post-surgery for breast cancer,
but it may never have been measured. I think it has been
measured in many studies, but it may not have been measured. So you probably should know what
are the outcomes that people in a field believe are important. So in any cancer study, they're going
to believe that death is important. And you want to know this, because you want to know what
to measure as your outcomes. Because when we're talking about the O,
the outcomes. You are concerned both
with the question and the outcomes that were present
in the clinical trials. That you're going to include
in your systematic review. And the outcomes that you think are as
important as the systematic reviewer. It's kind of tricky, but these may be different especially because
the trials may have been a long time ago. Before a certain outcome was
really recognized to be important. And quality of life is one of those
outcomes that thank goodness we're now realizing is important how the patient
feels, but it wasn't always measured. So on one hand you need
to guard against testing a hypothesis that you formulated
after you've seen the data. And, on the other hand, you probably, or someone on your team probably needs
to know a little bit about the topic. So you can choose outcomes
that makes sense. So that's the end of section D, and next we're going to give some examples
of what I've been talking about. I think it often helps
to have some examples, to try to make the whole
scene look a little clearer.
Welcome back. This is section E of the lecture
framing the question, and this section is entitled Some Examples
of Framing the Question. I'm K Dickerson. So now I want to cover a few examples of
how to frame a question, and we've been talking in sort of somewhat theoretical
terms about the considerations you have. Now let's talk about
how you actually do it. Remember that were going to go
with PICO or P-I-C-O or P-E-C-O. And we're going to use that as
a framework for asking our question. So here's the question that I'm interested
in, is drug therapy associated with long term morbidity and mortality in
older persons with moderate hypertension? So our P, our population or patients or people or
older persons with moderate hypertension. Our intervention is drug therapy,
our comparison isn't stated. And you'll often find, in the answerable clinical question,
that the comparison group is not stated. So, I just wanted to tell
you that ahead of time, but you will have to think about that and
that's what I'm going to show you next. And then O is long term morbidity and mortality, those are the outcomes
that we are interested in. Of course we have to
say what morbidity and mortality are,
as you know from previous sections. So here we have the question. So it's a little different order, it's
not PICO but all the elements are there, I-O-P, or setting, and
condition of interest. So the comparison goes on
through the intervention there. And as I mentioned in this particular
question the comparison group is not mentioned. So when we talk about drug therapy,
which is our intervention, what drug therapy are we considering? What's eligible for our systematic review? So you
have to decide this. And we decided ahead of time that drug
therapy for moderate hypertension, we would include ACE inhibitors,
angiotensin receptor antagonists or ARA. Beta adrenergic blockers, combined alpha
and beta blockers, calcium-channel blockers, diuretics, central
sympatholytics, and direct vasodilators. So those are all drugs that we
would consider all right in answering our question. And so you can see already that
it's sort of a broad question and that we've decided to include all these
different types of interventions for moderate hypertension, because they
come from very different classes. But we're going to include all of them
because it's really a much bigger question which is,
morbidity and mortality. No matter what you're taking, does it
work, is basically what we're asking. And what's long term? Well, you have to define what long term
is and we're saying at least one year, greater than or equal to one year. What's morbidity, what's mortality? So we're
including then as outcomes fatal
and nonfatal strokes, fatal and nonfatal coronary heart disease, cardiovascular
events, and total mortality. And you would have to define each of
these very specifically, and I will say, again, that sometimes people choose
just to use whatever definitions authors use in the individual studies
included in your systematic reviews. And that probably is okay, unless you're
in a field where there's a good deal of disagreement about what constitutes
an outcome or a diagnosis. What's an older person? Well I'm very sorry to tell you that
it's people older than 60 years old who are outpatient. That is, I wouldn't call those
older persons necessarily but I'm afraid our healthcare setting does. And outpatient, so they're not
interested in people who are inpatients. And then what's moderate hypertension? For this systematic review, it's
a systolic blood pressure of 140 to 179 millimeters of mercury, and
diastolic is 90 to 109. So you can see that even though
the question looks pretty simple, there actually behind the scenes are definitions
of what each of these terms means. So let's try an epidemiology example so
that you have some practice doing both. Here's the question. Is a history of exercise training
associated with falls in community dwelling and institutionalized people? So how does that parse out? So here we have
it, the exposure
is history of exercise training. The outcome is false,
the setting is community dwelling, and the population is
institutionalized people. And we're putting,
in this case, settings and population together
because I tend to do that. So history of exercise training, we're saying has to have
been in the past two years. And it could be anything from
balance training, mobility training, physical therapy,
strength training, Tai Chi. So any of these count
as exercise training. And again,
here's a place where some experience with the subject matter probably is useful,
because people know that Tai Chi has been tested to look
at its association with falls. What about outcomes? Well, the outcomes that were determined
to be of importance by these systematic reviewers were number of falls,
falls that were injurious and that has to be defined,
hospitalization, fracture, and death. So just having a fall Is an outcome. Well, what's a fall? Is it losing your balance but
catching
yourself on the edge of the counter? What if you fall in your house and
nobody knows about it? Is that a fall? So you have to consider
all of these things and how you're going to record those falls. Do people keep diaries? Is it only in certain situations and
so forth. And in this study, institutionalized and
community dwelling people were defined as outpatients and people in nursing homes,
that's considered community dwelling. And even though it didn't say
anything about the elderly, it is just patients greater than or
equal to 65 years old. So those are the definitions
that are behind the scenes for this shorter sentence. So those are two examples of how you
would go about asking your question, and the behind the scenes eligibility criteria
associated with that shorter version. So here's a question that
someone who took the course, or group that took the course,
in 2009 came up with. They looked for their topic at
the association between the level of alcohol consumption and
the incidence of stroke. Their research question then,
with a question mark, was does moderate to heavy alcohol
consumption reduce the risk of stroke? Their population was adults
without prior stroke. Their exposure was alcohol consumption and it could be presented in
the paper as drinks per day. It could be measured over
the past month or longer, binge drinking was all right but so
was looking at it as short term. So exposure had a pretty
broad definition here. The comparison group was non-drinkers,
total non-drinkers. The outcome was ischemic or
hemorrhagic stroke or both. And enough information for
the authors of the systematic review, to be able to estimate for each individual
study, a relative risk and odds ratio. And attributable risk in
the 95% confidence intervals. And, then, finally,
these epidemiologists and clinicians, who were in this group in 2009 said, we
are only going to look at cohort studies. So that was a very interesting situation,
it wasn't an intervention, it was really an epidemiology study or
etiology study you could say. So that ends our examples in section E,
and our next section will be section F where
I will talk about analytic frameworks.
Welcome back. This is Kay Dickerson, and I'm speaking in the course
Systematic Reviews and Meta Analysis. We're about to begin section F,
on Analytic Frameworks. Now this discussion of
analytic frameworks, I'm putting in there because I
believe it's very important and it's one of the standards that
the Institute of Medicine has proposed for systematic reviews, but in this class,
because you already have plenty to do, we're making development of an analytic
framework optional for your review. I encourage you to try it because it
actually is a very helpful framework and my guess is ten years from now,
you'll see them in all systematic reviews. But we're limiting how much you do here
and so I'm telling you because I think it's important, but it's optional
in your own systematic review. So, an analytic framework is just one
of the names of what this thing is. It's also called a logic model, and
what it does is it links the evidence and how it all fits together in
the populations as it relates to outcomes. I'll show you an example, don't worry. But you'll also see terms like logic
models, conceptual frameworks, influence diagrams,
theoretical frameworks, those are all terms that are used
to discuss analytic frameworks. So don't be confused if
they're different words. They're really all for the same thing. You'll see analytic frameworks
in the evidence-based practice reviews that are done here for
example, at Johns Hopkins, and are funded by the AHRQ, the Agency for
Healthcare Quality and Research. So why use one of these
analytic frameworks anyway, to set up the question that you're asking? What it does when you write
down what you're thinking of? How is your intervention
going to affect treatment, how is that treatment going
to affect the outcome? What other ramifications of
the treatment might you expect? It helps you to clarify what
exactly you're thinking about. What are the steps in how a person is
treated, or diagnosed and then treated? And to help you think about the fact that
you may have several questions here, not just one. And therefore that you need to address
them separately, because they may be using different sorts of evidence, as I
mentioned very early on in this lecture. So especially if you're addressing
multiple questions in a review, you want to have analytic framework so
that you can parse out the different questions that you're addressing and
a complex chain of logic. So what are the components? Well, an analytic framework, as we've been
talking about, specifies the population, the interventions or
exposures to outcomes, and sometimes you'll see timing,
settings and comparators put in. But usually it doesn't focus on that so
much. And it clarifies these links between
the intermediate outcomes and the health outcomes. Remember when I mentioned hemoglobin A1C,
as it would be measured for example in this study of people with
diabetes or who are at risk of diabetes. That could be considered by some people
an interim outcome or a surrogate outcome, and not the actual outcome that you're
interested in, which might be death. It might be amputation,
it might be neuropathy or diabetic retinopathy, for example. So it helps you to find when
you have a analytic framework, what's an interim outcome along the way,
and what is the actual final outcome that
you're interested in in your study. These analytic frameworks use arrows and
boxes, and squares, and circles that tell you how
everything is joined together. And if you're interested in
constructing these things, you can look at an article by Cindy Mulrow
in Annals of Internal Medicine quite awhile ago in 1997,
where she explains how to do this. So here's a typical depiction
of a research question. You'd have these boxes
with the rounded edges and you might have a square box,
each meaning different things. And these numbers represent the key
questions that you're asking in your analytic framework or
in your series of systematic reviews. And you might have interim outcomes or
side outcomes, such as in two. If you're interested in
reading more about this, you can read an article by
Evelyn Whitlock in American Journal of Preventative Medicine,
about ten years ago, 2002. So here is a sample framework and you might start with people who
are at risk of a disease, let's say, women over 60 might be at risk of breast
cancer, just by being women and over 60. They are screened using mammography. That's a question. Is screening
effective in preventing
outcomes that we don't want, such as breast cancer mortality? And you can look at both the effectiveness
of screening, that is early detection, which is really an interim outcome or
even the adverse effects of screening. Were women upset by results that
meant they had to come back for another test, for example. You can also see that with
early detection and treatment, sometimes with breast cancer people
are treated who really wouldn't need to be treated because nothing
bad was ever going to happen to them. Unfortunately, we don't know ahead of time
who needs to be treated and who doesn't. So that's a tough one to
address in a real-life setting. But your early detection then can
lead to adverse effects of treatment. So you could treat someone with
early breast cancer various ways and that person could have adverse
effects simply from, let's say, chemotherapy, nausea and vomiting,
hair falling out, various effects. Some forms of treatment,
talking about bone marrow transplant for women with breast cancer could kill you,
and actually even some of our forms of chemotherapy can kill
you in rare situations, and even radiation could kill you over the long
term if you follow people long enough. So you get an interim outcome,
which might be detection of breast cancer, it might be the type of surgery a person
has, it might be progression of disease. Those are all intermediate outcomes where
the final outcome that you're interested in is mortality, or perhaps,
something else depending on your disease. But by drawing it this way,
you can see the different questions, the effectiveness of screening, the possible harm, the effectiveness
of early detection, possible harm. You can look at intermediate outcomes, and
you would have to define what they are. And you can look at the final outcomes
that one might be interested in, reduced morbidity and
mortality at a later date. So that would be a sample working format. Again, setting up your question this way, using an
analytic framework can be
important, just as asking your question in the proper way in those examples we
showed to get proper data collection. You don't know what data
you want to collect until you have your question properly formatted. The analytic framework helps you to decide what
are the questions
you're actually asking. What are the interventions, what
are the exposures, what are the outcomes, what are the populations. And especially,
it will help you to define the final criteria that you're going to use when
you're selecting studies for your review. One of the hardest things, and
we're struggling with it right now in a systematic review that we're doing,
is deciding which data to be abstracted. On one hand, one never wants to abstract too much data because it takes
a lot of time, especially checking. You want to do it independently and then
check and there might be disagreement. But you also don't want to
collect too little data. So making sure that you get just the right
amount, like the three bears, too much, too little, just right. But there's no crystal ball that
says what it is the right amount so takes a lot of discussion. But getting that question right and
having an analytic framework are all steps that help you decide which
data will be abstracted as well. So it's a very important part of
the whole sequence of events in doing a systematic review. All right, so now we've framed the
research question, we're going to have you talk in small groups about this and
get your research question straight. As I said at the beginning, this will
take probably more than one session, actually, because it's
a very important part. And you'll see how many questions
there are as you go along. But what we're going to talk about in
lecture next is how to develop a search strategy for searching the bibliographic
databases, the journals, and clinical trials registries, and all
the other sources that are out there that maybe you can't search electronically,
that you still have to search by hand. But still, you have to develop
a strategy for what you're going to do. And that's the next step, and what we're
going to talk about in the next lecture. Thank you very much. [MUSIC]
I can first tell you how I didn't get into it and then I'll tell you how I got into it. When I was a graduate student, Jean
Glass, who is the man who invented the term meta analysis, came to my university, to my own department, to give a
talk. But it was winter and I stayed home and I didn't go listen to his talk. But then later and probably about another two
years, a professor joined the faculty and his research area was Research Synthesis and Meta Analysis and that was Larry
Hedges, and he needed a teaching assistant for his class. And so, I was assigned to do that and then came to realize that I
actually knew a little bit of mathematics and he started giving me other tasks to do and I got interested. So, that's the
beginning of the story. The things that I have seen that are different are one, the size of the reviews which comes down
to the extent of the problem. Reviews medicine in my view compared to education are much more narrowly defined. I
suspect this may have to do with clinical utility and trying to come up with a targeted decision or answer about a
particular maybe medication or treatment. In education, the reviews tend to be five, 10, 20 times as large in terms of
numbers of studies and in terms of the scope of the review. Now, one thing that we noticed is that there is an, in fact, a
colleague and I intend on to publish on this. There is really no analog to pick up in social sciences. Well, there is one but
people don't use it. And so, we have a paper that we have just about ready to send out that is going to lobby for people to
be much more thoughtful about preparing the questions before they do a systematic review. So, this is the things that I
have noticed.
Hello, this is Claire Twose again. I'd like to introduce you to the section
that covers searching principles. We're going to introduce
you to the standards for doing a search, a high quality search
that supports a systematic review. Among the things you're going to learn
are the sources that you need to search, the electronic databases,
how to structure the search, so it actually works in those databases and
is recordable and reproducible. We're also going to
introduce you to one of the key tenets of doing a systematic
review, document your search. Document what you're doing. I'm going to be covering some of
the evidence that's available on publication bias, on why you need to
be doing this sophisticated search, when you're doing a systematic review. Dr. Kate Dickerson will introduce
you to additional forms of bias and assessing the risk of
bias in clinical trials. There are two ways that bias can get into
your systematic review and meta-analysis. And that we're going to talk about
in the middle part of this course. The first way is there can be bias in
the individual studies that you include in your systematic review. And the second way is there can
be a bias in the way that you do your systematic review and
meta-analysis. We're going to talk about both. The first type we call a risk of bias,
and we'll talk about that. Some of you may think about it in terms
of the quality of the included studies. The second part which is bias in how
you do your systematic review of meta-analysis, we refer to as metabias. And we're going to talk about that. In general,
those ways have to do with
publication bias, as Claire mentioned. That really relates more to metabias. In the individual studies, you can think
of how the patients are allocated to the treatment they receive or the exposure
that they get, how we collect the data. Those are different ways
that bias can influence the individual studies that are included. Then one other aspect that we're
going to include in this middle part of the course has to do with something
called qualitative synthesis. This comes before you actually
do the meta-analysis, or the quantitative synthesis. And the qualitative synthesis is
arguably the most important part of the systematic review, and also difficult to sort of get your
arms around and figure out what it is. It's a way of talking about the studies
that you've included and what you've done. It's how the characteristics of
the studies can influence the total view. It's how bias in the individual studies
can influence your overall picture. And it's a qualitative putting together of
all the individual studies that you have before you do a meta-analysis. So those are the aspects that I'm going to
be talking about and that we'll bring into the course, about in the middle,
before we get to the meta-analysis.
[MUSIC] Welcome to today's lecture,
my name is Claire Twose. I'm the Associate Director
of Public Health and Basic Science Informationist Services. And I'm going to be talking with
you about finding the evidence to support a systematic
review in meta-analysis. We're covering Various
searching principles involved. This presentation is going
to be in four parts. I'm going to start with
an introduction to various themes. Spend the next section going
over the variety of sources that you need to use in order to do
a comprehensive, unbiased search. Then we're going to drill down and
I'll show you how to build the kind of search strategy that works in some of
the large databases that you need to use. And I'll end with a few words about
Documentation and conclusions. So, first part. At this point in the process you now
have an answerable research question. What is it that you need to do next? The next step is to develop the protocol. In
doing a systematic
review in meta analysis, you are doing a scientific experiment. The only difference from other
randomized trials is that your subjects are the literature and
the citations that you're going to find. So, probably, foundational to the quality
of everything you do from here on out, is the search that
you're going to perform. Almost all of the researchers I work
with are surprised, to put it mildly, at the amount of effort and time that
it takes to do a high quality search. I'm going to be going over
the various steps and processes that you need to do this. The other major piece is
developing your inclusion. The next exclusion criteria. Both of the readings, the Institute of Medicine Standards
on developing systematic reviews. Talk about the need to
include an information specialist if you where going to do
a systematic review in the future. An excellent resource that's going to
go into more detail that I can cover in this lecture. Is chapter six from the Cochrane Handbook
which is also available in your readings. Another tip. If you can start thinking from
the beginning to document what you do at every stage. You will be saving yourself a lot
of trial and heartache later on. Don't underestimate the difficulty
that you're going to find in identifying the articles that
you need to include in your study. Kay Dickerson has already talked
to you about publication bias and the problems with data
not being published. The area I'm going to be covering is the
challenges in finding the published data. You might be asking yourself
some questions at this point. Why is it so important to do such
a comprehensive, extensive search? Wouldn't it be okay to go to PubMed? PubMed is a huge database
with 20 million citations. You find the articles, look in the bibliographies of
the ones that are included. Does doing a good search
really matter that much? I can give you several examples,
about how important the quality of the search is to your
research, and to research in general. One of the most heartbreaking examples
happened right here at Hopkins. Somewhat over 10 years ago a researcher
undertook a study on asthma. And used healthy subjects and bronchial challenge to mimic
the experience of asthma. I think I'm referring to
the hexmethonium trial, in which a healthy 24 year
old research assistant. Who was part of that study, died because of the adverse effects
from the drug that was used. Once this became public knowledge, librarians in various places
started doing searches. And it became clear that evidence
information that would have flagged this as a dangerous procedure
was actually easily available. Had the researcher consulted with
an information professional and done an extensive search. The quality of your search
is going to certainly effect the quality of the research you do. It can even have Impacts
on individual's lives. How do you do the search? You're going to develop a, sort of,
sub-protocol for the search itself. What you're going to need to do is
identify and document the sources you use. This will include Large developed
databases of citations and abstracts in various areas. Will also include additional processes, like hand-searching, which
these
days can be done electronically. It means reviewing tables of content,
it means reviewing the reference lists, sometimes following citation tracks. You're going to need to document
how you did the searches, the dates you did them, the exact
strategies that you used in the databases. You're also going to need to determine
how you make decisions about what you include and exclude. For systematic reviews and meta analyses
to reduce the possibility of bias. Duplicate screening is used, that is
two people independently review each citation and abstract to see
whether it's included or not. So, in the next session I'm going to give
you some more information about the key sources you're going to use and
the various techniques for searching.
Welcome back. Here we are now in Section B. We're going to cover
Identifying Key Sources and the Techniques that you use for
searching those sources. That will include a variety
of electronic databases. I'm going to introduce at this point one
of the features of the database called a Controlled vocabulary to help mem be
able to describe the various databases and indexing, particularly for
randomized control trials and observational studies because I know those
are the types of studies that you all are going to be using in
your systematic reviews. I'll also be going over the other sources
that need to be covered in order to do a comprehensive search. The next two slides present a list
of Major Bibliographic Databases that you would use to find randomized
control trials and observational studies. The first set are focused
within the medical domain, the next one is an example of
regionally based database that covers literature produced within
particular areas of the world. And then there are a whole series
of subject specific databases. And I've picked out a few
to illustrate this for you. You may be wondering whether
really is important or really necessary to do
less extensive a search. What this slide shows you are results
from a study by Lawrence on five different areas, key areas,
in public health injury prevention. He looked at four different databases and identified the studies that would
be relevant to those five areas. And if you look at the right hand column, you see that It's
labelled Unique articles. Every single one of those areas, every single one of the databases
contributed something unique to the study. So it was in fact necessary to
search all of them in order to get all of the literature
relevant to the questions. Almost all of the large, bibliographic
databases, at least the ones that are subject specific, include something
called a Controlled Vocabulary. What this is, is a standardize
list of terminology that's used in indexing information to
facilitate retrieval. So that means that regardless of whether
a paper is published in English or French, if it's about heart attack and the controlled vocabulary term
is myocardial Infarction. When you use that you will get all
of those articles in your search. So it provides a Consistency. In doing a systematic review though, it's important to use
both
controlled vocabulary and keywords. By keywords I mean any kind
of free text searching, that you would do to look for words in
articles or titles and other fields. Little more on PubMed. People often use the term MEDLINE and
PubMed interchangeably. PubMed is the online access that the
national library of medicine provides to the MEDLINE database and additional
citations that are ruled into the product. MEDLINE consists of citations and
abstracts from over 5,600, journals. And each of those
citations has been indexed. I just described controlled vocabulary. There are actually PhD professionals
at the National Library of Medicine who review the article. Select terms from that controlled
vocabulary and add them to the record so that, whether or not something is
mentioned in the abstract, or even if it doesn't have an abstract, you'll still
be able to find the relevant article. Currently, there are over 19
million records in MEDLINE, total of 22 million in PubMed,
that includes newer articles that haven't been indexed yet and some of the older
articles that have been rolled in. A little more about PubMed. The controlled vocabulary that's used in
PubMed is the medical subject headings or MeSH that consists of Descriptors and
subheadings. There's a thesaurus available that
you can search for these terms, and you can find also the definition that
the indexes use when they apply a term. The list is not just one straight list,
there are narrower terms and broader terms. And the indexes, index at the most specific term
that would apply to the article. Particularly for those of you looking for randomized trials, publication type
is something to pay attentions to. And in terms of keyword searching, there are titles available back
to 1966 and abstracts to 1975. This slide shows you an example
an entry for a mesh term, a medical subject heading for
Macular Degeneration. See the term at the top. The definition that's used by the indexer. Narrower terms called
subheadings that
are used when an article is specifically about for example Microbiology
related to Macular Degeneration. For the purpose of a systematic review, I would stay at the descriptor level,
that is the Macular Degeneration. You're going to get too specific
if you're using subheadings. If you scroll down to
the bottom of the page, which is on this slide,
you'll see what are called Entry Terms. For indexed articles,
any of these terms that appear this article will have the MeSH term
Macular Degeneration applied to it. You can use this list of entry terms
though to find good keywords with non-indexed articles that have for
example, Maculopopy, wouldn't necessarily
come up with Macular Degeneration. And at the bottom of the page, you see
the tree I described with narrow terms and broader terms. If the article is about
Macular Degeneration, the indexer should use that term. For searching purposes, know that if there are narrower
terms below the term you used, they will also be included automatically
unless you tell PubMed not to do that. Now I'll say a few words about
indexing specifically for randomized control trials and
observational studies. Despite the importance of randomized
trials to clinical medicine, the has been a challenge to develop indexing
to find these studies effectively. Up to 1977, there were no specific
terms for randomized trials and as you can see from since then,
the terms have changed over time. Again the point here is if you
want to get all the literature, including the older literature,
you need to include in your search strategy the older term
as well as the current term. The Cochran Collaboration here at Johns
Hopkins was involved in a partnership with the National LIbrary of Medicine
to improve the indexing for randomized-controlled trials. So, it has, indeed,
got better over the years. When it comes to observational studies, there's an even broader
range of terms available. Depending on the topic that you have,
you'll want to check and see what the terms would be, here is a
list of some of the terms that are in use for Epidemiologic studies, Case control,
Cohort studies, and so on. Earlier indexing is shown here, as well as the website that you can go to,
to get into the MeSH browser. And I would encourage you to take a look
and see which terms would apply for your study. There hasn't been a whole
lot of research done on finding Observational Studies in PubMed. There was one pilot study done by
Susan Wieland, who looked at how well indexed these papers were, and
how easy it was to find them. Here's a table that shows
some of her results. She found that Outcome terms and Design terms were fairly well indexed but
Exposure terms less so. If you look at the bottom of the table,
you'll rows for Precision and Sensitivity. Precision refers to how well the search
did at Identifying only the studies that were relevant and not getting
false hits or inappropriate articles. And Sensitivity refers to
how good the search was at being comprehensive of getting all
of the citations that were relevant. And she could calculate this because she
knew how many studies were available for the various searches that she was doing. PubMed has over 22 million
citations and covers over 5,600 journals but
there is still more material out there. The other major medical bibliographic
database to consider is EMBASE which is like
a European Medline that was based on the index called Excerpta Medica,
and is now provided by Elsevier. We have access to it through endbase.com,
and they actually licensed the MEDLINE course,
so when you search endbase.com you're searching both of them,
unless again you tell it otherwise. There's nineteen million citations here,
and over 7,000 periodicals indexed. There's also a controlled vocabulary,
but it's not MeSH terms, they call it EMTREE terms, and
it's a somewhat different vocabulary. The other very important source to use is
the Central Register of Controlled Trials that's provided by the Cochrane Library
by the Cochrane Collaboration. This is a group of
researchers that a voluntary organization that's committed to doing
high quality systematic reviews. They're arranged in review
groups based on topic area. And each of those groups combs
the literature for controlled trials and then contributes them to
the Central Register of Trials. So you can benefit from their work
by doing your search in the central registry to compliment other
searches in PubMed and Embase. They index their articles
using MeSH terms. At the beginning of this section, I talked
about a set of databases or two databases, the Web of Science and Scopus that
allow you to do citation searching. Let me just say a little bit
more about the Web of Science. It differs from the first three
databases in two major ways. One is, it's much broader in
terms of the disciplines covered. You have articles from the social
sciences, arts, and humanities, as well as all of the sciences. For every single article in this database, some of which go
back to the 1800s,
the reference list is included as well. What that does for you is create a web. If you have a key article that's on topic,
from say ten years ago, not only can you
look at the reference list online, but you can look and see who has cited
that article since it came out. So you can follow trends
of research over time and augment, the search strategy
that you do with specific terms. Because it's so broad, there is no
controlled vocabulary and I've given you some information about some of the ways
to use the database on the slide. What I was just describing is
sometimes called Snowballing. You can start from one reference and
then select other relevant references from the reference list or
move forward to the citing articles. In PubMed you can also do that
using the related articles feature which other databases
sometimes call Find Similar. All of these sources I have
talked about now present you with peer reviewed published literature. It's also important in doing
a comprehensive, sensitive search for a systematic review, to cover
sources of unpublished literature. There's been a move in the last few
years to requiring systematic reviewers, to search clinical trials registry, and
I've given you a list of three, well, four possibilities to go look for trials. Additionally government sites for example
the FDA are an excellent source for literature for references to studies. At least for those drugs that the manufacturer has
put forward for licensing for use. Identifying key Organizations and
foundations, I might also be publishing
literature in this area and then finding their websites and
searching for literature is a good idea. In doing sort of a general, quote unquote gray literature search online you
might consider using scirus.com. It's tailored to find high
quality literature and various sources from the open web. And here I've given you the web
address for the FDA site. So again,
there has been some research done on, whether it makes a difference to
include unpublished literature. Hart and his colleagues did
a meta-analysis of citations related to nine new medications and
compared the results, with and without including the articles of the studies that
were identified through the FDA site. As you can see from the slide,
in several cases, including it or not made a difference. Sometimes, it actually showed
a higher estimate of efficacy, sometimes it showed a lower
indication of efficacy. In general however, the estimates of
harm was always greater when unpublished data was included in the meta-analysis. A few more words about Hand-
Searching for
Cochran reviews, this is one of the mandatory aspects of doing a
comprehensive search to support a review. What you need to do is identify key journals that will cover your topic
area and select a number of years. And then review the table of contents. It's no longer necessary
to do this by hand. The journal table of contents
are available online, so it's easier but it still needs to be done. Conference proceedings
are available from Biosis and open web searching, and you also need to review the reference list of any study
that you have included in your study. And again,
I've listed here a couple of sources for Gray literature,
Dissertations, and reports. There's also been research done on whether
doing this kind of Hand Searching, which takes time and
effort makes a difference. Sally Hopewell and her colleagues who
looked at systematic reviews to see whether all of the RCTs relevant to the
topic would be included, Hand- Searching identified those reviews very
effectively 92 to 100% accuracy. You can see from the list that
the search is in MEDLINE, Embase and PyscINFO were less sensitive. There's something I'll be showing
you a little later called the Cochrane Highly Sensitive Search
Strategy for RCT's that was developed initially in 1994, and
that was more effective at finding them. Unfortunately it takes too much time to
just hand search all of the literature. So what we wind up doing is
establish search strategies for these databases and then augment that
with enough hand searching to feel that it's being fully comprehensive. And don't forget also to use your personal
contacts whether it be experts in the field that you're working in or any
contacts you have within the industry to help you identify and find articles or
studies that are relevant to your topic. What I'm going to do in the next section,
is explain how to put together the kind of strategy that you
would use in the large databases. High -quality electronic search strategy.
Welcome back. We are now on Section C of the third
part of this presentation. And in this section, we're going to cover How to Build
a High-Quality Search Strategy. I'm going to use the Medline PubMed
database as an example. And show you how to find
medical subject headings and other elements from a PubMed record. One of the things you're going to be
asked to produce in the class is a table of the mesh and keywords that you use and
we'll go into that. In building a high quality electronic
search strategy, one of the guiding principles is being able to replicate it,
as in any other experiment. So you want to record it in
a way that nails it down. And I'm going to show you some of
the tools that you can use to do that. I'll introduce you to the idea
of study design filters. So if you're looking for randomized
trials there are concepts that you can add to your search that will
help focus in that area. And we will also introduce
you to a nice simple tool for doing a double check on your
search when you're done. So the process that's suggested for
developing a MEDLINE Search Strategy for a systematic review for this class in particular is to start
with a relatively simple search. You have a review that you're starting
with that will have some citations in it. But from that, take terms,
create a relatively simple strategy and retrieve some additional reports. Analyze them as in pull out
the medical subject headings and the keywords from those studies and
create a table. And identify the ones that look like
they're going to be the most useful to your search. Then revise the strategy using
those terms and rerun it. This looks like a nice
linear set of steps but I think what you're going to
find is it's pretty iterative. You do a search,
you find things, you try it out. You look again. I'll say again, if you can remember to
document what you're doing at this point, you will save yourself repeated effort. Even the terms that don't turn
up to be useful, note that and then you want, a day or two later, go
back and say, oh, did I use that or not? Once you have got the optimal search
strategy, you run that, retrieve the citations and import them into end
note and go on to the next database. So in developing your search strategy, what you need to do is take your research
question and break it into concepts. The PICO format that many of you may
be familiar with is helpful here. That stands for Population,
Intervention, Comparison, and Outcome. So here's an example that has to do
with age related macular degeneration, and whether Intravitreal injections of Lucentis are better than
Avastin to prevent vision loss. So the slide gives you how you
would take that question and break it into those four concepts. Once you have done that then
you can start translating this into a format that
a database can understand. So here you see the four concepts for population with synonyms
OR-ed to each other. And then at the end you and
that with the next concept, the series of synonyms that
represent that concept. In doing a search for
a systematic review, less is more. The fewest number of concepts
that you can use, and have a manageable set of results, which
is still in the thousands, the better. It's rare that for systematic review
you include the outcome terms, because it's so easy simply to
overlook one of the terms or words that would be used to identify it. And once you've got that together,
then if there is a tested field study type filter available,
you add that into the search. What I'm talking here is basically logic. And I don't know of any large databases
doesn't have that available or doesn't use that as a basis for searching. So AND gives you an intersection
between two sets. In this case, macular degeneration and
intravitreal injections. Or expands what you have by
using synonyms or other terms or ways that an author would describe what
they're doing in an abstract or title. NOT is a tool that I use
when I'm assessing a search. I might add in a term, and it says,
oh, it's added 50 citations. Well, let me see what those are. I can take the two searches and NOT out
everything except those 50 citations and see if it was really helpful or
not for example. But I would not use that in
the final search strategy. Let's move on to PubMed itself. And let me show you what this
might look like in practice. So you come to PubMed just using the terms
that we had in the previous slide. You type those into the search box up at
the top and you get a series of citations. You see we got 400 hits here. So, what I typed in is at the top. But the search
that actually got done,
is what's down at the bottom right. The Search Details. And if we go underneath that box,
there's a see more. You'll see the whole thing. So, even though all I typed
in was macular degeneration. PubMed, behind the scenes,
is trying to help me. It went ahead and ran those terms against
its list of medical subject headings and said, oh there's a subject
heading macular degeneration. I bet that's what you wanted. And put it in the search for me. Now in this case, it worked
just fine. It is really a good idea to check
the search details when you do a search, because it usually gets it right, but sometimes you may get some things that you
really don't want in a search strategy. So if I were to click on one of
the citations that was retrieved from that search,
I might see something like this. Here's a record in abstract view. You can find keywords to add to your
search from the abstract and the citation. You can look to the related citations
on the right hand side of the screen, to do some Snowballing of articles. If you scroll down you're most likely
to see this what you see here. A single line with a plus sign that
says publication types MeSH terms and other terms. If you click on that plus sign,
what you will get is a list of the publication types that
have been applied to this article and the MeSH terms that the indexer applied. So this is a great place to identify
the terms that were used for the specific articles. Back at the abstract view,
on the left hand top side, you have the opportunity to
change the display settings. If you go from Abstract to Medline,
let me show you what you'll see. I've been talking about
searching a record. This slide shows you the structure that the record is actually
found in the database. When we search for MeSH headings, we
search in the fields that are labeled MH. That lists on the left are what
are called Field Codes. And they identify the specific fields
where particular information is put. So, if somewhat about that you see TA,
that's the journal title. There's a field for the TI for
the title and so on. When you do keyword searching,
you pick the other fields, other than the MeSH terms, that you
want to search, to find these words. Another way to find MeSH is to go straight to the database of MeSH terms that
the national library provides for you. So if you go to the home
page of PubMed and click on the link on the right,
that will take you into the MeSH database. Now if you put in search terms
you're searching in the vocabulary, not in the articles. And what you get back
are a list of any entries for terms that matches your search. So here we have, again,
the entry for Macular Degeneration. You click on the title and it takes you into the record that you
saw previously for Macular Degeneration. If you see at the bottom of the slide, I said that the search will search on
everything unless you tell it otherwise. You see where it says do not include
MeSH terms found below this term, you click on that box, and
that's one way to limit the search. Here's the bottom of the screen again. And this is an example of the kind
of table that you'll be asked to put together with your
collected MeSH terms and keywords. And what's being suggested is that you
use the rows for the individual papers. Use the columns for
all of the various terms. And then identify which
papers have what terms. And that should help you
get a overall picture for which terms are going to be most
helpful to you in building your search. I showed you the slide a couple of
slides back that had the field tags. Here's a list of some of
the most common ones. At the bottom is the URL where
you can find the complete list of fields that you can search. You can also get to it from
the Help button that's found on every page in PubMed. When you do keyword searching, in general, starting with the
text word
field tag is a good compromise. That includes most but not all of
the fields that have text in them. For example,
it excludes the affiliation field. So if you're searching for
something in Baltimore, you wont get every institution that
happens to be in Baltimore in your search. It also doesn't include individual author
so you won't get Mr. Baltimore's papers. If that still pulls up
too many false results, you can narrow it to just title and
abstract. In some cases,
using all fields is actually a good idea. Particularly for topics that are not sort of core
medical topics or less well indexed. So once you have done the first iteration,
done a simple search, collected your MeSH terms, now is
the time to refine your search strategy. Add in the additional
terms that you found. Pay attention to plurals. You can use, at the very bottom
of the slide you see Truncation. That may be one way to take
care of those variations. Think about Abbreviations that
are specific enough to your topic to be useful without pulling up again,
false hits. Remember spelling variations. British spelling versus American spelling. I have run into snags when not
remembering
that anemia has ae in various places. Other questions that come up
in building a search include whether it's okay to limit
say by language or by year. This slide shows some results of studies
that have been done on systematic reviews. Egger for example looked at papers that
related to the same research project that German researchers published in
German and English simultaneously. And in this case, more positive findings tended to be
published in English than in German. So if you were only to include English, you would be unintentionally biasing
your results towards the positive. The same trend was found in complementary
medicine areas and studies from China, Russia, and Taiwan. Juni did a study looking at Cochrane
reviews in comprehensive searches and that was not found to be the case. Additionally, just in general,
be very cautious about using limits. You need to be able to justify
any limit you add to your search. Date limits are generally only used if,
for example, a drug was introduced in a specific year or a particular
disease emerged at a particular time. Now a few words about finding or use of search filters for Study Design. We've
been talking a fair bit
about developing concepts, that represent the actual
content of your search. I know that some of you are going to
be doing reviews of randomized trials, and others of observational studies. There are some filters developed
that have been tested in MEDLINE and NBASE for
identifying randomized control trials. What you see on this slide is the Cochrane
Highly Sensitive Search Strategy. If you go to chapter six, you will see
a couple of different iterations of that. And, you can read about which
ones would be useful there. If there are tested filters for
the databases, you can add them to your search. Here are some examples of articles
published that have done in research on MEDLINE, MBase, and
other subject specific databases and optimal search strategies for
finding different types of studies. This slide gives you a picture of one
finished search formatted for PubMed. The top set of terms, word together, represent the population
that's being studied. The bottom set are all ORed together and
actually represent both at the intervention and the comparison for
the example that we've been using. And after, as I've said, you get those subject specific concepts
represented in the terminology. Then you can add study
filter terms to that search. Margarete Sampson and her colleagues
who are information professionals doing research on the quality of searches. Reviewed the literature on tools
that have been used to evaluate the quality of electronics
search strategies. They identified 26 and
teased out from that seven key criteria that can be used
to assess your search. It's a nice checklist,
it's available with this set of slides. The PRESS stands for Peer Reviewed
Electronic Search Strategies. Sampson and
McGowan have also done research on the kinds of errors that creep into
even published search strategies. One of the key areas that
people tend to overlook, is the need to adapt your strategy. I've been using PubMed and
Medline as an example. Once you've developed your strategy,
then when you move to the other databases, you're going to search. You need to take that and actually
find the controlled vocabulary for that database, find the appropriate
truncation symbols and other items to make the search work
effectively in the other source. In summary, this section of
the presentation has covered what it takes to build a high quality electronic
search strategy that you can then record and have as part of
your systematic review report. You start by developing your search. Take your question and break it into
concepts using the PICO format. Identify synonyms and control vocabulary. Use the tools that are available
from the database, both Boolean Logic and
Controlled Vocabulary. Remember, this is going to be iterative. You're going to be revising your search
strategy until you come to one that is optimal. And remember when possible to remember when there are tested study
design
filters available to use them. And we've introduced a tool that you
can use to double check your work when you're done, the PRESS checklist. In the final section I'm going to cover
a few more words about documenting you process and conclusions.
Welcome back. This is the final section of
the lecture on finding the evidence, searching principles and
I'm going to be talking now about documenting your search and
just a few final words in conclusion. There are two main areas that
we're going to talk about related to documentation. The first is you've been asked
to purchase and used Endnote. This becomes your database of records
of all the citations that you collect from the various sources. Endnote is one of the pieces of
bibliographic management software that's available to you. Some of you may be also
familiar with RefWorks, which is also available free at
Johns Hopkins or Reference Manager. There's another tool called QUOSA
that is also available to you, at least while you're at
Hopkins free to download. QUOSA's advantage is that it
allows you to do automatic, full text retrieval for
at least some of the citations. Any of these sources have fields that
you can use to store key information about the search that you've ran, whether
you've included or excluded a citation. And that brings me to a review
of what it is that you need to record to document
the search that you're doing. You need to record when you did
the search, the day and the month and the year, which sources you used,
which databases. The exact strategy that you used. So in PubMed, for example, you would
record something that looked like what you saw in the search details field
that I showed you some slides ago. You need to identify,
which trials registers you searched and the search strategies you used for them. Any communications you've had,
any bibliographies that you've searched or citation tracking process that you used
in the web of science, for example. You also need to be able to document
what you've found specifically. PRISMA is a standard for transparent reporting of systematic
reviews and meta-analysis. It's an excellent source. It's a nice tool that you can use
to sort of get an overview of what's required to do a high
quality rigorous review. I've given you the website here. There are various articles
that present this information. There's a 27 step checklist available and
also a flowchart. There are now over 125 journal
editors that have signed on to the PRISMA standards and if you wanted
to publish a systematic review or meta-analysis with them,
they would expect to see certain features. The flowchart that's shown here, shows you the various pieces of
information you need to have recorded. On the left at the top, number of records
identified through database searching. So you would add up the total number of
hits that you found in PubMed, Endbase, Cochrane, any of the subject databases and
put that in the box on the left. Box on the right,
you would include the number of records that you found through any of
the review of references lists, tables of contents,
other snowballing techniques. Many of these databases
include the same journal and there's quite a bit of overlap, for
example, between PubMed and Endbase. So you're going to be removing duplicates
as a next step and you need to keep track of how many duplicates you remove, because
you're going to need to report that, then you'll be doing
a title abstract screen. And you'll have the number that you
screened, the number that was excluded at that level, the number you
needed to review as a full-text. And even at that point, often there are additional citations that
are excluded from your final analysis. I started this presentation back
in section one with a story about the impact that doing a high quality
search can have on your research and even the life and health of your subjects. This table is taken from another
study that I find quite compelling. The researchers search the literature for
randomized control trials that study the effectiveness of aprotinin, a
blood substitute used in cardiac surgery. I know it's kind of hard to read, but what this figure represents is
all of the studies that they found. The oldest is at the top and the newest
one that they found is at the bottom. The intention here is to represent
how well the various authors did at citing the older literature. If they cited an article,
the square is in black. The sort of half grey articles down
the diagonal are ones that you could reasonably expect the authors not to cite, because they were published
within a year of their study. So at the top, the first study that ever came out
doesn't have any other studies to cite. As you go down, you would expect
to see more and more black. But as you can tell,
there's a surprisingly low citation rate. The most frequency cited
study is that original study. The study that has the most in route
subjects number 17 is only cited 7 times. Why is this important? Every single one of these published
studies had positive findings. They all showed that aprotinin was more
effective than a comparative treatment. Researchers did a meta-analysis
of these results and after study number 12,
the results were unequivocal. Between that time and
the last study cited here, over 4,000 people were randomized
to prove the same point. That means that at least half
of them did not get what would be know to be best treatment
had the researchers actually relied on the literature that
had already been published. In conclusion, thank you very much for
your time and attention to this lecture. We have covered the various components
that go into creating a comprehensive, sensitive search to support your
systematic review and meta-analysis. That includes developing search
strategies for large, electronic, bibliographic databases. We've introduced an approach
that includes breaking your research question into
components using PICO. We've covered the variety of
electronic and sources for peer reviewed literature and
gray literature and the other key point, starting from the beginning is document
what you do when you do it and you will be well on your way to
producing a great systematic review. Thank you for listening and
talk to you soon. [MUSIC]
[MUSIC] Let's look first at the steps
in a systematic review. You know this picture well. It shows where we are in discussing how to
do a systematic review and meta-analysis. And we're at step nine and
ten, developing the forms for assessing the 'risk of bias and
extracting the data. And then,
assessing the risk of bias itself. Number nine developing forms for
assessing the risk of bias and extracting the data and
number ten assessing the risk of bias. Let's begin by talking about why
bias in the individual study is important to a systematic review and
meta-analysis of a group of studies? What do we mean when we say risk
of bias in systematic reviews? I think most of you would think ahead and
say well, when we do a systematic review and meta-analysis, we
only want to include high quality studies. Well that's what we mean. We dont want there to be bias in the
methods that are used in the studies that we include in our systematic
review of meta-analysis. That is we want a low risk of bias. We've talked about the fact that you
want a low risk of bias in the studies themselves and you also want a low risk
of bias in the systematic reviews. Today, we're just going
to be talking about bias in the methods used
in the included studies, which some of you might think of as a
quality of the studies that are included. So, our objectives today are to
define the sources of potential bias in studies included in
the systematic review. To look at the potential impact of
bias on the summary results and to discuss how to deal with potential
bias in your meta-analysis. Now all of what I'm about to
say wouldn't make any sense and we wouldn't need to worry
about the risk of bias, if it had no impact on the results
of the meta-analysis itself. That is when we summarize all the studies
together, if there is no change in the quantitative findings from the
meta-analysis, then we know that the risk of bias has had no impact on
the results of your meta-analysis. On the other hand, we never know
whether that's going to be true or not, and we have to assume that bias
could have an impact on our results. And that's why we're so worried about it,
and we want to make sure just to include what we would call high quality studies,
or studies at a low risk of bias. So what is study quality and
why aren't we using that term anyway? Why are we using a term like risk
of bias which is convoluted and kind of hard to understand? So, I think what we mean when
we're talking about study quality is we're talking about the methods
used to conduct the study, the way the study was actually done,
but we don't know that. All we know from reading a journal article
or looking at clinicaltrials.gov, or wherever we get our data,
is the way that the study was reported. That is the way people
wrote up the paper or the way they entered the data
into clinicaltrials.gov. So, we're never able to find out
how well the study was done. We're just able to find out
how well it was reported. And I will mention at the end and
I'll mention now, that there are reporting guidelines to make sure that
that study is reported well. The CONSORT statement for
reporting clinical trials, and the STROBE statement for
reporting observational studies. I'll talk about that again at the end. So some of the elements of study quality
can be assessed by reading a study report, for the most part. That is we can asses the internal
validity of the study. And as you've learned in your
first year epidemiology class, this means minimization of bias. This means decreasing that risk of bias
that I'm going to be talking about today. External validity is how well your
study relates to the outside world. Some people say whether the study
findings are generalizable. Some say whether the study
findings were applicable. But that's what external validity means. We can usually tell that
by reading a report. We can usually tell how relevant the study
is to the questions that we have, how original it is compared to other studies
that are done, and whether the study appears to have been done with ethical
constraints that we all would accept. For example, has an institutional review board
approved of this study being conducted. But there are parts of the study
quality that we can assess. We have no idea usually what the protocol
violations have been like in, for example, a randomized clinical trial. Usually, this isn't reported in the report
from a randomized clinical trial. We don't know how well they kept records, we usually do not know
what the forms look like. We don't know how often errors had
to be corrected in data entry. What about study procedures? They may say that they measured,
let's say visual acuity a certain way, but we don't whether visual acuity
was actually measured that way. How much does the report,
in fact, reflect the actual study? We have no way of knowing that. People have talked about that
as being problematic, but we really don't have a way for accessing whether the report reflects
the actual study and how it was done. And we certainly have very
few ways of finding out whether the data were falsified or
fabricated. That's something that every now and
then we hear about. And we just hope and pray that the results we're reading
are totally honestly reported. So there's a lot about a study that we
can't tell from reading in the report. But what we're going to talk
about here is what we can tell, those first elements of bias in
the internal validity of the study. So the studies that you include
in your systematic review and meta-analysis should be unbiased or at least you should minimize the bias
in the studies that you select. Now what does bias mean when we're
talking about intervention studies? There are three main types
of bias that I'll mention. And I'm just going to mention
the names of the bias themselves, but you have to understand what they mean. We can't just rattle off the names
because these names are used generically to talk about bias. The first is selection bias,
the second is information bias, and the third is a bias in the analysis. It's not really called analysis bias, but
I'm trying to make it understandable, and I'll explain a little
bit more as we go along. Now we're going to take a break, and
when we return we're going to talk about minimizing bias in the included studies
and specifically selection bias.
Welcome back to section B. In this section, we're going to talk about
minimizing bias in the included studies, specifically selection bias. So here's a pictorial graph
of what I am talking about. Selection bias, which you can
see on the upper left hand side. Usually refers to a bias in how
the treatment was assigned. The first thing we want to look for is,
was there random sequence generation? Was the trial randomized? And, then,
also related to selection bias is, was that randomized sequence,
somehow concealed from those who will be doing the assignment to
protect against selection bias. I'll explain a little bit
about that in a minute but those are two separate ways that
we protect against selection bias. Random sequence generation which is
usually done by computers this days and you can do it other ways too, and then concealing that sequence from
those who are doing the assignment. Next, we're interested
in information bias, and that is getting the correct
information from the patient, from the doctor and
recording it correctly, accurately. Reflecting what actually was
done on a form of some sort. Now this is done by masking, or blinding. Those are two words
meaning the same thing. I tend to use the word masking,
because I do a lot of vision research, and we don't like to use the word
blinding to talk about this, but you may be far more familiar
with the term blinding. So at least three different groups can
be masked in a randomized trial or in a trial. The patient, him or herself,
can be masked into what intervention he or she is getting. The person, giving the caring
such as the doctor can be masked as the treatment being received. And those who are assessing
the outcome can be masked. This protects against information
bias being a part of your study. For example, if I know I'm getting
placebo I might be more inclined to say that I'm experiencing a bad
outcome of some sort, or a side effect. I know why I'm getting the test
intervention and my carer, my doctor, might also be inclined if he or she knows
I am getting a placebo to get me another drug, because I couldn't possibly be
getting better if I'm getting the placebo. Let's say an outcome assessor
is looking to see how well I see after having a particular
procedure on my eye. That outcome assessor might push me
a little harder to see more lines on the Snellen eye chart if they know
that I haven't had the procedure. Or if they think I have
had the procedure and want me to see better with the procedure
compared to the control group. So you can see how it's important that the
patient, the person giving the healthcare, and the outcome assessor, are protected
against bias by a masking, and not knowing what sort of
treatment the patient is getting. And then finally we want
the analysis to be unbiased. There are a couple of different
ways to do that,one is we want to use what's called an intention
to treat analysis and you've probably learned about this in your first year
epidemiology and statistics classes. What that means is that once
randomized always analyze and that very first analysis
in a randomized trial is people assigned to a certain group,
stay in that group. They don't switch groups no
matter what they really got so that the analysis is unbiased. And that's called an intention
to treat analysis. We also want to make sure that
we use pre defined outcomes and not switch outcomes
depending on what we find. If we use pre defined outcomes that we
defined before the study has even started then our readers and those assessing
our results can feel more assured that we haven't switched the outcomes
based on the study results. So those are the three types of bias we'll
be covering, selection, information, and analysis bias and where they actually
have an effect during the process of not only assigning patients to treatment,
but on assessing the outcomes as well. So let's talk briefly about
random sequence generation. I did already mention that this
is how patients are assigned to a particular intervention, and it happens before the participants
actually know that they've been allocated. It happens in the background. In studies I've done, for
example, it's happened a month or two before we actually started enrolling. The computer generates the random
allocations just to make sure everything's right. And we check the programs a couple of
time to make sure things are really randomly assigned. The benefits of random assignment is
that they account both for known and unknown confounding variables. That is, in an observational study
comparing two interventions, we know what to look out for. What might be potential
confounding variables. But there are unknown confounders too. And we can't possibly make the group
similar on those if we don't know what they are. So the advantage of randomization is
that if there are systematic differences between the groups, and you have a big
enough sample, theoretically, there's no difference between the two groups on both
known and unknown confounding variables. That is, it prevents selection bias, a differential selection depending
on which treatment you get. So do we know this exists? Oh yes, we know this from so many studies. I debated whether
to put
a study in here in this slide from a more recent study because
it's being shown all the time, but. I use this slide from 1983, just to point out how long we have known
that randomization really matters. This is an example comparing
the coronary artery bypass surgery using both randomized trials and what are called quasi-experimental
studies in this particular publication. What that means is observational studies. And this was a meta analysis done
comparing studies that were randomized and studies that were not randomized and the authors looked at mortality
in the medical group. One group was receiving medical treatment
and the other group was received surgery, that is coronary artery bypass surgery,
and when they looked at the differences in mortality, there was a much smaller
difference in smaller standard deviation in the randomized control trials than
in the quasi expirmental studies. Look, it's 4.4% versus 13.8%
mean difference in mortality. And a bigger standard deviation for
the quasi experimental group. What does this mean? This means it's probably
selection bias at work. That is, by randomizing the patients and not knowing which patient was going
to be assigned to which treatment we are avoiding confounding by
both known and unknown variables. That's why, when you use
the quasi experimental design, you see a big difference between
medical and surgical treatment. And you see a much smaller difference, probably one that doesn't make much
of a difference at all, between the patients who got medical treatment and
those who got surgical treatment. So does randomization
protect against bias? I used to think oh well, we can just
take the observational studies and use a multiplier of some sort, if we
knew that, let's say, their estimate of effect is, on average, three times greater
than the estimate of effect you'd get for randomized trials, but
that doesn't really work. This is a systematic view of meta analysis
of studies that have compared the results when you use randomized trials versus
when you use observational studies. And as you can see from these estimates,
this is the mean difference here of zero. Sometimes the randomized trial
has a larger effect, and sometimes it has the smaller effect,
which is what we expected. And often there is no effect what so ever, that is the 95% confidence
interval crosses zero. And so there's no consistent difference
between randomized trials and observational studies, so there's no
such thing as a multiplier we can apply. We think that randomization
protects against selection bias but we can't tell what direction
the association will be in. And so there's no way we can really
say what the size of the effect is. What we do, though, to protect against
the possibility of selection bias, is that we say that when the assignment is
unpredictable, there's a low risk of bias. And unpredictable methods for assigning the treatment to one group versus the
other are use of a random numbers table, such as you might have on
the back of your statistics book. Computer random number generator that's
computerized some sort of stratified or block randomization, which probably
uses the random number generator using some fancy statistic for
stratification or block randomization. Or even a coin toss is considered low
risk of bias as long as no one cheats. Now I have to say that I might not
consider that low risk of bias because of the tendency to just throw it another
time or throw the dice another time. But theoretically it is as good
as randomizing using a computer. They're also methods of assigning
treatment that is of the generation of the treatment assignment,
that are considered predictable and put a study at high risk of bias. That's what's in red here on this slide. Those methods
are considered,
some of them, quasi random, this is a different use of quasi
random then we saw on the other slide. So that's confusing justifiability. For example you might see,
we assign patients by their date of birth. Those who were born the even year got X,
and those who were born in a odd year got Y. That's theoretically random, the trouble
is you can fool around with it. The day of the visit, the patient ID,
whether it's an odd number or an even number. You can use alternation. All those methods are meant to
be as good as randomization, but the trouble is they can
be predictable also. So they aren't considered good methods
to assign patients to a treatment, and then there are totally
non random methods. For example, a patient or participants or
clinician can make the choice as to what intervention the patient is to get and
this non random method probably puts the study at
high risk of selection bias. For example, with a new treatment, we might not put sicker patients
in the new treatment group, we might put the patients who are doing a
little better in the new treatment group. Not because we're bad people or deliberately trying to skew the results,
but it's just human nature, how we might assign participants
to a particular intervention. Now, allocation concealment. This is a complicated concept, and one
that people have a lot of trouble with. They often mix it up with masking or
blinding, but it's not the same. Allocation concealment happens
at the start of a trial as the patients are being
allocated to their treatment. So when you recruit a patients to
a study you don't know ahead of time how they're going to be assigned and
to what intervention. And you shouldnt be able to
know until exactly the moment when they're assigned
to that intervention. So here's an example on the right hand
side of something that believe it or not, does happen. I might have a list that was randomly
allocated of assignments c or d. And I have patients who come in,
in a particular order. And the patient who comes
in first gets allocation C. The patient who comes in
second gets allocation C. The patient who comes in
third gets treatment D. So if this is up in the wall, I can see that my fourth patient
is going to get treatment C. Now if I believe that treatment C
is in fact the new treatment and treatment D is the old treatment,
I might say to person number four, who happens to be the husband of my
next door neighbor, I might say why don't you sit down, I'm going to take Mr.
Smith next, who's actually number five. And make sure that number five gets C,
because I don't believe C is as good as D. And then, my next door neighbor's
husband is going to get Treatment D, because they've switched places. I can see what the treatments
are up on the wall. So even though these treatments
have been randomly allocated, that is the order has been randomly
allocated, I can see what comes next. That is, the allocation is not concealed. And in that way,
why did you even bother to randomize? Because I can see what's next and
we could have selection bias. That is patient's assigned to
a treatment based on some characteristic that they have. Does allocation concealment
make a difference? Yes. This is an area where people
really are worried about the possibility of
subversion of randomization. And skewing of the results. This is a systematic review that was
published in 2008, by Pildal and his colleagues. And what they did is they took
various studies that examined whether randomization and allocation concealment made a difference
in terms of the size of the effect. In the meta analysis. Now each of these studies, and you can see that there are seven of
them, looked at another group of studies. For example,
Schulz looked at 250 trials and got a ratio of odds ratios that is,what
would be the odds ratio that you get if you had allocation concealment
that was adequate divided by trials that had allocation concealment
that was not adequate or was unclear. And what they found, on average,
was that trails with unclear or inadequate concealment showed a more favorable
effect of the experimental treatment. So this is very worrisome. And for this reason people advise that
you either not include or do something in your analysis that takes account of
all the trials that you've included, if you do include them, that had inadequate
or unclear allocation concealment. In the same year, Wood and his colleagues
examined, a little bit more closely, where there might be a problem with
inadequate allocation concealment. And what they found is that adequate
allocation concealment makes the biggest difference, when one is
examining subjective outcomes and outcomes that are not mortality. Presumably, they are more subjective. For
example, breast cancer mortality,
prostate cancer mortality, cardiovascular mortality. Assigning that cause of death. Probably is somewhat subjective, and
when you have subjective outcomes,
it appears that inadequate or unclear allocation concealment is
a problem in terms of potential bias. What they found is that studies that
are inadequately concealed had a more beneficial odds ratio than studies
that were adequately concealed. So this is where some
of the focus has been. That if you're looking at
mortality,all cause mortality, then perhaps, the allocation concealment is less of a potential problem than
if you're looking at other outcomes. Now most studies do look at outcomes
in addition to all cause mortality. So one could correctly surmise
that allocation concealment is an area of potential risk of
bias in any randomized study. So what we consider a low risk of bias for allocation concealment when we're
examining a trial that we consider possibly to be included in
our systematic review and meta analysis is if there's
central allocation. That is, if one has to call into
a coordinating center by telephone or dial in using the Internet,
or perhaps going to a central pharmacy and asking for
the next assignment for the patient. And this is why some people confuse
allocation concealment with random generation of
the sequence of assignments. That is, maybe the assignments are done
at the coordinating center by a computer. But the allocation is done
by calling in by phone and finding out what the next
patient is assigned to. Now some studies use sequentially
numbered opaque envelopes, and some use sequentially numbered
identical drug containers. These are also considered
fine if you want to label a study at low risk of bias
due to allocation concealment. However, there are some studies that
you would want to consider at high risk of bias. That is, allocation was predictable. The random sequence might be
known
to the staff in advance for example, day of the week or year. The envelopes or whatever packaging is
used may not have all the safeguards. That is, envelopes may not be opaque. You may be able to hold them up to the
light and see what the next assignment is. Any kind of predictable or non random
sequence puts a study at high risk of bias due to failure to conceal
the allocation adequately. That ends section B.
Welcome back. Now we're in Section C. In this section we talk about
minimizing bias in including studies. That is, we're going to specifically
be talking about information bias. So when we talk about information bias, whether we're talking about random
bias trials or observational studies. We're talking about getting information
that may not be exactly true from the patient, from the doctor, whoever
is filling in the data collection form. And, what we want to do is prevent or
avoid information bias, because we want all the information that we record, in
any kind of study we do, to be accurate. In fact, the study depends on it. In deciding whether there's an association
between an intervention or exposure in an outcome, depends on the information
about both being accurate and correct. In a randomized trial,
what we do is we mask or blind the participants in the study,
the doctors or other healthcare providers, and the people who are assessing
the outcome in the study. Because participants, for example,
have different expectations and behaviors, they may give out
different types of information. Certainly the way I perceive pain may
be different from the next person. We just experience pain differently,
or we express it differently. But we want it at least to be expressed
correctly for how I perceive that pain. We want it to be unbiased. That's not to say that
it's correct on some global standard of zero to ten,
but it is to say it's unbiased. It's how I perceive pain. And for me to say how I'm perceiving pain, it's important that I not
know
what intervention I'm receiving. Because that might affect
how I perceive pain. So when you're looking at an article,
as you may be, to do a systematic review, it's important
that you look to see whether there might be information bias in the way that
information is transmitted in the study. For example, many studies say,
this was a single masked study, or this was a double masked,
or double blinded study. But that really doesn't tell you anything. Who's double? Who are the people who were blinded?
Were they the patients and the provider? Were they the patients and
the outcome assessor? So it really doesn't tell you that much. We want it to. We want to know what they meant, but
there really is no way of finding out
what they meant when they wrote this. And so, it's unclear, for example, whether a study was blinded in the way we
want it to, to prevent information bias. Do we know whether masking or
blinding was broken in any way? Were there any patients who
found out what they were taking? Or was it possible to guess, for example, if the two interventions didn't taste
the same or didn't look the same. And so
think about the impact of masking or of information bias on your study results. Now some studies it's not
possible to mask or blind. For example you might have
a surgical study where people have two different types of surgery. Or one group has surgery and
the other group does not. It's too bad that we might ding
a study because it's unmasked and it wasn't possible to mask. And you have to take that
into consideration, as well. So sometimes it's just not possible. Other times, and I've worked in studies
like this, a surgical study for example, where one group of patients got the
surgery and the other patients did not. Even though it wasn't possible to mask the
provider, for example, or the patients, it was possible to mask
the outcome assessors. And so they didn't know, after the surgery
had been done at some time and you couldn't see the scars anymore,
it was eye surgery, they didn't know what surgery the patient
had had or even if they had had surgery. And so in that way,
the outcome assessors don't know. And as long as the outcome,
for example visual acuity, is being measured and written down
by an outcome assessor who's masked, it doesn't matter really
if the doctor knows or if the patient knows, unless in fact
the patient is acting differently. So assess this carefully. It's a very difficult area to tell just
from reading a paper, but it's very important in terms of your assessment of
the possible risk of information bias. So we consider studies at low
risk of information bias, if there was masking, and it's unlikely
that the masking could have been broken. And again, you have to say who was masked. And sometimes there was
incomplete masking but the outcome is unlikely to be influenced,
for example, death. So for example, the study may have been
unmasked in terms of whether a patient received surgery or not. But if the outcome is death,
it's unlikely to have been influenced by knowing whether the patient had surgery or
not. At high risk of bias are studies
that are not masked or not blinded. Those that have broken masking, that is,
it was easy to break the mask and see what treatment you were getting or
the patient was getting. And where the outcome is
likely to be influenced. For example, an unmasked study
where the outcome is pain, is highly likely to be influenced
by knowing what you got. I know I would be influenced knowing I
had placebo versus an active treatment. So another meta-analysis was done
by Peldel looking at the effect of double masking, or double blinding,
on the estimate of effect. And you can see here, there are seven
studies that were done, and again, there's a ratio of odds, ratios
in studies that have double blinding, or double masking versus those that didn't. And what they found is that
trials without double blinding tended to show a more favorable
effect of the experimental treatment. And this confidence interval
just barely touched one. And so although there is
a statistically significant effect, probably, do you want to consider
that statistically significant? It's a smallish effect, and
I think that's why people put less emphasis on information bias
than on selection bias. They put less emphasis on masking
than on allocation concealment, even though masking is important,
for obvious reasons. And it's actually more intuitive,
I think, than allocation concealment. As you might expect, Wood and
his colleagues did a meta-analysis that examined the effect
of masking on odds ratios. And similar to the other results for
allocation concealment. What they found is that when
they're subjective outcomes or mortality other than all cause mortality,
there tended to be a potential bias, where studies that were not blinded
tended to be more beneficial for the test treatment than
studies that were blinded. So again, I think when a study is looking
at something at all cause mortality, other than objective outcomes,
the importance of blinding or masking the participants,
the health care providers, and the outcome assessors
becomes more important. And this is probably typical
of most studies today, where we're looking at quality of life and
outcomes that matter to patients. That ends section C.
We're now going to move on to Section D. Minimizing Bias in the Included
Studies: Bias in the Analysis. This is the last area where
bias may be important in the studies that you're including in your
systematic review and meta-analysis. There's nothing that's
really called analysis bias. So, I sort of weaseled
around the terminology here by calling it bias in the analysis. I think you'll know what
I mean as we go along. The things that you're concerned about and you no doubt learned about in the first
year at epidemiology and statistics class. Is that the following things can
increase the bias in the analysis of an individual study. If there are losses to follow-up,
this can increase bias. For example, maybe those dropping out or
everybody who got better, or everybody who got worse. And maybe you tend to get better if
you're in the test treatment group. Maybe you tend to get worse if
you're in the test treatment group. We often can't tell which direction
the bias is supposed to go, but nevertheless, it is a potential bias
when there are high losses to follow-up. Similarly, I think it's intuitive
that those who don't comply with the treatment they're assigned
can bias the analysis. For example,
if we have a surgical trial and people are assigned to one type
of surgery versus another. And many of those allocated to type A
surgery decide not to have the surgery and either to have type B or
not to have surgery at all, you can see how that
might bias the analysis. That is, if we drop them out entirely. Why didn't they have the surgery? Maybe because there's
something about
that surgery we need to know or maybe they're analyzed with the other group,
which could bias that analysis as well. So noncompliance is a very
complicated analysis but we do know that noncompliance
can possibly bias the analysis. Similarly, withdrawals. Some people withdraw from a study, and
they either stop taking the treatment, or they completely withdraw. They don't come back for any visits. Those people
can bias the analysis or
lead to bias in the analysis. And then, as I mentioned earlier briefly
and we'll talk a little bit more about it here, there can be changes
in the outcome measure. That is, as we're going along let's
say I measured pain two possible ways using two measures. And I said, I could see that using one
of the measures there wasn't going to be a difference between the test treatment
and the comparison treatment. And so I switched the pain outcome
measure that I was planning on using to one where
a different stitch show up. Wasn't a big difference, but it was
a statistically significant difference. So this so of post hoc defined outcome
can lead to bias in the analysis. So missing data is an increasingly
recognized big problem in your analysis. Unfortunately, there's no simple rule. So you might ask, well,
if we come across a study and there's been 10% dropout or
missing data, what do we do? Should we eliminate that study from our
systematic review of meta analysis? No, probably not. What about 25%? Gee, that's not so good but I don't know,
I hate dropping studies out. What about 50%? Well now I'm getting a little bit worried. So you have to think about
what's
the question you're asking, what's the overall
proportion of missing data. And what kind of affect
are you looking for and how could missing data affect this? Now you have to make all of these
decisions before you start. You can't make those decisions on
the basis of seeing the data and deciding whether to include or exclude a study
because it has a lot of missing data. What we do know is that missing
data can influence our findings and that is worrisome. What we don't know is when some
missing data is too much missing data. That's the hard part to say, and it's a judgement call,
like a lot of what we're talking about. Now some people say well,
the amount of missing data was balanced between the treatment group and
the comparison group. So, that's probably okay, right? Or, the amount of data was similar and
when we looked at the characteristics of patients who
had missing data, they were similar so it's probably okay to include these
studies and assume there isn't bias. No, you cannot know, ever, if missing
data has influenced your findings. You just have to make that
judgement call ahead of time. You can certainly look at the data, see
if it's balanced, see if the two groups are similar who are missing data, but
it doesn't mean that everything's okay. So you might want to do what we call
a sensitivity analysis, with and without that study, and see whether your findings hold up based
on whether that study is included or not. So this post hoc definition of outcomes
is becoming more and more important. We did talk in an early lecture
about selective outcome reporting. And I think since 2004, so
that's in the last ten years, people are realizing that
this might be a real problem. That investigators are changing the
outcome, sometimes quite innocently and not realizing what a bad idea it is,
and sometimes not so innocently. They're changing the outcomes based
on the results that they find. Now I gave you one result of
measuring pain in two ways, and selecting one because it had a beneficial
effect shown where the other one did not. Another way that outcomes can be changed,
is you can say, well we're going to change the time
point at which we measure the outcome. That's our primary outcome. So a great deal of emphasis
is being put now on what's the primary outcome that was pre-specified
before the beginning of the study, and what are the secondary or
other outcomes that were pre-specified. Now, I'm not going to get into all
the details, but by pre-specifying these outcomes, you are protecting
against what we call type 1 error. And by pre-specifying, we're saying
this is the main focus of our study. This is the outcome we use
to estimate sample size. And type 1 error that is saying that
there's a statistically significant difference when there
really isn't a difference between the two intervention groups, you
do not want to make that kind of error. And, so by switching the outcomes,
you're putting yourself at higher risk. You could have a hundred outcomes
you're examining, for example, the associate between your intervention
and a hundred different outcomes. And, by chance, you're likely to
find an association then with five of the outcomes and the exposure or
intervention that you're looking at. So, that's why you don't want to look at many different outcomes
that are not pre-specified. I'm not going to go into a lot detail. We're happy to talk to you
about this further, but this is something from epidemiology 1,
and your first course in statistics. So let me give you some examples though,
of the problems we're up against, when we look at data either in
journal articles, or for example, the data from clinicaltrials.gov. This is from a paper published in
2011 by Deborah Zarin that describes clinicaltrials.gov and the outcomes
that are specified in that database. So that database asks the investigators
to say what are your primary outcomes and what are your secondary outcomes? Now what we would call
an outcome is called a domain. That's the big picture for example. Visual acuity, or all cause mortality,
or quality of life. That's your primary outcome domain. And what they found is,
that a median of one primary outcome domain
was specified per trial. However, the range of primary
outcomes specified was 1 to 71. That is, some people specified almost 100,
three quarters of 100 primary outcomes. Well, as you know, if you test for an
association between your intervention and 71 primary outcomes you're
highly likely to find a statistically significant
association just by chance. What about secondary outcomes? There's a median of 3 secondary outcome
domains specified per trial and the range was 0-122. This is too many. I don't know what the right
number of primary outcomes or secondary outcomes is, but
it's not 122, and it's not 71. It's a much lower number than that. And there's a lot of debate going on right
now about what is that correct number. It's very hard to say, especially now given that we want outcomes
that are relavent to doctors, to patients, and they could be measured in the short
term as well as over the long term. I'll say a little bit more
about outcomes because it is such an area of
concern in modern times. There's not much detail that people go
into about what their primary outcome is. And this is concerning because you want
to know how they measured that outcome. You might be doing a similar study and
want to know what outcome measure to use. But in doing a systematic review of meta
analysis you definitely want to know whether these data can be combined. So for example, in outcome measure
descriptions
that were on clinicaltrials.gov, the descriptions went from very vague,
such as anxiety to a little less vague. Hamilton anxiety rating scale to
even a little less vague than that, Hamilton anxiety rating scale at
12 weeks to something that was much more detailed and
much more helpful to be truthful, such as proportion of participants with a
change of greater than equal to 11 points at 12 weeks from baseline on
the Hamilton Anxiety Rating Scale. Now one still wonders what baseline was,
but okay, that's pretty good. So if you're doing a systematic
review of meta analysis, you want a lot more detail than anxiety or
Hamilton Anxiety Ratings Scale. You want to know when it was measured,
what was considered a change if you're looking at a change, it was
a change between when and until when. And so you want a lot of detail to
help you do your meta analysis. Here's another example. What clinicaltrials.gov does,
and now were expecting this for systematic reviewers as well, and
you will see something like this in the Cochrane Handbook, is that we're
expecting people to talk about the domain. As I mentioned,
it could be anxiety, visual acuity, pain, that sort of the big
picture of what your outcome is. How it was measured, anxiety could be
measured by the Beck Anxiety Inventory, the Hamilton Anxiety Rating scale,
a fear questionnaire. What metric they used. In the example I just gave,
they use a change from baseline. But, you can also say, time to event, like, time until death you might use,
if death is your domain outcome. Some n value, you might not use change,
you might use whatever the rating is at that anxiety rating scale
at 12 weeks, for example. How you aggregate the data,
are they categorical data or are they continuous data. Are you going to look at a proportion or
are you going to look at a mean or a median. And in our example we use the proportion
with a decrease not of greater than or equal to 50%, but it was, I think,
greater than or equal to 11 points. And then you want a time point. And that time point might be one year,
it might be six months, it might be one month,
it might be eight weeks. And you want a time point. In clinicaltrials.gov,
time point is not a separate level. It's not what's called a separate element,
but it's an overarching element which each
of these elements have to be measured. But I think the movement now is to
move it to five separate levels, so that each outcome has to be specified
in these five separate ways. This is more detail than I'm sure
you're asking for at this point. And that has directly to do with the risk
of bias, but I think it helps you to see how one could change an outcome quite
easily without anybody knowing, if you don't report very much about
when or how you measured your outcome. So again, this is from the same article in the New England Journal of
Medicine about clinicaltrials.gov. And what Debra Zarin found in this
study was that only about a third of the studies reported on clinicaltrials.gov
reported the domain only. That's really not very much. We hope that you get more than
domain if you're trying to avoid bias in the analysis. If you really want to know
what that outcome was. Was it pre-specified, and
exactly how was it pre-specified? About two-thirds, nearly two-thirds,
did specify a time point. That's very good, but
we didn't know much about how the outcome was measured, the metric that
was used, or the method of aggregation. And so
I think you can see now why outcomes and selective outcome reporting have become
such an interesting question for those that are concerned with the risk of
bias in the individual studies included. This is a huge problem and something that
people are trying to address in a number of different ways, including open
access to clinical trial data. In that way, if this information is
not reported in the journal article or in clinicaltrials.gov one could theoretically go into the original
data set and find out. Why don't we take a short break and
when we return, we'll talk about assessing the risk
of bias in observational studies. I've been talking about
randomized clinical trials, and we'll move on to observational
studies after the break.
We're going to talk in this
section about how to display study quality in your systematic review. And you may remember, I said that
there's no such thing, really, as study quality since there's so much that can
be measured and unmeasured in a study. And also since we're depending so
much on the study report, we don't actually know what
happened in the study. Nevertheless, you're going to
want to somehow display what you found about study quality, or
risk bias, or controlling for minimizing the risk of bias in
an individual study, because this is a concern that most readers of systematic
reviews and meta-analyses have. That is they're concerned
about garbage in, garbage out. And they don't want your systematic
review to be full of garbagey studies. So the first thing I want to say,
and I'm saying it really loud, and that's why it's in red,
is please do not use quality scores. Quality scores were used 20 years ago. You will even still see them
used in Cochrane Reviews, which I don't understand at all. But quality scores are not used anymore. It's tempting. It's
really nice to assign a score to
each study based on what quality you think it is. But the fact is, they don't work, and I'm going to tell you
a little bit about why not. There's some quality scores that
are what we would call generic. That is, you can use them for
all research studies, certainly all research
studies of a certain type. That is, you might assign
a score to randomized trials, that would say, for example,
was there random sequence generation? Was there allocation concealment? Was there masking? And it was an intention
to treat analysis done. And then, give it a score of four if
the answer is yes to all those questions. The score is what the problem is,
not the individual question. You will also see some quality
scores that are specific to a particular field like emergency
medicine or cardiovascular disease. But again, you don't want to use a score. It's fine to use a particular system for
assessing the risk of bias,
but not a score. And one of the reason is, how do you
find out whether this is a valid score. What does four mean? There's no way to validate that score. All you can do is look
at the individual
questions that are being asked, and whether the study appears to
have addressed that question or problem correctly. You can't assign a number to it. These numbers aren't reliable and
they can't be validated. So if we see any quality scores
in your systematic reviews, we will not consider that good. This slide shows an example of
the Cochrane, risk of bias summary. What it shows along
the side is six studies in how each was assessed in terms of the risk
of bias in that particular study. And the elements that were assessed
are adequate sequence generation, allocation concealment, blinding,
in this case, for subjective outcomes and mortality, incomplete
outcome data assessed, and short-term outcomes and long-term
outcomes, and then two other categories. So the first study, for
example, Barry, 1988, was judged to be adequate, based on that
plus sign on adequate sequence generation, but not adequate based on the minus
sign under allocation concealment. Barry was also adequate based on
the plus signs for blinding both for subjective outcomes and mortality, but
not adequate based on the minus sign for the other possible elements
of risk of bias, and so on. And if you sort of squint and look at this table overall,
you can see that all of the studies use adequate sequence generation based
on the fact they all use plus signs. That's actually to be expected
because in a Cochrane review, often, randomization is
a requirement to be included. But for allocation concealment, three of the studies had
adequate allocation concealment. Two had question marks,
therefore was unclear whether they did. And one, the Barry study did
not have adequate concealment. So, this is a good way to
demonstrate whether your studies are at risk of bias, and
if they are, at what risk of bias. So, I can see, looking quickly,
that the Goodwin study faired pretty well. Well, the Cooper study probably didn't
fair very well, and Sanders, as well. Now, this is just presenting the data. It's not saying anything about
how it's going to be handled in this particular systematic review. I did mention that sensitivity
analyses could also be done where you do the first analysis with
all the studies that you've included. And then you withdraw studies
based on some element. For example, risk of bias,
the method of randomization, allocation concealment, masking,
intention to treat, and so forth. So, you could withdraw studies and see whether that estimate stays
sort of firm, even when the studies that do not meet the minimum risk
of bias criteria are withdrawn. So, here are the types of questions that
you can ask in a sensitivity analysis. What happens if I exclude
studies at a high risk of bias? What if I use different
inclusion criteria? What if I use different
exclusion criteria? What if unpublished studies
were included or not? What if conference abstracts
were included or not? And what if a study lost a different
proportion of randomized patients to follow-up? So, what if I excluded all
studies that lost more than 15% of the randomized patients,
what would happen to my results then? So if my conclusion remains
robust no matter which studies I withdraw, assuming that each time
it's a fairly small number of studies, then that's a good sensitivity analysis. And that says that my conclusion is robust
no matter which studies I include and one is reassured. If in fact conclusions change when
you take away some of these studies, for example, the unpublished studies,
one might be concerned about possible reporting biases that
should be taken into consideration. So I said earlier in this lecture
that I would mention the CONSORT and STROBE statements. These are statements that are very
important to follow these days for proper reporting of clinical trials and
observational studies. They don't guarantee that
a study was as well done as the report may indicate, but
it certainly is a good beginning. You wouldn't choose not to report a study,
well, just because of the flaws it might be hiding behind
the mask of a good publication. So hopefully the studies that you
include in your systematic review will adhere to the CONSORT statement or
the STROBE statement. Or some other statement depending on
the type of study design that you use. The CONSORT statement, this is from
2001 and it's constantly being updated, but it's stayed pretty much
the same over the years, and it's for parallel-group randomized trials. One of the most important parts
of the CONSORT statement, is that every randomized trial
should include a flow diagram, of how patients progress through
the phases of a randomized trial. And this can help you a good deal. So you see how many patients were
randomized, how many actually took the treatment they were assigned, how
many were lost to follow-up at each stage, and how many were handled in the analysis. This is very, very helpful if
you're doing a systematic review. And so I wish you all,
if you're doing randomized trials, that the studies you find adhere
to the CONSORT statement. The STROBE statement is for
observational studies. It's a very long article. This is just the statement itself. But what's called the E and E,
the explanation and elaboration, goes into why each element is there,
and is very, very helpful for learning. So this is guidelines for
reporting observational studies. So in summary, what I've talked about
here is that what we all want to know, which is that the systematic
review includes studies that were generally of a high quality
cannot be assessed. That is, there is no way to really
say that a study is of high quality, of low quality. We can't assess a risk of bias. And particularly in randomized trials,
this is pretty well worked out by the different elements of risk of
bias that I've talked to you about. With observational studies, it's more
difficult to assess the risk of bias, and has a lot to do with how the exposed
groups are defined and selected. And how the outcomes are defined and
determined. And it's much harder then to assess
personal bias in an observational study. But we will certainly
help you however we can. So the quality of a study is
related to its internal validity. An internal validity is how well
the biases have been avoided, how well the study is designed and
carried out. We don't know a lot about
how a study is carried out. We can only determine what is written
about it either in the journal article or in ClinicalTrials.gov. But nevertheless we do have clues in these
design elements, in its internal validity. We've learned that quality scores and scales have problems,
mainly because they can't be validated. There's no way of saying what
a particular score means. Nor that anybody else would
assign the same score. And instead, what we use today
is reporting of the components. That is,
how was assignment to treatment done, was that assignment concealed until
the moment that it was revealed. Were patients and doctors and outcome assessors masked to
the intervention that a patient got? How was the comparison group chosen? How is a case group chosen? How is
exposure defined? How was the outcome defined? So there's no question that even though
we have a hard time defining quality, and we use risk of bias as
our way of measuring quality, that this has to be considered
in interpreting the results. This is what everybody is concerned with
in a systematic review, a meta-analysis. We do not want garbage in, garbage out. And so,
what we do to try to use a quality filter is that we use a measure of risk of bias,
either to decide whether to include a trial in a systematic review and
meta analysis, or to exclude it. And we also perform sensitivity analyses,
where we do our analyses with and without a study where we might
have some questions about bias. So in that way, we account for individual
study quality in a meta analysis. That's all I'm going to
say in this lecture. I think it's a lot to take in. We are concerned about bias
in individual studies, and how that can affect your
meta analysis in particular. There's a lot being written and
a lot being learned all the time. So I would advise you,
if you're interested in this area, to keep up with it. It's a very important area in
terms of the legitimacy and validity of systematic reviews and
meta analysis. [MUSIC]
I was at Tufts University and
I happened to get connected with Thomas Trikalinos and
Joseph Lau, and Chris Schmid. And at the time I was working
on machine learning and natural language processing stuff. And it occurred to all of us jointly,
along with my advisor, Carla Brodley, that perhaps we could expedite some of the
processes that were being done manually in evidence synthesis using machine
learning and natural image processing. So ever since then I've
been working on that. The aspect that's received
the most attention is probably on the screening side. So this is where you're
identifying the literature, the evidence to be included in the
systematic review that you're performing. And there's been a lot of work that's
looked at using automation techniques, and in particular, automatic classification
techniques to expedite that process. And I think those technologies
are actually pretty well established. And I think the general consensus is that
you can reduce the labor by about half without sacrificing sensitivity or
recall to the relevant evidence, which is really important. So these tools, we've developed some
of them and other people as well, including for
example James Thomas and his group. Our stuff you can find at Abstrackr, so
it's free to use and it's open source. But again, that's just one one instance, and I think generally these kinds
of technologies have been helpful. The other bit that's kind
of more emerging, I think, are methods for automating the actual
extraction of structured information from publications describing the results
of trials and other findings. And this I think is still in the research
phases, but it's certainly something that I'm working on and
a bunch of other people are as well. But from my own plug I would say we have
this thing called RobotReviewer that we've been working on that
does a lot of this and tries to automate things
like risk bias appraisal. Julian Higgins has also done similar
things with one of his students. So I think there's a lot of
interest in this space generally. So it'll be exciting to
see where it all goes.
[MUSIC] Hi everyone. It's Kay Dickersin again, and we're
going to continue in our discussion about systematic reviews, and meta-analysis. I'm going to start out with a refresher
on standards for systematic reviews. In the past it's really been
every person for him or herself about what makes
a good systematic review. It's amazing how slowly the world moves as
I mentioned in the very first lecture for this class. In the old days,
we did review articles any way we wanted. No one taught us how to do them,
but we knew the general idea was to review the literature for
a particular question or set of questions. And then during the 80s and more like
the 90s for the field of medicine and public health, people started saying, hey,
we should do these reviews systematically. It's a double standard requiring certain
minimum standards for the primary research that we review, and then not
to care how we do the review ourselves. And so for that reason we move
towards doing reviews systematically. That is they have a method section, a
results section, and a discussion section. Standards have gradually risen and these
standards that I'm showing on this slide, shows that the institute of medicine
came out in 2011 with standards for systematic reviews, how to do them. And this is welcome
because systematic reviews are proliferating in literature and most
of them don't follow any set standard, so it appears people are just doing whatever
the next guy in their field did. So, let's just briefly review some of
these standards for systematic reviews. And in particular I'm going
to be interested in how to manage the bias of doing the systematic
review, how to minimize that bias. It's not bias in the individual
studies that are being reviewed, but bias in the process of
doing the review itself. What you see on this slide is a cover
of a book called Standards for Systematic Reviews, that was published
by the Institute of Medicine in 2011. And the Institute of Medicine gathered
together a group of experts in the field of medicine, public health,
systematic reviews and other topics, and asked them to help it develop
a series of standards that others and anyone interested in doing or assessing
a systematic review would follow. Some of those standards relate to
minimizing bias in a systematic review, and that's what we're going
to be focusing on today. In this slide, I show you the first
steps of initiating a systematic review. But in that case, we aren't really talking directly about
minimizing bias in that systematic review. Of course, we want to be careful to
minimize the conflicts of interest sitting around the table, and those who are
providing input to the systematic review. But that's not what we're going to
be spending a lot of time on today. On the next slide, we start getting into how to minimize
bias in a systematic review. Perhaps the most important step
in a systematic revue in terms of minimizing bias is to find and
assess the individual studies. One might imagine if all published studies were a random sample of all studies
ever done for a particular question. That just doing a quick look
in a filling cabinet or search of PubMed, might give you
a sample of all studies ever done and a comprehensive search for
studies would not be necessary. However, it is and I'm going to show you
some of the data backing up that claim. Comprehensive systematic search for
the evidence has to be done. And then, action has to be taken so that we address the problem of potentially
biased research results reporting. I'm going to talk about this and
managing the data collection, and how it relates to bias
in a systematic review. The following two slides
are other standards for doing a systematic review, but we aren't really going to talk about this
today, when we're talking about metabias. We will talk about these topics
in other context, however. This is how to report a systematic review. Again, we'll just touch on this briefly. So finally to
sum up the topics
that I've covered in Section A. Is that there can be bias in
conducting a systematic review; and what we're going to call this
in this lecture is metabias. We've already talked about bias and
the methods that are used in the included studies in the systematic
review in an earlier lecture. Today though, we're going to be talking
about bias in the methods used in doing the systematic review itself, or metabias. If you want to read more about this,
there's an article from Analysts of Internal Medicine 2010 and
the first author is Steve Goodman. I'm the second author. And this talks about metabias and
a little bit more about it, if it's a topic that interests you. There are three forms of medibias
that I'm going to talk about today. Selection biases, information bias, and
bias in the analysis, or analysis bias. And we're going to take these one
by one in the following sections.
Hi again. This is Kay Dickersin in the class
Systematic Reviews and Meta-Analysis. In the next section, Section B, I'm going to be talking about selection
bias in doing the systematic review. Recall that we also talked about selection
bias in the individual studies and this is also selection bias. But it manifests in a different way
when we're doing a systematic review and meta analysis. Probably the most important selection bias
or the way that selection bias manifests is a reporting bias, and there are many
different types of reporting bias, and that's why I think in an earlier
slide I called it reporting biases. I'm just going to talk about a few here. One of the most important reporting
biases is publication bias. And this is the case where there
are unpublished studies that tend to have different results than published studies. And we see that this is generally true.
I'll talk a litttle bit more about it in a
few minutes and why it's important to us. There's also selective outcome reporting,
which we haven't known about or thought about as long as
we have publication bias. That is,
the results from a study can be published, but only some of the outcome's reported. For example,
if one examine adverse events and there were some negative adverse events,
and some that were very severe and
some less severe, and only the less severe ones were reported then this would
be called selective outcome reporting. In the next slide I add a slightly
different type of reporting bias. And that is that studies
that are easier to find may have different results from
those that are harder to find. For example, we all know there
are studies that are pretty easy to find, that is studies published
in open-access journals, studies that are published in some of
the larger journals that we all know. But it might be harder to find
a journal that's not from the U.S. from conference abstracts, from books. These may be harder-to-find articles
that have published but since they're harder-to-find, you have do some
extra digging for your systemic review. And the final type of selection bias,
that's different from a reporting bias, is what we're calling inclusion bias. That is, what if the author of
the systematic review knows the results of many of the studies that are out there
before beginning the systematic review, and the author sets the eligibility
criteria for that systematic review. So the study is included or
excluded, specifically. Or perhaps, those inclusion criteria make a difference
in how the data are abstracted. This could affect the outcome of
the systematic review and meta-analysis. And so, it's very important to
consider inclusion bias as a potential meta-bias in systematic reviews and
meta-analysis. Let me give you an example
of inclusion bias. You all probably know that whether one
should institute mammographic screening for breast cancer in women under 50
is an area of great controversy and has been for decades. These studies are so well known that there
have been many systematic reviews and meta-analyses done by investigators who
set inclusion criteria ahead of time, knowing perfectly well which randomized
trials will be included or excluded. Or perhaps studies of
other designs as well, because they know the results that they
want to find Aand what the different studies do in terms of
influencing the final result. That's a metabias that has to do with
whether studies are included or excluded. So let's go back to the question
of publication bias and whether all studies are published. The first question is, is there any
failure to publish at all, and if so how big is that problem? So we know now from a number
of similar studies, that studies regardless of their design,
whether they're clinical trials or observational studies,
may not always be published. You can see in this listing of
various studies that have examined the likelihood of publication,
a broad range of rates of publication. So for example, clinical trials that came through
a Barcelona Hospital Ethics Committee show that only 21% were published by some
reasonable date that they examined. On the other hand, studies that came
through the Johns Hopkins School of Medicine Ethic Committee Were about 81% published, and
clinical trails funded by the NIH in the late 1970s have a very
high publication rate. About 93% of them were published. So you can see there's a broad range here, from 21% to 93%
published
that we initiated either as identified through an ethics
committee or through a funding agency. People tend to use
the number 50% publication. I don't think that's based on anything
exact or even a summary estimate. It's just looking at the data we have and
making a judgment. So for example, Johns Hopkins may have a higher
publication rate than a small hospital. And so,
there are differences not only in terms of where the studies were initiated,
but also who funded them. There's been a wonderful
systematic review done in 2009, 2010 by Fu Jen Song looking across a very large spectrum of studies
examining reporting biases. And I'm just going to show you a few
of his graphs and meta analyses. So what he did is he said lets look at
all the studies that examine whether positive results are more likely to be
published then non-positive results. And first we're going to start with
what he called inception cohorts, and that is all cohorts of studies that either
came through an ethics committee or came through some sort of funder. And so for example,
he has about 15 studies here started either in the 80s or
90s or even 2000s. And over that period of time, you can
see almost every single one has found that publication favors positive results. The one exception is the Stern
study done in Australia that had a point estimate where
non-positive results were favored, but the confidence interval does cross one and
go into favoring positive. So, it's possible that
those results include some of the results found
by the other studies. Overall, the summary estimate for
publication of positive results over non-positive results is
about an honest ratio of 3, meaning that positive results
are more likely to be published. Fu Jen Song also looked at what
he called regulatory cohorts, and that is all studies that came through
the FDA or other regulatory authorities. So look and see what percent of
studies that were part of a package given by the drug companies to
a regulatory authorities such as FDA, ultimately were published. And again publication favored
positive findings by studies done by drug companies and submitted to the FDA or
other regulatory authorities. The odds ratio in this case is about
five although the confidence intervals are much wider because they are smaller
studies and there are fewer of them. So bottom line is that there
is a publication bias, and we have a tendency to publish positive
results over non-positive results. That's not good news if you're
doing a systematic review and meta analysis because how are you going
to find those unpublished studies? Well, one might say,
what if unpublished studies wouldn't influence a systematic review and
meta analysis? What if they're not very good studies? What if they're so small they
wouldn't have that much of an impact? Or what if there is a difference, but not that big a difference
in terms of their findings? This has been examined in
one study that I know of, where the authors looked at
studies coming through the FDA. And they calculated the summary
statistic for a series of studies of that topic using unpublished
FDA data and without the FDA data. And what they found is sometimes
the unpublished data increased the summary statistic and
sometimes it decreased it. About 40-some percent each direction,
and then, in the middle, there was about 7% with
no change whatsoever. And so, one can't say that in the case of
the FDA data, that the unpublished data always push the summary statistic in one
direction because it's just not true. It's about half the time in one direction
and half the time in the other direction. But we need to know more. This is just one study where
the impact of unpublished results have been looked at a meta analysis. Most people remain quite scared that
unpublished studies are going to affect their meta analysis and so
the IOM standard said other standards recommend doing a comprehensive search for
all studies, unpublished or not. There is a study, and we'll talk
a little later in the course about clinicaltrials.gov, which
is a register of clinical trials and observational studies to
some extent that have been initiated. Since clinicaltrials.gov was authorized
by law, there's also a requirement for studies that come through the FDA
of drugs, biologics and devices, that they not only are registered,
but their results are posted. This study came out in 2013 and
showed that actually the results that are in cinicaltrials.gov
are more comprehensive and easier to understand than the results published
in journals for the same studies. If you look at the bottom of this slide,
you'll see that half the trials with results posted on clinicaltrials.gov
had no publication at all. So you would really need
to go to clinicaltrials.gov to find out what the results for
that study are. And that reporting was more complete
at clinicaltrials.gov for a number of different topics that are important to
a systematic review and meta-analysis. The flow of participants in the study,
how well the intervention worked, adverse events associated with that
intervention, and serious adverse events. And so bottom line here is,
you want to go to the FDA and you also want to go to clinicaltrials.gov
databases to see if there's any data in there about the studies that
you would include in your systematic review to make sure you aren't
missing important data, both for comprehensiveness and also that you
aren't just getting positive results or a majority positive results by
looking at published findings. Although people have been concerned
about publication biases for many years, selective outcome reporting is something
that's really only been noted and examined in the last decade. The first major report of selective
outcome reporting came from Anlin Chan,
who in 2004 published the first of a couple of studies examining whether
selective outcome reporting existed. This study on this slide shows
results from his JAMA study, where he followed up
protocols that were submitted to two ethics committees in Denmark, one
in Frederiksberg and one in Copenhagen. And he looked at the protocols and examined all that were associated
with full publication. What he found was something
that was shocking to all of us. And that is about two-thirds of
the time the investigators changed what was in the protocol and
listed as a primary outcome to what they reported as the primary outcome in
the publication and vice versa. That is two-thirds of the time, the primary outcome was different when one
compared the protocol and the publication. And we know from other
courses you've taken and also this course that the primary
outcome and designating it before you begin your study is very important, not
only for selecting the sample size, but also for assuring the reader that
you haven't been data dredging. So what you say is the primary outcome
they believe is what you set ahead of time, which is very Important in
inferences that can be drawn from your P-value,
your statistical significance and other claims that you make
as a result of your study. Just as importantly, he found that
statistically significant findings had a higher likelihood of being
reported than non-significant findings. So it appears that what one chooses
as a primary outcome is related to whether it's found to be
a statistically significant result or not. Well, this is scary, again, if we're talking about beneficial
result such as survival. If we have a couple of different outcomes,
death from all causes, death from a specific cause, and if we
only report death from a specific cause, which had a positive effect
of the intervention, and don't report death from all causes, which
had no statistically significant benefit. So, if we change our primary
outcome between the protocol and the publication, then we aren't
telling the reader the truth about what we were initially looking for
and what we found. So what Anlin Chan's results and other
people since then have shown us is that we have to dig deeper than the publication
if we want to know what was found for all the outcomes, both to find out what
was the original primary outcome, and are they reporting that correctly? And what were the results for outcomes
that were both reported and not reported? You can find these unreported
outcomes sometimes in FDA database, which I've already mentioned. Or in clinicaltrials.gov, also mentioned. And
sometimes you can find outcomes
that weren't mentioned, and results that weren't mentioned in
what we call the grey literature. So the grey literature we
define as conference abstracts, unpublished data, that for
example, appear in documents that are submitted by contractors
to government institutes, book chapters, and
other sources like pharmaceutical company data, letters, theses, et cetera. So, it turns out, and I'm not going
to show you the data here because we looked at that in an earlier lecture,
but only about half the time. Our study center reported an abstract
form, such as to a professional society, reported fully in a journal
article only half the time. So we want to look at those conference
abstracts because it may include information about some
very important studies. It turns out that one of the reasons
that conference abstract studies are not reported in full, is related to the
findings, whether they're positive or not. So, Sally Hopewell did a systematic
review of the grey literature to look and see whether it might really make
a difference in systematic reviews. Most systematic reviewers are reluctant to
go back and search conference abstracts, search thesis, search book chapters
because it's so much harder to do than say searching PubMed where we
have databases at our fingertips. Conference abstracts often require going
to actual books which are hard to locate, and book chapters,
well where do you begin? What Sally Hopewell found in her
systematic review is that studies that are published as opposed to remain
in the grey literature tend more often to show a positive result compared
to studies in the grey literature. And that this can affect
the results of a meta analysis. Again, it was a small effect. But given the small studies that
are done of this topic so far, many people are reluctant not to
hand search conference abstracts and other grey literature when they do
a meta-analysis, because it could possibly affect their systematic review and
meta-analytic results. It's a lot of work, though, and something around which
there's a great deal of debate. So finally when we think about selection
bias, people think about the fact that there is often data missing
from a particular study publication. And we already know that there's
selective outcome reporting. But there may be other
unpublished information as well. What we find is those doing systematic
reviews often try to contact the original authors or the original investigators
to get unpublished information. They may send an email. They may write them
a letter in the old days. And there have been studies, as a matter
of fact there has been five studies, that have examined whether it's
worthwhile to contact authors to try to get unpublished information. This slide shows data from a systematic
review published in the Cochran database. And it's a systematic
review of five studies. A meta analysis was not done because
the studies were probably too dissimilar. The study was done by Young and Hopewell. And Hopewell was the author of
the previous study we looked at. What they found was this,
that when a systematic review investigator writes an email to an investigator
of an individual study, they're likely to get a better response
than when they use a different method. There have been two studies on this
topic and both showed the same thing. So, if you're doing a systematic
review for this class, and you are concerned about some missing
data or an outcome that was not examined apparently in the study that's
published that you're including. You might want to correspond by email with the authors to ask
whether they have the data. And we can talk to you, the TAs and the instructors can talk to you
about how to do this in your email. And what to send them to make
it as easy as possible for the original author to send
you the data that you need. You could also write the original
author to ask about clarification of the methods that they use. In particular, you may find if you
include conference abstracts that you need a lot of information about how
the study was conducted because conference abstracts can be so brief. Again, we will help you with how
to write an email to ask for clarification, so that you maximize
your chances for getting an answer. Authors of one study found sending
repeated emails to the same person using the same methods really wasn't beneficial. That didn't increase the likelihood
of
getting the unpublished information you were seeking. If sending an email doesn't work,
you might try telephoning. And if you have five things to ask
the investigators, that theoretically won't influence their response
based on the results of one study. So as long as they're getting out of
their chair to get you the answer to one of your questions, they might
as well get you the answer to all five. That's what it looks like. So, don't be afraid to ask for what you need when you're
doing your systematic review. It's really hard to get data from drug
companies about unpublished studies. And, it has been done, however, there are some methods people
have used to get the data. But, I think very little is known
even though there's one study that shows that knowing exactly
what to ask for, what's the study. And then saying we'd like details
on this particular study, does enhance your chances of
getting a positive response. But just writing to or
calling a drug company and saying, what are your unpublished
studies on x, really doesn't work. So you have to be pretty specific, unless you know someone working in
the area in which you're interested. So that's about it for selection bias. It's a topic I love, so you'll hear me talking about
it quite
a bit when I walk around one on one. But we're going to move on
to other forms of meta-bias. Some of them you may find to be quite
important in your systematic review as well.
Hi, again. This is Kay Dickerson. Now we're moving into Section C, where
we're going to talk about information bias in doing the systematic review. So, what are our particular concerns as
regards metabias in
doing a systematic review? I would say, in terms of information bias, we're worried about the accuracy of our
quality assessment or the risk of bias, the accuracy of the data abstraction
that's done from the individual studies included in the systematic review, and
how complete that data abstraction was. Let's talk first about inclusion bias, as a subset of information bias,
that is, and I already mentioned it, if we know who the study investigators
were on a particular study, or the outcomes that they assessed or
what they found, does that, it affect what we
include in our systematic review? If I know that the Canadian
study of women 40 to 49, of mammographic screening
showed a nulled result for mammographic screening in terms
of breast cancer mortality, am I more likely to make up
inclusion criteria that would ensure that Canadian study is not
included in my systematic review? That's what we're worried about
in terms of information bias, in terms of whether studies included it or
not. Now, how about if I know
the results of a study? Does that change how I abstract the data? Let's say, a study was actually included,
and I know ahead of time the individual study that I'm abstracting
data from had positive results. Does that change how I abstract the data? Am I more likely to think
they did a good job? Am I more likely to look to make
sure they examine multiple outcomes? Or maybe I'm more likely to look
if they have negative results. And how about if I know the study
investigators, or outcomes? Just as before,
does it affect if the study gets in? Does it affect my quality assessment? There'd been numerous studies
on other topics that we tend to think a study is better
done if it has positive results. Or, we tend to think,
there are results on this, that a study is better done if
the author is a man compared to a woman. And so, these things are known
in individual studies to affect our assessment of the quality of
the study, would this also be true for those people who are working on
systematic reviews and meta-analyses? There have been about five
studies that have examined whether it's worth it to blind or
mask the author doing the systematic review to who the authors
are of the individual study. Their institutions, what journal their
article's published in and their findings. So for example,
let's say I'm extracting data for my systematic review from a series
of ten different studies. If I can see where each
study was published, am I more likely to think that a study
that comes from a high-ranking journal is of higher quality than a study that
comes from a low-ranking journal? If a study was published by one
of my close colleagues, but it's really not a very good study, am I
likely to give it a higher mark because this is one of my close colleagues, and
I know that he or she does very good work, and so it must just have been
a fluke that this got out somehow. So perhaps I'm influenced,
knowing who did the study, what journal it was published in, what institution
they come from, and their findings. Maybe that could influence me. So, it turns out that five studies
have looked at whether you can mask the person doing the data abstraction for
the systematic review to these elements, and to see whether that makes a difference
in the data that they extract. So in the old days, what we used to
do is something called differential photocopying, and it took forever. You cut up an article and you just present to the
data extractor
the title and the method section. And then the person would extract
the data on risk of bias and what are called quality items for
that study. Then we would compare the results
that were extracted for that study, that was differentially photocopied, with
results of a study extracted where there was no differential photocopying, and
see if they got different answers. It turns out, really, all this blinding or
masking of reviewers to who did the study and what they found
really didn't make a difference. Although one study in 1996 did find
that it made a difference to do masking, the rest of the studies really didn't so
much. Maybe in specific cases it
appeared to make a difference, but over all, a systematic review and
meta-analysis of this particular topic showed that it doesn't
make any difference to mask the authors to the author's institution journal and
results before they abstract the data. This is good news for all of us because
that differential photocopying took a long time and was a real hassle to do. So what this means is
that you can abstract data from your individual
studies without anything fancy. Just make sure two people do it,
and then you can compare what the two people extract, and that's
a good way to see whether they're finding the same results and to discuss any
differences that they're finding. So what are some other issues that
are related to information bias? Sometimes we're worried about whether
the data that we get from a graph is not accurately abstracted. You may find, for example, as a matter
of fact I know you will find, in some of the papers that you are looking at for
your systematic review, that the only way that you can get data on an outcome
you're interested in is from a graph. And the graph might be proportions, and you have to guess then,
at what the numerator and denominator are. Well, that's scary to do, and a lot of what we do in systematic reviews
is scary and involves a judgment call. But this is one of the scariest,
where you're assuming a numerator and denominator and
all you have is that graph. We do have some software now,
I'll show you, that can do this and apparently is pretty accurate. You can also go back to
the investigator in that email and say this is what I deduced,
looking at your graph. Is it correct? And some of you might be
able to get an answer. Another place where information
bias might be an issue, is whether the experience of
your abstractor is a factor. So for example, when we're doing
systematic reviews in our Cochrane Eyes and Vision group, we hire graduate
students, and we wonder whether someone who's brand new at the task isn't as good,
isn't as accurate as the person who has a lot of experience reviewing this
type of study and extracting data. Another question about
information bias might be, and I mentioned it briefly on the previous
slide, is it necessary for two people to abstract the data and
then to compare what they found in order to assure that we have reliable
data from that particular study? I don't think we have complete
data on that yet, but most of us do duplicate abstraction and
certainly for the results. And then finally, the question of
course you have by now because of what I've said in previous sections,
can we rely on what's in the publication? And unfortunately,
the answer is probably no. Whether there's a bias or not is to
some extent open to question, but I think we know they're reporting biases,
and so we probably will have to look in more than one place to see if
data agree in those gray sources, in FDA database, clinicaltrials.gov,
as well as the publication. So what I'm showing you here is
an example from JAMA, where there was a correction made because there was
an error in data extraction and analysis. Now this is probably pretty rare,
that someone published a correction and we're glad for it of course, but
errors are made all of the time. I don't know about you, but I make errors
and I am very grateful when there's another person doing data extraction,
because we get tired we read wrong. It's just human nature, and
we're human beings at the end of the day. So, I think we have to accept
that errors are made and build in some protection for
our systematic reviews. Here's another example I mentioned, which is how you can extract
data from a published article using a little piece of software,
or even just using your ruler. So, if you're interested in this and
you come up against this problem, there are sources you can use to
try to minimize information bias. And then finally, the question that I mentioned about
whether experience affects accuracy. These are some data from a 2009
study where people with less experience were compared with experts,
let's call them. And to see whether they
don't do as good a job, make more errors than the people
who have more experience. What the authors found, interestingly
enough, is that the error rates were similar and it didn't depend on how
much experience the person had. However, inexperienced people
took longer than the experts, but the error rate was the same,
and so that's reassuring for us. Certainly bears replication in another
study, but it was initially reassuring, at least, that the experience
does not affect accuracy. Finally, I've already mentioned a couple
of times, that one of the ways we can protect against errors in
data extraction is to have two people do that data extraction and
then compare what they've extracted. So what are the possible ways
that you could extract data for your systematic review? The first is you could have one reader go
in there and extract the data onto a form. The second is to have one reader go in,
extract the data, and a second reader come in, look at what
the first reader has extracted, and say, yeah, I agree with it or no I don't. That's called single data extraction and
verification, and that's what was done in the study
that I'm presenting on this slide, the Buscemi study published in 2006. And then the third thing that you could do
is you could have two separate individuals go in and extract the data without
talking to one another, or knowing what the other person did. And that's a completely
independent data extraction. Those two people who extracted
independently get together, then, after their extraction, look at where there are
differences in what they extracted, and they decide between them what
is the correct response. So what Buscemi did is he compared
the single extraction plus verification with double extraction, and
he found that there was less inaccuracy, and overall, a lower error rate
with the double data extraction. It's only one study though and some people are concerned that
this is much more expensive and time consuming than if you just do a
single data extraction with verification. One of the solutions that was suggested in
the IOM Systematic Review Standards was, perhaps you could do data extraction for the results that you will be
combining in your systematic review. And for less important data such
as the investigators, the journal, which was published, and the dates, this could be done by
a single extractor plus a verifier. That was just a suggestion for
those with limited resources, and I think the jury is still out on whether
one needs to do double data extraction. In the systematic reviews that we
do in the Cochrane Collaboration, we do double data extraction, and
I hope you will also in this course. So that concludes the section
on information bias. In the next section we're going to
talk about bias in the analysis and ways that we could possibly prevent or
address it.
Hi, this is Kaye Dickerson. We're beginning section D,
Bias in the Analysis. Bias in the Analysis is the third of
the types of meta bias that we're confronted with when we do
a systematic review and meta analysis. So you may see in the literature and
here about a concern. With the type of statistical model
that's used for the meta analysis. The two typical methods that are used
are random effects and fix effects models. Now can this choice of model
influence your overall estimate? Yes, it can in certain instances and I'm
going to show you a result, where it does. But, I think the thinking now, is that let's not get too hung up
on random versus fixed effects. That most people start out their
careers in systematic reviews thinking that the random effects
model makes much more sense. And over time, I've heard a lot of people
say that they changed their preferences, for which model they'll use. So I think the bottom line there is that,
there's no right or wrong here necessarily, although you might
want to consider different models for different situations. Try to understand what the models are,
so that you can gauge in conversations about them, but
we aren't going to come down hard and fast saying you should use this model or
that one. So, just on the question of whether the
choice of model can affect your estimate. There was a study,
as I mentioned, by Jose Villar. It was published in
Statistics in Medicine. And they looked at 84
Cochrane meta-analyses from the Cochrane Pregnancy and
Childbirth Group. And they compared the meta-analyses that
use the fixed and random effects model, by whether they had a significant amount
of statistical heterogeneity or not. Let's look at a slide that
goes through what they found. It's a little bit confusing so
let's take it slowly. What Jose Villar and
his colleagues showed is that sometimes, there is a difference between
using a random effects and a fixed effects model, and
it can influence an overall estimate. While I and others,
nowadays 13 years later, tend to minimize this
as an important choice. That is which model you use. They showed something
that gives us pause and makes us think about which
model we want to use. It's worth, thinking about,
that's for sure. So here what we see are two,
two by two tables. The top one, the meta analyses
were those that had significant statistical heterogeneity,
amongst the studies that were included. And there were 21 meta-anaylses
that had statistical heterogeneity. In number two, there were 63 meta-anaylses that did
not have statistical heterogeneity. That is, we're more confident that these studies
tend to have results that are similar. Now let's just look for a minute at the 21 meta-analyses
with statistical heterogeneity. What they did is, they did the analysis in
these 21 meta-analyses two different ways. First, they used the fixed effects model,
and then second, they did the analysis
using the random effects model. If you look along the top, you'll see, using the fixed effects model did they
get statistically significant results? That is, did the results exclude unity? So, yes means statistically significant. And
with the random effects model, excluding unity also means something like,
statistical significance. They're saying it very carefully. So when we have statistically
significant results, regardless of whether we use the random
effects or fixed effects model. That means they found pretty much
the same thing in terms of whether they were significant. Whether you use the fixed effects model or the random effects
model,
you did not exclude unity. That is,
it was not statistically significant. So with the 21 meta-analyses
with statistical heterogeneity, only 5 times out of the 21
meta-analyses is there a discordance, when one uses the fixed effects
versus the random effects model. That's 5 out of 21, or
about 25% of the time. When we look at the 63 meta- analyses
without statistical heterogeneity. We see that 4 out of 63 times, there are discordant cells,
that's about six percent of the time. A much lower proportion of the time,
than when there statistical heterogeneity. And so the take home message here
is that when you have statistical heterogeneity, you may want to
use the random effects model. It will give you a more
conservative answer. That is, it may not exclude
unity compared to if you had use a fix effects model when you
might have gotten a statistically significant result that you didn't
get with the random effects model. When you don't have statistical
heterogeneity in your meta-analysis, then probably it matters very
little which model you choose. So a random effects model doesn't
fix the heterogeneity problem but because it makes those
confidence intervals wider and excludes unity less often,
it may be a more conservative approach when you have statistical
heterogeneity in your meta analysis. So it turns out that
meta-bias is not always analyzed in the systematic reviews and
meta-analyses that are in the literature, and sometimes, this is related
to bias in the analysis, and sometimes, it's related to selection
bias and information bias. This is an example of examination
of systematic reviews in urology. This is an example of systematic reviews
assessment for pediatric oncology. And, this is a blowup of a systematic
review in pediatric oncology. We often,
you'll see the terms transparency, which are, can we tell what was
done in a systematic review? We often, as you will find out or
have found out in this course, find that you can't tell what was done in
the individual studies that are Included in our systematic reviews. Well, it turns out we often
can't tell what was done in the systematic reviews and
meta-analyses themselves. So, this is a table,
a figure that comes from that urologic systematic review quality
paper that I showed a few slides ago. And what you can see is they found,
looking at 57 systematic reviews in urology,
that about 40% did duplicate screening and data extraction, .close to 50% did
a comprehensive literature search. Only about 30% published
their legibility criteria. In terms of analysis methods about
60% published what was done. Only looks like 18% or so,
assessed whether there was a likelihood of publication bias in
the way they search for relevant studies. And so you can see that the quality of
systematic reviews is not only not high, but the chances of metabias
are very high indeed. So much of what I've been
showing you is related to clinical trials which is a problem
we have as you know through out this course is a lot of the studies
of how to best do a systematic review are done on clinical
trial information. But we do have some information
about systematic reviews and prognostic questions. They weren't any better at defining
outcomes, at looking at confounders, at describing the analysis at doing
an appropriate analysis and so forth. And this is just a quick snapshot to
reassure you that even though we're looking, for the most part, at studies
that were done examining clinical trials. When people have looked
at observational studies, addressing other types of questions,
and intervention effectiveness. We're finding the same thing. That basically,
the vast majority of systematic reviews or let's just say, the majority, similar
to the majority of individual studies are not well done, and we need to improve. Part of that is the education
you're getting in this class, and part of is that journal editors and
reviewers need to know a little bit more before they review and
pass on these articles for publication. So let's talk for a minute about
a situation that comes up now and then. That a number of people
are concerned about. I'm less concerned about
in the majority of cases. But I know that It does
bother people that sometimes systematic reviews on
the same topics don't agree. They contain a different
number of studies, they used different methods of analysis,
or their searches were different. And what does this mean? How could there be two
answers to the same question? The majority of time,
it's the scientific process. It is a slightly different
question that's being asked. And if the systematic review is
transparently reported then the reader can go back and see whether they might
prefer that it's done a different way, and either add studies or redo it so that the question they're interested
in is addressed and so forth. But this example on
this slide was one that I found quite intriguing because
it shows a number of things. So on the left side. What we have is a meta- analysis done by
the Cochrane group on the efficacy for H pylori eradication in
non-ulcer dyspepsia. On the right side we have
the same general topic that is the eradication of H
pylorean non ulcer dyspepsia. And this systematic review was done by
the AHRQ evidence based practice center. So, look at these two meta analyses,
they're very different. We're just looking at them generally. The Cochrane review has many more
studies than the EPC review. What's the overlap? Looks to me like in general, the same studies are there, but
the EPC review includes data and pathos and they aren't included
over on the Cochrane side. So what's going on? If you look at the bottom line, the Cochrane review favors
eradication of H pylori. Well, the EPC review also favors it but
it's not statistically significant. So in fact the proper interpretation
is It does not favor. There's no evidence to favor H.pylori. It doesn't disfavor it. It's just no evidence favoring
H.pylori eradication. So what's going on here? Well it turns out that
the dates of the search for trials that are included in the two
systematic reviews are different. The Cochrane search
terminated in May of 2000, while the Annals of Internal Medicine,
the EPC review, curtailed the search in December of 99,
about six months earlier. That later search resulted in
three more trials, abstracts and a paper was how they reported. Now the abstracts didn't
always report the outcomes just the way that the systematic
reviewers were looking for them. As a matter of fact,
one of the abstracts in the Annals, or EPC review had a different outcome or
endpoint reported than the one that the EPC's designated as the one they were
interested in for the systematic review. When this happened for the Cochrane
review, they contacted the author and obtained data for the exact outcome
that they were interested in. When it happened for the EPC reviewers,
they decided that the outcome, that was reported, was close enough to the one they
were looking for and used the data there. And this made a big difference. So, it turns out there were
a number of things going on. The searches were different. Studies that were excluded and included were different, and
this endpoint also made a difference. That the Cochrane authors called
the authors of the individual study where the endpoint was not reported in the
form that the Cochrane authors wanted it, and they got the actual
data that were unpublished. Bottom line, reviews can differ in
their findings in a legitimate way, can be perfectly legitimate. One isn't right, one isn't wrong. Part of it is the difference
in the scientific method, and you just have to read them
very carefully to see how it matters to the question
you are asking as a reader. But also there can be a difference in
methods used that make a difference, in this case, the search being done
closer to the time of publication, and finding out what the results for that outcome really were was important
to the findings in the review. So now I finished up talking about biases
in systematic reviews and meta-analysis, or meta-bias, and I'm going to finish up
by talking about reporting transparently.
Hi, again.
This is Kay Dickersin and we're in Section E. I'm going to talk about
reporting systematic reviews and meta-analyses transparently. We've talked about and you've heard about
in other classes reporting guidelines such as consort for clinical trials,
strobe for observational studies. There also are reporting guidelines for
systematic reviews and meta analyses. PRISMA is the reporting guideline for
systematic reviews of clinical trials, and MOOSE, a little outdated by now,
but nevertheless still useful, is the reporting guideline for
observational studies. You should know by the way that
an updated MOOSE is being worked on. I'm just showing you here, and you
should definitely go and look it up for your systematic review,
the PRISMA checklist. It includes some of the items
that are included on PRISMA and which items should be included
in your systematic review. These are reporting standards. These aren't how to do your systematic
review, we've already talked about that with standards for systematic reviews,
but this is how to report. One of the most important components of
reporting your systematic review is to have a flowchart of how you
examine the different studies and the data from those studies. This is comparable to looking at a flow
chart for a clinical trial and how the individuals pass through the clinical
trial and how the data were collected. In this case, however, it's how you
identify the studies that were included in your systematic review and
what was analyzed in the end. You can see that you start out by
how many records you identified through database searching and
other sources. How many you had to remove because they
weren't eligible, or they were duplicates. And you end up with a certain number of
studies for each of your meta-analysis. You may conduct more than one
meta-analysis per systematic review. MOOSE is very similar, it was published
a number of years ago in 2000, and it's being updated now. It presents, roughly, a similar outline
of how to report a systematic review and meta-analysis of observational
studies in epidemiology. By the way, in the old days, we referred
to systematic reviews as meta-analysis, instead of separating the two concepts. So you will see,
in some of the older literature, systematic reviews being
referred to as meta-analysis, even though they don't always
contain the quantitative synthesis. The final thing I wanted to talk
about is something called GRADE. And grade is how you summarize
the body of evidence. Now this is a term that's often
used incorrectly, body of evidence. People often use it to refer
to the meta-analysis or the systematic review of
the individual studies. Grade comes after that. And grade was developed because
physicians wanted to know, all right, now that you've done the systematic review
and meta-analysis, what do you think? Do you think we should rely on this? So let's not just do the systematic
review and show no evidence of effect. We want to also know how good
were the studies in general. Is this reliable? Should more studies be done? Where are we in this? It's a grading of the
body of evidence and it's not the same as the systematic
review or meta-analysis. So how good is the evidence? That's the quality of the evidence. And, you would give a lower
grade if
there was a risk of bias in the evidence. You would give a higher grade
if the effect size was bigger, if there was a dose-response effect. If it seemed like a plausible effect,
and so forth. And this is done somewhere between
the systematic review, and let's say a clinical practice guideline. It isn't necessarily done
by the systematic reviewer. It could be done by the guideline
producers instead, so you won't always see it as
part of a systematic review. But I wanted to mention it because
you will hear summarizing the body of the evidence often
misused in your travels. In summary, what we've talked
about are three areas for meta-bias in doing your systematic review. Selection bias, which is largely seen as
reporting biases but also inclusion bias. Information bias,
which has to do with how the data for the systematic review are extracted and
identify actually. And bias in the analysis itself. And that's a really tricky one. We just touched on that. I think not a lot
is known compared for
example reporting biases. We talked about having standards for
minimizing meta-bias, which includes doing a thorough search,
watching out for information bias, and considering
the impact of the methods of analysis. This is all important to you as you do
your systematic review and go forward. I'm not just presenting theory here. I'm presenting some theory, but it's also
advice about how you
conduct your own systematic review. The methods are not set in stone. You have to decide for
yourself which methods you are and are not able to apply in the short
eight weeks that we have. But, you should know, and it should be
part of your discussion section when something is an issue, you should
be paying attention to even if you didn't have time, for example,
to search more than three databases. That's all for this lecture,
and thanks for listening. [MUSIC]
[MUSIC] Hi, this is Kay Dickersin, and I'm going to talk today about Qualitative
Synthesis and Interpreting Results. Our objectives in this lecture are first to identify the main
components of a systematic review. And by now, you really know what those
components are, but let's look at them in this lecture, in terms of where
does the qualitative synthesis fit in. To try to understand what it is. Second, then we'll review the importance
of the qualitative aspects of systematic reviews, and
some examples of a qualitative synthesis. Before I begin, I will say that this is absolutely the
hardest part of your systematic review. It's also the most important. We're talking about
systematic reviews here, and sometimes you do a quantitative
synthesis that is a meta analysis, bringing together the results
from all the studies. But, the qualitative synthesis is bringing
together the results of all the studies with out the numbers. So how do you actually do that? And it's hard to do. I'll try to
give you my
interpretation of how to do it. But in the end it's going to take looking
at examples, talking amongst one another, and really trying to figure out
what it is as you go along. But remember you are the one
who knows the most about the topic that you are doing
the systematic review on. You know more then me,
you know more then any of your readers because you have been immersed in
the topic doing your systematic review. In section A, we're going to talk about
why do we need a qualitative synthesis, and then we're going to move onto and
what's it all about. So let's look at a definition
first about systematic reviews. Now, in the old days, systematic
reviews were called meta-analysis. We now divide the systematic
review into two parts. The idea of doing the review
systematically and then the quantitative synthesis part,
which may or may not be there. And that's called meta-analysis. But in the old days,
which is 20 years ago, 1995, meta-analysis was
considered the whole thing, so let's just think of that when
we look at what's being said here. I'm going to read it. Meta-analysis was developed to replace the artistic narrative
review with
a scientific and systematic method. So far so good. Yet in fear of allowing bias to creep in,
meta-analysis is typically mechanistic, driven more by concerns
about reliability and replicability than about adding to
understanding the phenomena of interest. And that's what's important here,
is we're not just trying to do a mechanist synthesis of all the studies that
are out there on a particular topic. We're trying to also bring in the part
about knowledge, the part about understanding the topic,
about interpretation, because as I said, you know more than anybody else by the
time you finish your systematic review. And what I've said before, meta-analysis
is not the same as systematic review. A meta-analysis always requires
a systematic review but not vice versa. That is, a systematic review does
not have to have a meta-analysis. So what is a systematic review
if it's not a meta-analysis? We've talked about the structure
of systematic reviews, that there has to be a background and a description of why you need
a systematic review and meta-analysis. You need the methods section, which
describes exactly what you did in your systematic review, the search strategy,
the eligibility criteria. How you do data abstraction and data management,
your assessment of the quality and the risk of bias of the individual studies
you include and your statistical methods. And then when you come
to the results section. You'll talk about what you
found when you did your search. And that will typically involve
a flow chart of some sort. And then both a qualitative synthesis and
a quantitative synthesis. And sometimes, as we said, the quantitative synthesis
won't be warranted for various reasons. We'll talk about that in another lecture. But the qualitative synthesis. A part of
the results. Now, you will see sometimes in
some systematic reviews, and this has been true sometimes for
Cochrane systematic reviews, that this qualitative synthesis
is in the discussion section. People see the qualitative synthesis
as putting the findings in context. But we're not interested
in context here so much as what people found qualitatively,
not quantitatively. So that may involve the discordance
among the different studies that you're including in
your systematic review. When, for example, people have looked at
different systematic reviews themselves. How do they differ if
they're on the same topic? Well they might have asked a slightly
different research question. That is, the population of
patients may be different. The interventions might
be slightly different. The outcome measurements might be
slightly different in the settings. And we've talked about this before. Difference among the individual
studies you'll combine as well as differences in the final systematic review
from another systematic review that might be on a similar topic. Well what is that then? That's the qualitative nature
of the systematic review itself. A systematic review might differ from
another systematic review because of how the author's went about selecting
the studies to be included or how the data were extracted or how the study quality
was assessed or the risk of bias assessed. And then whether the studies
could be combined or not. Some of you may have already come
across in your own systematic review that you don't think maybe these studies
should be combined quantitatively. While in the index review that we've
given you to look at on the same topic, they did combine the studies
to get a meta-analysis. You might disagree with them. Well, why do you disagree with them? What is different about
your
interpretation of the studies you found than theirs that you don't think
a quantitative synthesis is warranted? So there can be difference among
systematic reviews just as there can be differences among the studies
that go into a systematic review. And that perhaps would be
best explained if you look at the data qualitatively and say. What are they like? How do we interpret them? Why do we
believe they
belong under this question? In the next section, we're going to
discuss what goes into a qualitative synthesis, and this is where things
start getting a little bit fuzzy. But more soon.
Welcome back, we're talking about a qualitative
synthesis in your systematic review. And this section is specifically concerned
with what is in a qualitative synthesis. As I mentioned before, defining
a qualitative synthesis is very difficult. I'll do it in sort of a fuzzy way,
or maybe it sounds good. But it's harder to do, actually. And that's what we'll spend
this section talking about, is how do you actually do it? So, we define a qualitative synthesis
as an assessment of the body of evidence that goes beyond
factual descriptions or tables. That, for example, simply detail how many
studies were assessed, the reasons for excluding studies. The range of study sizes and
treatments compared, or the quality of each study as
measured by a risk of bias tool. So that's all fine, and
you have to include those tables and those descriptions. But that's not the same as
a qualitative synthesis. The purpose of a qualitative
synthesis is to try to convey your understanding having looked at
the data now in it's entirety about how this intervention
might be working or not working. Whether an association that you find or
don't find really exists, whether it works for
a certain group of people perhaps, and under what circumstances it works. So for example,
you will have to make number of judgments as you do in your systematic
review, and you will have to say how these judgments might have affected
your interpretation of the results. I've said before at the beginning of
this course, that systematic reviews and meta-analyses are the most difficult
research I've ever done, and the reason is all the judgment involved. Well here's where you're
going to have to describe what effect these judgments may
have made on your interpretation. So for example, the evidence,
do you believe it? Do you have any uncertainty about it? Are there certain studies that
bother you for a particular reason? For example,
the report wasn't very comprehensive, and there were a lot of unclear areas. What about the implications
of missing evidence? For example, one study that you include
may be missing quite a bit of data from patients who discontinue treatment
and dropped out of the study. How do you feel about that? Might it have affected your results, that they just analyze
people who finish the study? Probably, how would you handle that? Would you do a sensitivity analysis? What do you
think about the interpretation
of your findings given this factor? What about the technical
methods that they used? Perhaps some of the studies in your
systematic review didn't describe allocation concealment or were unmasked. How do you feel that might've affected
the outcomes that you assessed? And then finally, you should be
talking about the reasonableness of conducting a meta analysis,
whether you've decided to do it and why. What might be the downside of believing
a summary estimate of effect, particularly for outcomes that may
just have very little evidence or evidence where there have
been quite a few dropouts? One of the most important areas of a qualitative synthesis is being able
to describe patterns in your evidence. Maybe you see that
studies in younger people tend to have results that are more
dramatic than studies in older people. Or it could be that a sub group
analysis of yours defined an initial baseline characteristic
as very important in the outcomes. Maybe that's an important pattern
that you want to talk about. Would this help you understand
underlying science? Would it help you to
interpret your findings? You should definitely describe that. Or perhaps you didn't see any patterns,
and maybe that will leave you with
questions about what you've found. So, this is your chance to be honest
about what you think of the data and to help draw the reader's
attention to things that you think they should be looking at as well. The idea of the qualitative synthesis
is to try to orient the reader. You want to describe the studies. What happened to the participants? What about the
interventions,
the comparators, and the outcomes? How do the choices that you've made
in designing your systematic review affect the findings that you have? How do the differences in the design and
execution of these studies potentially
influence the findings and results? And how about the execution
of the studies? Perhaps some of them were conducted
in outpatient setting and others in inpatient settings. Might that have affected the outcomes? What about if there was a
coordinating
center for a multi-center study for several of the studies and
others were single center? Might that have affected the outcomes? I might to choose to look and
see whether there were different results by whether there was coordinating
centers and multiple clinical centers in the trials that were included, or
whether there were single center studies. This may easily have
affected the outcomes. These are the types of things the reader
expects you to have investigated, and to bring into your qualitative synthesis. Again, the whole purpose of a qualitative
synthesis to is to integrate what you've found and to call attention to specific
situations, groups, interventions, outcomes, the way the studies were done. So that people who are reading your
systematic review can interpret both your findings and the robustness
of your meta-analytic results. So here are the things you
probably want to describe. First of all, the things I've
been mentioning, the clinical and methodological characteristics of your
studies, that might be, are they big, are they small? How might the size of the study
have influenced the outcomes? Whether subgroups were left out. For example, you could have in your
systematic reviews some studies of just older people, leaving out younger people, while other studies include
both younger and older people. How might it have influenced the findings
to have included just one subgroup of patients? The older people, or
just the younger people. What are some of the strengths and
limitations of the studies? How might risk of bias or
flaws in the design execution have made these results different
from one study to the next? And what is the relevance of your
findings to populations that are of interest to you? What about the applicability
to populations that are important both in this country and
other countries? Might poor populations
react differently to the intervention than
less poor populations? What about the importance of
co-interventions, of the setting where the research was conducted, or the outcome
measures that were used in the study? For example, were the outcomes
of importance to patients or were all-laboratory measurements
made in most of the trials? Nowadays we have a lot more interest in patient important outcomes
than we have had in the past. And it's worth it to comment on whether
these were included in most of the studies included in your systematic review,
and meta-analysis. So, in summary you want to describe
the nature of the evidence. You want to interpret it. You want to evaluate its strengths and
weaknesses and come to a conclusion. This happens before your quantitative
synthesis, if you do one. But the conclusion that you're
attempting to reach is, should I combine these
studies quantitatively? If so, why do I think it's all right? And if not, why not? We've talked about the Institute
of Medicine standards for doing a systematic review, and in fact the book of their standards is one
of the recommended texts for this course. If you go to standard 4.2, you'll see the standard on conducting
a qualitative synthesis, and there are five different items
outlined there for how to do this. Again, it's pretty vague just
as I've been somewhat vague. It depends on the systematic
review that you're doing. But sometimes guides
like this can be of help. And if you think it might, definitely look
it up and see what the chapter has to say. So what are the take-home messages? Systematic reviews
are integrative syntheses. So you want to integrate
all the information that's out there that addresses
the question that you're interested in. You will systematically
assemble the evidence, and you'll critically appraise it. But you also want to think
about the qualitative nature of the evidence as well as
the quantitative nature. The quantitative synthesis is not
always something that you want to do in a systematic review. It's not always warranted. Maybe the quality of the studies
that
you wish to include isn't good enough. Maybe the results that each study found are all over the place
in terms of results. Sometimes they show a beneficial effect of
the intervention, sometimes they don't. What does that mean? When that happens, you probably don't
want to do a quantitative synthesis because something's going on. These results are just too different. Well, we use the
qualitative
synthesis to talk about that. Why are these results so disparate? What might be the explanation? Or, if the results are
very similar,
this is encouraging you to do a systematic review, and
you might want to talk about that as well. In the next section,
I'm going to give you a few examples, or show where you can find them. I'm actually not going to talk a lot
about them, because that could be boring. But I'm going to tell you
where you could find them.
Welcome back to Section C,
examples of qualitative synthesis. In this section, I'm actually not going to
talk much, but I'm going to steer you in the direction of where we've put some
examples up of qualitative synthesis. Some of these examples come from
people who were in past classes. They've given us permission to put
up the qualitative synthesis so that you can learn more about
what might be included. This first example,
which is from a systematic review on abstinence-plus programs for
HIV infection in high-income countries, has a couple of sections that
might be interesting to you. The first is the description of
studies and what they found, their methodological quality,
and then they have tables where they described the characteristics of
the included studies, as you can see here. This review actually doesn't have much
in the way of qualitative synthesis. And I think I've mentioned that
Cochrane reviews put the qualitative synthesis in the discussion section,
which I don't happen to agree with. I think it deserves its own
section as part of the results. The next systematic review is done
by a group that took this class a number of years ago, and they looked at
that form and use in polycystic ovary. They did a systematic review that
included these tables on the studies that were included,
each of the studies that were included and the subgroups that they included. They also looked at
other meta-analyses and which studies those
meta-analyses included. And as you can see from this slide,
they included all different studies, which is what I was talking about in one of
the early slides in Section A, I believe. And that is that different
systematic reviews on the same topic may include entirely different
studies with some overlap. But you can see there is
no study that I can see, that it's in all the systematic
reviews which is very interesting. The one here, Creanga, that's highlighted is the one by
the former members of this class. This is from a systematic review from
another former class member looking at school-based intervention. And they talk about allocation,
and how this method of allocation may have affected
the outcomes of the study itself. And that's part of
a qualitative synthesis. Here's another example of how
publication year might have influenced the outcomes of a systematic review
of treatment for latent TB infection. This group also examined
the interventions and outcomes in each of the studies and
also in the systematic review as a whole. And showed how the differences
in these interventions may or may not have affected the efficacy
of therapy, and may also have affected what was ascertained to
be treatment-related side effects. I'm not going to go through
the rest of the slides. I'm going to leave them for you. This is the reason that if
I were go through them, I would be reading every word
because this is what's written in the write up of the systematic
review that past students have done. And I don't want to just read the slides,
I'll leave that to you. But these are some good examples of how,
in past years, students have used information from their systematic review
to enrich their qualitative synthesis. So do look at the slides. I'm not going to go through them. But they're there for you
to look at. And examples, that is the full example that these
were taking from, are up on CoursePlus. I'm going to conclude by saying that
I encourage you to look at examples. I encourage you to look at any systematic
review you can get your hands on, for how they might have done
the qualitative synthesis. Because this is sort of a vague area. There is no right and wrong. However, you definitely need
a qualitative
synthesis in your systematic review and we're going to look for
it when we do the grading of your papers. So I can't emphasize enough how
important it is that you include this. We consider the most important
part of a systematic review and encourage you to find a way
to use thinking about and interpreting what you find in the write
up of your systematic review. So that's all in this lecture
about qualitative synthesis and I look forward to seeing you next time. [MUSIC]
Well, many many years ago I was working at Tufts Medical Center, and Joseph Howe, who was one of the founders of
medical med analysis came and joined us and he needed a statistician to help him on some of his projects and I got
involved. For any journal you want to be transparent in what you do and you want your statement very clearly and you
want to give good clear discussions. But be careful when you're submitting a paper. It's a little bit luck of the draw what
kind of reviewer you get but try to always be very polite in your responses to the questions they ask. And just try to get
answers back that will answer their questions but don't give away your integrity. It's your paper and you should make it
what you want it to be. Well, my big thing these days is outcome reporting bias. Basically, if you do the meta-analysis
and you find a bunch of studies but only very few of them report a particular outcome, it's really a problem if you just
report on a few studies that report that outcome without saying anything about all the studies that don't because there's
probably a good reason why they didn't report those outcomes. And it's probably related to the fact that the result was
insignificant, and so you'll get a biased answer.
In the previous modules, you have heard about the basic steps
in conducting a systematic review. You start by forming an answerable
clinical question, and then you will design a search and look for all the evidence supporting
that clinical question. Then you assess the risk of
bias of included studies, and do a qualitative synthesis
by putting them together. Remember that meta analysis is only an
optional component of a systematic review. In the last module,
we will discuss how to do a meta analysis. A good meta analysis has to
address your research question, which means that you have to plan it well. You have to think about the comparisons
you're going to make, the data you need to make that comparison, and
how you're going to combine them together. And in combining them together, there are
different models and assumptions you can use, for example, a fixed effect
model versus a random effects model. But we'll cover those models
in the last module, and we will talk about the assumptions
underneath those two models.
[MUSIC] Hello everyone. Today we're going to talk about
planning your meta-analysis, why planning your
meta-analysis is important? Because you have spent all the efforts
in this course to identify studies for your systematic review, and now we're at the stage of quantitatively
analyze the data from those studies. And the learning objectives for today
are: formulate a general framework for quantitative analysis
in a systematic review. Quantitative analysis
refers to meta-analysis. I will define formally what
is a meta-analysis and talk about the potential
advantages of doing meta-analysis. And for any meta-analysis it's very
important for you to understand the types of data you have and the measure of
effect or association we're going to use to quantify the effect of intervention or
to quantify the association. By the end of the lecture, I hope you
will take away with you the key messages. The first essential element of analysis
is a thoughtful approach to both its qualitative and quantitative elements. And you have heard about
qualitative synthesis and I will remind you in the next slide. And you have to come up with
an analytical framework which include the specification
of the comparison. For example, are you comparing one
intervention versus another one? Or are you comparing people
exposed to a risk factor versus those who are not exposed
to a risk factor. And then you have to think about
the types of the data you have. Are the data continuous, are the data
dichotomous or are they categorical? And depending on the type
of data you have, you will choose the summary measures. It could be risk ratio, odds ratio for
dichotomous data, or mean difference for continuous data. And all these decisions, or the choices you have made should
address the study question. And lastly, meta-analysis is really
the statistical combination of the effect estimates from two or
more separate and independent studies. Let's start by talking about
planning your analysis. Well, the results of a meta-analysis can
be very misleading if you haven't paid enough attention to the first few
steps of a systematic review. You have spent a lot of time in
formulating the review question, and the review question determines
this eligibility criteria for your systematic review. Then you're going to follow your
eligibility criteria and protocol to identify, select, and critically appraise
the studies for the systematic review. And after you have decided
on the studies to include, you're going to collect appropriate
data from those studies. And lastly you're going to do
the meta-analysis which is an optional component of a systematic review, and today we're going to focus
on planning your analysis. Systematic reviews contain analysis of the
primary study, in other words the unit for your analysis is the individual study. And primary studies include
analysis of their participants. And there are really two
components of any analysis. The first one is qualitative analysis, or
we refer to it as qualitative synthesis. And the second part is
the quantitative synthesis. And I put qualitative synthesis
before quantitative synthesis, because it's the most important
part of any systematic review. It refers to a structured summary,
description, and discussion of the studies' characteristics
that may affect the cumulative evidence. For example, you have to look
at the studies carefully and ask yourself the question,
are the studies similar or different? Are the study participants comparable? Are the interventions, homogeneous across
studies you're
including in the systematic review. Are the studies designed and
conducted and reported properly. Do you have enough information from
those studies for your meta-analysis? So please spend a lot of time and put a lot of efforts in
your qualitative synthesis. And after you have done a thorough
qualitative synthesis, you're at a stage of doing quantitative analysis,
which we refer to as meta-analysis. And the general framework for a synthesis include answering and
thinking about the following questions. What is the direction of effect or
association? Is the intervention preventive? Is the intervention harmful? And what is the size of effect? So that refers to
the point
estimate from your estimate. And then are the effects
consistent across studies? Are all the studies estimating
a similar effect or are they estimating a different effect? If the results from studies are not
consistent, why they're different, and lastly, what is the strength
of evidence for the effect? The strength of evidence also relies on
judgments based on the assessment of study design and risk of bias, as well as
statistical measures of uncertainty. For example, you may have a small study
that was poorly done and reported, but it shows a very strong effect. Then you don't want to put a lot of
confidence in that particular study because of your concerns about
the risk of bias, weaning that study. How to frame analysis plan? Well, analysis plan should follow
from the aims of the review. Depending on the aim of the review, you
will use different analysis techniques. For example, if the goal is to obtain size
of effect from similar studies estimating the same effect, and you have only a pair
of interventions you're comparing. For example a mechanical verses manual
chest compressions for cardiac arrest. And then you will choose standard
pair-wise meta-analysis, and that's the type of analysis
we will focus today. You may have more than two
interventions for the same condition. For example,
we have four different classes of drug for primary open angle glaucoma. And more than 16 different eye
drops weaning those four classes. And if the goal is to draw inference
about the comparative effectiveness of all these interventions for primary open angle glaucoma, a pair wise
meta-analysis won't be very helpful. And in that situation you would
use network meta-analysis, and we will talk about network
meta-analysis in a separate lecture. You may also want to evaluate the
relationship between the size of an effect and some characteristics of the study. For example, is the efficacy of the BCG
vaccine, which is the vaccine used to prevent tuberculosis,
associated with the latitude? And that means, if the study was
conducted in the higher latitude, the BCG vaccine may be more or less effective, comparing to studies
conducted in the lower latitude. And in that type of situation, you will use meta-regression
to look at the association. And again, we will talk about
meta-regression in a separate lecture. There could be other aims for
your analysis, but overall, remember that your analysis plan has
to follow from the aim of the review. And depending on the objective of
the review you will choose different meta-analytical technique. And in the first section of this lecture
we have talked about analysis plan and why it's important to have a plan. And to do qualitative
synthesis before you move onwards to the quantitative synthesis. In the next section,
we will talk about meta-analysis and a brief introduction to it.
Welcome back to the second
section of the lecture. We will talk about meta-analysis and
define what is meta-analysis. Meta-analysis is really an optional
component of a systematic review. Not every single system systematic
review have to have a meta-analysis. And here's a classic definition. Meta-analysis is the statistical analysis
of a large collection of analysis results from individual studies for
the purpose of integrating the findings. Well, then you may ask the question,
who decides which studies to combine? Another alternative definition
is a statistical analysis, which combines the results of
several independent studies considered by the analyst
to be combinable. As a systematic reviewer, or the data
analyst, you have to decide whether it makes since to put a bunch of studies
into analysis or into a meta-analysis, and we will talk about how to make
that judgment in the next few slides. Conventional meta-analysis focus on
pair-wise comparisons of inventions or exposure in outcome relationships. And we will talk about
network meta-analysis and meta-regression in separate lectures. As a systematic reviewer or data analyst,
the most difficult decision to make is to decide which studies
to combine in the meta analysis. Justification for combining results are,
while studies are estimating in whole or in part, a common effect and
studies addressing the same fundamental, biological, clinical or
a mechanistic question. So here's one example. If we're asking the question what is
the affect of interferon therapy on Hepatitis C, well there rarely you
will have two identical studies. The studies you're going to include in
your systematic review usually differ in one way or another. And the size of the interferon
therapy might be higher or lower when the participants are older,
more educated, or healthier than others. Think about one example that maybe
one trial is conducted in Asia. For example, in China,
where the Hepatitis C. Is more prevalent. In another studies
conducted in North America, do you want to combine those two
studies in one meta-analysis, because the underlying or the base line
rates for Hepatitis C is different? And there are different forms of
interferon and different doses and there are different
subtypes of Hepatitis C. You have to make the judgement whether the
differences between the studies are small enough that you feel comfortable enough
to combine them or maybe they are too big that you don't feel comfortable to
throw them into the same analysis. When you do a meta-analysis and
this is a typical figure or we call it forest plot,
showing the results from meta-analysis. You probably have seen this in previous
lectures or in other courses here. And in this particular
analysis we have 5 studies. Kunif 1997, so on and so forth. And each study is represented
by it's point estimate. And the 95 confidence interval. So the square in the middle for
each study is the point estimate. And you will notice the size of
square differ between studies, so the larger study will take a larger, the square will be larger because it
takes more weight in the meta analysis. In the line of no effect or the now value
is in the center, since we are using risk ratio as the measure of association, and
the risk ratio of one is the now value. And then you can label your graph favors the different intervention
on the bottom of the figure. So here the new thing you will notice
is the little diamond in yellow down on the bottom of the graph. So that shows the meta-analytical results. The center of
the diamond lines
where the meta-analytical result is. And the width of it is proportional
to the 95% confidence interval. So here since all the 5
studies are showing more or less very similar effect and the put
estimate lines around the null value. So, in the forest plot or
in the meta analysis. Each study summarized by
an estimator effect or the result. For example, the risk ratio. In the overall measure of effect is
a weighted average of the results of the individual studies. So the overall measure of
effect is the yellow diamond you saw from the previous slide. And the weighted average reflect
varying component of the trial. And trials would desire more weight
if it has more information and more information mainly
refers to the sample size. More information leads
to increased precision. If you still remember
from the previous figure. We have the squares representing
each individual study. The larger square means that
particular study is taking more weight in the meta-analysis. More formally, you can write
the inverse variance weighted average. Using this formula. So let's say we have estimates
from individual studies, and now we're just combining
them as a weighted average. Here, YI refers to
the intervention effect, measures of association or measures
of the effect, estimating the study. And the WI is the weight
given to the ice study. And as we can see from the formula,
the point estimate or where the center of that diamond lies, is just the weighted
average which sums up the estimate times the weight in the the numerator
divided by the sum of the weights. In addition to getting the point
estimate from the meta-analysis, we also want to know the variance. And we use the variance to
construct 95% confidence interval. And the variance for the weighted average
equals to one over the sum of the weights. And we will show how you can use this formula to get the meta-analytical
results in the next slide. Here's one example. Let's say from the first study, if you look at the table,
on the upper left corner. So here we showed you the results
from the first study. Of those treated,
12 experienced the events and 53 did not experience the event. And of those in the comparison group,
16 experienced events and 49 did not have the event. So and we can use the formula odds ratio,
which is the cross-product, the ratio of the cross product to get the
odds ratio from this particular study, and this should not be something new to you
because you have learned this in your and Biostat that course is. By plotting the numbers,
we have an odds ratio of 0.69. Now, we're going to get the YI, the YI of the formula I showed you previously
by taking the log of that odds ratio. So the log of the show equals -.36. And the variance for that YI, again,
by plugging the formula you have seen in other courses, especially the biostat
courses, you can get the variance, which equals .18. And now the weight for the particular
study if you remember from the formula I showed you in the previous slides,
WI equals 1 over the variance. And you get the weight for this particular
study which equals 5.4, so you follow the same steps to get the log (OR1) ratio
as well as the variance and weights for each individual study that you're going
to include in your meta analysis. And now here are all the data you have. Let's say for this particular
meta analysis you have 6 studies. And I just showed you how to get
the odds ratio, log odds ratio, variance in weight for
the very first study. And if you remember the data from
the previous two by two table, their reflected here
in this table as well. So you follow the same steps and
you calculate the odds ratio, log odds ratio variance in weight for
each individual study and the results are showing in
the table on the bottom. If we focus on the variance column
of the table on the bottom, you will see the variance for
the first study is .19. The variance for the second study is .29. And comparing these two studies,
which one is more precise? Well, we know the smaller the variance,
the more precise the study. Which means that study will take
more weight in your meta-analysis. So let's move on to the next
column on the same table. You will see that the weight for
the first study is 5.4, and the weight for the second study is 3.45. And if you look across all 6 studies, you will notice
that study four takes
the largest weight, which is 17. 16. So that's the most precise
study among the 6 and that study,
while dominate to your meta analysis. Which means that will take the largest of
weight, relative to all the other studies. If you sum up all the weight,
that equals 42.25, and then if you do the calculation
times the weight for each individual study with
the log odds ratio and sum them up, you get the summation
of WI times YI which equals -30.59. And the reason I'm emphasizing those two numbers is because you can use
those two numbers and plug them in in the formula I showed you previously
to get a better analytical result. And here is the formula again. So remember, the meta analysis is just a voided
average of individual study results. And there are different
ways to take the weight. In the example I just showed you, we're
using the fixed effect meta-analysis. So the weight equals the inverse of
the variance of the effect estimate. You may have heard random
effects meta-analysis. Or other methods of weighting studies. And the differences between
these analysis, it's just a Wi, the weight that given to the I study,
is slightly different. And for now, let's just focus on
the fixed effect meta-analysis. And by plugging the numbers
from the example I showed you where the summation of YI x WI = -30.59. And the summation of WI = 42.25. You get
the log oz ratio for your meta
analytical results, which equals -.72. If we exponentiate that number we
get the odds ratio which equals .48. Remember, we also talk about how to get
the variance for the pooled or ratio. And the variance equals one over
the summation of the weight and that equals .024. Square root of that number, you get the standard error,
and you can get the standard error plug into the formula to get the 95%
confidence interval for your odds ratio. So these formulas, you probably have used
them in your other other courses, and this is really a quick refresher of
the methods you have already known. After we do all these analyses, and
hopefully you don't have to do it by hand and the software will do it for
you, you'll get a nice forest plot. So, here again we have the 6 studies. Each study is represented by a square,
and the 95% confidence interval. Now we have the summary odds ratio, shown
as the yellow diamond on the bottom, and as I said before, study 4, the lang study,
takes the largest of weight. In the relative weight,
that study takes is 41%. As you can see, that study shows the strongest defect
comparing to all the other 5 studies, so that's why the diamond lies between
that point estimate and all the others. And this is reflected by
the formula you just used. Which is weighted average of all the point
estimates from each individual study. There are different types of software
you can use to do the meta analysis, and they are represented slightly
different in the published papers. Here is one example from a published
systematic review that compare conventional occlusion verses
pharmacologic penalization for amblyopia. And the outcome is mean
difference in visual acuity. Here we have 3 studies,
each is reflected by a dot, and 95%. Well, we don't see the diamond
down at the bottom. And why is that? Because the 3 studies are so
heterogeneous, as shown by the statistical measures of heterogeneity as well as
by the authors' qualitative synthesis. And the authors decided not to combine
the 3 studies in a meta-analysis. So a systematic review does
not have a meta-analysis. But you can still. Use the method or the forest plot to
show the individual study results. This is very important, because I got asked all the time
by students a question that, how many studies do I have to have in
a systematic review to do a meta-analysis? Well, the first answer is,
you don't have to do a meta-analysis. You only do a meta analysis when
the studies are comparable, when the studies are homogeneous, when you feel comfortable that studies are
estimating more or less a similar effect. The results of a meta analysis and
the estimate, as well as the confidence interval as
any other study must be interpreted. In the context or
clinically important effect size. Statistically significant result might
not be clinically important, but result that is not statistically
significant may still be compatible with a clinically important effect. So this is an important note
that I would like to make. Absence of evidence is
not evidence of absence. So be very careful when you're
interpreting the results from meta-analysis. Key value or the statistic of
significance is not the key. But you have to interpret it in
the context of the clinical question. In this section, we have introduced
formerly what is the meta-analysis and as I showed you, formulas to get
the meta-analytical results and meta-analysis is simply a weighted average
of the results from individual studies. And they're typically
representing a forest plot. And we will move on to talk about,
why do a meta-analysis.
Welcome back. We have talked about
what is a meta-analysis. And now we have to ask ourselves
the question, Why Do a Meta-analysis? Well, there are a couple reasons
you want to do a Meta-analysis. Meta-analysis can help you
to increase the power and precision if the studies your putting
to a meta-analysis are homogenous. We will be able to detect effect
as statistically significant with narrower confidence intervals. If the studies are homogeneous. And the meta-analytical
result, which is
the diamond on the bottom of your forest plot, will quantify the effect
size as well as their uncertainty. By doing a meta-analysis, we will be able to reduce problems of
interpretation due to sampling variation. And I will show you examples
of what do I mean by that. And we will be able to look
at all studies together. And asses the homogeneity or
heterogeneity of the results. And quantify between study variation. Meta-analysis can also help you to answer
questions not posed by individual studies. For example,
factors that differ across studies. If you don't look at them
together to begin with, how will you know that
the studies are different? You will be able to look at
the comparative effectiveness of multiple interventions for the same condition. And settle controversies arising
from conflicting studies, as well as generating new hypothesis. Here is a famous example that looks
at intravenous streptokinase for acute myocardia infarction. And the outcome here is
the three month mortality. You will see there are many studies. I believe there are 38 altogether
that have evaluated this question. But by looking at individual studies,
and particularly those studies in blue, those are smaller studies. Most of them have a 95% confidence
interval crossing the risk ratio of one, which is the null value. So many of the early
studies are pretty small. They were not powered to look
at mortality in three month. So, we're still uncertain whether
intravenous streptokinase is effective by looking at those smaller studies. And there are five studies in spring. Those are
the larger studies. And by combining all
these studies together, which is the diamond on the bottom,
you will see that it shows streptokinase is very effective in lowering
the mortality in three month. So meta-analysis helps you increase
the power and precision and quantify the effect size in uncertainty. And this question cannot be answered
by individual smaller studies. This is another example where we can
use meta analysis, or more specifically meta regression to examine factors
that may differ across studies. And here on the plot,
each bubble is one study. And again, the size of the bubble
is proportional to the weight that the particular study is taking. And the example we're using here is
the effectiveness of toothpaste. Whether the effectiveness of toothpaste
depends on the baseline population levels of cavities. So the y-axis of the plot is
the preventable fraction of new cavities, so the larger the fraction The more
effective the toothpaste is. And on the x-axis you see
the baseline cavities. So the question is lets say
the more baseline cavities you have is using toothpaste more effective. And here we can see the regression
line through all these studies which is reflected that there's
a red line on this plot. And what I can tell from this plot is,
it seems to me the more baseline cavities you have the more fraction of new
cavities that the toothpaste can prevent. So you can use meta
regression technique to examine factors that
differ across studies. And answer questions that cannot
be answered by individual study. Because for each individual study, you will only have one
baseline level of cavities. In another example i'm going to
show you is to use meta-analysis or network meta-analysis more specifically, to examine the comparative
effectiveness of medical interventions. I use the example of medical interventions
for our primary open angle glaucoma. As a matter of fact there are four
different classes of drugs you can use and classes of drugs are color coded. Within each class there
are more than one Drug. Okay, so, here we have 11 active drug,
plus the placebo and no treatment. And the size of the circle, each circle
represent one drug and the size of it is proportional to how many patients have
been randomized to that particular drug. When there's a line connecting two dots or
two circles, meaning that those two
treatment have been compared against each other in a randomized
controlled trial setting. So not all drugs have been
compared against each other. For example,
if you focus on the Upper left corner, you will see that Letanoprost has
never been compared to Carteolol. Here is the question that
clinicians need answered. When a patient comes to them, for
a drug to lower their eye pressure. They have to pick from these
four different classes, and from these 11 drugs,
which one to prescribe. Okay? And can we do analysis
to answer that question? The conventional pair-wise meta-analysis
will only compare two drugs at a time. So, that analysis won't be able
to tell you of those 11 drugs, which is the most effective. And then, we have to use network
meta-analysis to answer that question. What is the comparative effectiveness
of multiple interventions for the same condition? Can we rank those interventions? And what is the probability that a
drug
will be ranked as the best, second best, or the third best, or the worst? So, network meta-analysis is
the technique to address that question. And we will talk about formally
in a separate lecture. Systematic review and meta analysis can
also be used to generate new hypothesis. And this is a long quote from a review
published in the Cochran Library. Its not new to us that by the end of scientific paper that the authors
concluded more research is needed. So for example here they say,
well-designed, long term randomized trials
are urgently needed. But don't stop there,
you have to talk about in the new study, what should the authors include. So what should be the design for
the new study? So the authors continue saying, ideally, studies aiming at comparing different
treatment procedures should attempt to standardize all parameters
potentially effecting the outcome. In particular,
factors such as initial lesion size, the patients characteristics, tooth
type and location, the operator's skill, clinical procedures, magnification
devices, instrumentation and materials. So on and so forth. So by doing a systematic review, you will
see the deficiencies in the existing literature or the existing evidence, and you can come up with the recommendation
for the new studies. And the best design for the new study. When not to do a meta-analysis? Well, we have heard of
garbage in,
garbage out. A meta-analysis is only as
good as the studies in it. Let's say all the studies included
in a meta-analysis are biased or have deficiency in the design Hoarding. You're going to get a very narrow
confidence interval around the combination of the biased studies, which is worse
han the biased studies on their own. So, if you're throwing a bunch of
bad studies into a meta-analysis you're going to get a very precise,
but biased result. Beware of reporting biases. As we have heard from other lectures
that not all studies are reported. And the studies with the significant
results are more likely to be reported than studies with negative results. And people have talked about
mixing apples and oranges. Well, it's not very useful for learning about apples, although useful for
learning about the fruit. Again, this goes back to the aim
of your systematic review. How broad is your research question? Are you trying to say this
class of the drug works, or are you trying to say this particular
drug, at this particular dose, for this particular patient group,
is effective? So, there are different questions, and
depending on the scope of your question, you're going to include different studies. They might be similar in one way or
another but in most part,
they will be slightly different. So studies must address the same question,
though the question can and usually must be broader than the question
posed in the individual study. What does a meta-analysis entail? So, to do a meta-analysis you have
to answer all these questions. Which comparisons should be made? Again, this refers back to whether
we're comparing two interventions or we're comparing more than two
interventions for the same condition. Which study results should
be used in each comparison? Are we using mortality of three months, mortality of one year, or
quality of life at 24 months? You have to choose what are your outcomes. For each outcome, what is the best
summary of effect for each comparison? So, are we using risk ratio,
auth ratio, or the difference in means? We have to decide, are the results of
studies similar when each comparison? And that can be done by plotting our
studies together on a forest plot. As well as by examining
the characteristics of the participants, of the interventions, the outcomes, as well as the risk bias of the studies
included in your meta-analysis. And how reliable are those summaries? Again, that refers back to
the risk bias of individual study. In this section we have talked
about why to do a meta analysis and different aims or
objectives of the study questions really determines what type of
meta analysis your going to do. In meta analysis is not the solution for
every single question in a systematic review does not
have to have a meta analysis.
Welcome back to this
section of the lecture, where we will talk about types of data and
effect measures. This is not intended to be a new lecture
on the types of data or effect measures, since you have learned all these
in your Biostats and Epi courses. But I will emphasize on the elements that
are relevant to a systematic review in meta-analysis. What is the effect? Well, in systematic reviews, we may be
looking at various types of effect. Effect of health care interventions
that's the most commonly encountered one. Or, we might be looking at the association
between an exposure and an outcome. Or, we may be looking at prevalence,
or incidence. For now, let's consider comparison
of two interventions or two groups. This will usually be done
using a difference or a ratio. So, for example, if we're comparing a JACC intervention
versus a surgical intervention. And we're talking about randomized control
trials, we will typically use a difference or ratio to quantify the effect
of the intervention. What is the effect? It's often called effect measures for
randomized control trials, or measures of association for
observational studies. It is a way to describe the outcomes
of participants treated with different interventions or exposed to
different levels of risk factors. And to choose the correct or the appropriate type of measures of
association to estimate that quantity. It really depends on the type
of the data you have. Here's list of the types of data we
typically encounter in human research. We may have dichotomous data where you
will only have two category, alive or dead, or event or non-event, experiencing a heart attack,
no heart attack, for example. Or, you may have continuous data where
there's no gap between two levels. For example, blood pressure,
height, weight, vision. You can measure vision in different
ways and I will show you an example. We may have ordinal data where
we will have ordered categories, like a liquor scale for example,
none, mild, moderate, severe. You can use counts for
the infrequent event. For example, number of strokes,
number of adverse events. And we may also have time-to-event data. For example, survival time. Time to cancer
remission,
time to death, for example. For any clinical research, you may encounter all different
types of data in your results. And here is one table from
a research paper, and it summarized their efficacy outcomes for
that particular study. And this study compares two intervention,
PDT alone versus [INAUDIBLE] plus PDT for patients with age related
macular degeneration. If we focus on the first row of the data,
which is visual acuity at 12 month, and the authors presented the visual
acuity in several different ways. In the first way, which is lost of
less than 15 letters from baseline. It is a binary or dichotomous type
of data because a patient can either lost less than 50, or greater than, or
equal to fifteen letters from baseline. So, we will consider that
dichotomous data, and visual acuity can be also
expressed as a continuous data. If you look at the third row of the table,
the number of letters. Remember the vision chart, where they
have letters, and the optometrist will count how many letters you can
read from the vision chart. We can measure the number of letters
as a continuous measurement. So, here, for the PDT along group, that change from baseline was
minus 8.2 plus minus 16.3. That's the standard deviation for
the change from baseline. So, you can look at this table. If we focused on the last dissection
of the table on the bottom which is the repeated PDT, the number of
repeated treatment ranges from one, two, three, four. So, what type of data is that? We will consider it as
the ordering of data. Where there is a ranked number for
these orders of treatment. So for any research paper or
for any research study you may encounter different types of data,
even for the same study. For dichotomous data we can
use risk ratio, odds ratio, risk difference, and
number needed treat to measure. The treatment effect or the association. And dichotomous data are typically
represented in a two by two table. Here we have test intervention,
comparison intervention. And that a and c represent the number of
events for the two intervention groups. And the marginal total a + b is
the total number of participants in the test intervention group. And we use Nt to represent that. And you can also get the
total number in the comparison intervention
group by adding up c and d. And the risk ratio really
is a ratio of the risk on the treatment divided by
the ratio on the control and the risk or proportion or
probability depending on the study design. Is expressed as the a, the number
event divided by the marginal total, which is the total number in
the testing intervention group. So you can use that formula
to get the risk ratio. Odds ratio says it is the ratio
of the odds on the treatment, divided by the odds on the control. Again, you can use the formula
to get the odds ratio. Risk difference is the risk on the
treatment minus the risk on the control. And the number needed to treat is
the inverse of the risk difference. So there are the measure we typically use
for dichotomous data and this is really not new to you, because you have used this
multiple times in different classworks. Some features for the risk ratio. It's easy to interpret and easy to explain, because
you can say
the probability of having the event In the treatment group comparing
to the control group is this. But it's not typically the effect measure
reported in multivariate analysis. For example,
if you run a multiple adjustic regression, you're more likely to get odds ratio
out of it, instead of risk ratio. If there's no event in the control
group or in your denominator for the risk ratio, you cannot
really calculate the risk ratio. And what about odds ratio? Well, it has the best developed
statistical methodology, particularly for For
adjusting for covariance. And you can calculate odds ratio
from some study design that do not allow calculation of absolute risks or
rates. For example, in case control
study where you actually select how many controls to be included
in your study by design. There you cannot calculate
the absolute risk, but you can still use odds ratio to
measure the strength of association. It can be difficult to interpret,
particularly when the baseline rate is above 20%, and I will come back to
this point why that's a problem. Relative ratio and
odds ratio are not the same. Because, if you remember from
the formula I showed you, one is using the marginal
total in the denominator. One is using the number of
null value in the denominator. So there are calculated using different
formula, even for the same 2x2 table, you're going to get two different numbers. So let's say, based on a 2x2 table,
you get the relative risk of .8. And then you will find
a different odds ratio, because odds ratio is calculated
using a different formula. And both relative ratio and odds ratio are entirely valid ways of
describing an intervention effect. Problem may arise if the odds ratio
is misinterpreted as relative risk. So here's the question for you. Based on a two by two table,
if I get a relative risk of 0.8, what would be the odds ratio? Is the odds ration more likely to be
less than 0.8 or greater than 0.8? Well, as a matter of effect, odds ratio is always more
extreme than the relative risk. Meaning a way from the now value of one. So let's say the relative
ratio is point eight. The odds ratio will be less than point
eight away from the now value of one. What's the matter with that? Well if you're interpreting odds
ratio as odds ratio, you are fine. But if you misinterpret odds
ratio as relative risk, then you will overestimate
the intervention effects. So for example,
if the relative risk is 6.8, i can say you know the intervention
reduced the risk by 20%. But, you cannot use the same language to
describe an odds ratio based on the same study, because the odds ratio will be
something less than 0.8, let's say 0.7. And if you're still saying
the risk is reduced by 30% you're overestimating
the intervention effect. But it's not a problem if you
interpret odds ratio as odds ratio. So you would say the odds
is reduced by 30%. What if some studies reported relative
risk and others reported odds ratio? So this is the problem that you will
encounter in your meta-analysis. What to do? Well you can always analyze the studies
by the measure of association or the measures of effect that they use. Here is an example from a previous
student's work where in this particular meta-analysis they look at the risk of
Ischemic Stroke with any Migraine and they did the analysis
by measure of effect. So the first three studies reported
relative risk and they combined the first three studies using relative Risk Astem,
measure of effects. And then they have a set of
study that reported odds ratio. And they did a separate
meta analysis of that. And then they also have studies involving
hazard ratio, and incidence rate ratio. So, the key is that you
analyze the studies by the effect measures
reported in the study. You don't have to combine
them into one diamond. Here in this forest plot,
they actually have four diamonds. One for each of the measure of
the effect reported in the study. Sometimes we will have studies
with zero cell counts. What does it mean? For example, if we're looking at rare
events, let's say adverse event or stroke at 24 months for
Behavior intervention. You may not, for
some intervention groups or some studies,
there may be no event at all. It will be a problem for the analysis. But most meta-analytical software
automatically adds a fixed value, typically 0.5, to all the zero cells. It may bias the study estimates towards no
difference and overestimate the variance. And there are non-fixed
zero-cell corrections available. And if you have this problem in your
particular analysis, come to us. And we will show you how to do it. But if there are no events
in both arms from the study. You should exclude that study
from the meta-analysis or use risk difference instead. So when there are zero-cell counts in your
study, if you're working with ratios, relative risk or odds ratio,
you will have a problem. But you won't have a problem
by using risk difference. For risk difference, it directly related to the number
who will benefit from the therapy. And it's fairly easy to interpret. You can calculate risk
difference from any study design even when there are no
events in either group. So in the previous example where you
have zero events, you can still subtract zero from a number, but you cannot divide
zero using relative risk or odds ratio. It's less often constant, particular when there is substantive
variation in the baseline risk. It's not often reported in the studies and sometimes you cannot even
calculate from the studies. Particularly if multivariate
adjustment has been done. What do I mean by if
the measure is constant? So in this example, hypothetical example,
we have three studies, and the outcome is mortality. And for this first study, the mortality in the treatment group
is 20% In a control group it is 30%. If you subtract the control group
rate from the treatment risk, you get a risk difference of minus 10. So the baseline risk, meaning the mortality rate in the
control
group varies between the three studies. In the first study it's 30%, in the second
study 17%, in the third study 62%. So here we have relatively constant
measure in terms of risk difference, because the risk difference is about 10%,
or minus 10%, from all three studies. However, if you calculate the risk ratio,
or odds ratio, they're quite heterogeneous. So they're different. So when the baseline risk varies, consistency in one
measure of effect
means variability in another. As shown in this example, although you
have quite consistent measures in risk difference, you're having inconsistent
measures in risk ratio, odds ratio. And the inverse is true as well,
if you have constant risk ratio or odds ratio,
the risk difference may not be constant. For continuous data,
there are two measures you can use. One is the main difference, which means the difference in means Well
all studies are using the same scale. For example, for blood pressure we're
all measuring blood pressure in the same scale, then you can take the difference in
means to quantify the treatment effects. But for quality of life measures, for example, you may use different scales or
different instruments. The short form SF36 Ranges from 0 to 100, but the eura quality of
life has a different range. Another example is different
instruments for measuring IQ. Although they are measuring the same
underlying construct of underlying biological phenomenon but
they have a different range. In that situation,
you can use standardized means difference. Which is the difference in
means between groups divided by the standard deviation of
the outcome among the participants. And we can look at the example
in the next two slides. The first example, where we used mean
difference, or difference in means, to quantify the treatment effect for
the continuous data. The comparisons are multifocal
lenses versus single vision lenses. And the outcome is the change in
refractive error from baseline to one year. All these studies use the same
measurements for the outcome, or the same scale. And that's why the author's
decided to use mean difference as a measure of association
to gage the treatment effect. In the next example, which is
the substitution of doctor by nurses in primary care,
the outcome is patient satisfaction. Here if we look at the forest plot on
the left hand side of the forest plot you will see the value from
each individual study. The first study the mean is 77.9 for
the intervention group. And for the control group is 74.05. But if you move onto the third
study the mean is 4.4. And for the intervention group,
and 4.22 for the control group. So, although all these studies
are measuring patient satisfaction, but they're using different instrument or
different scale. The first two studies,
the satisfaction ranges from zero to 100, but for the third study it
ranges from zero to five. So although they are measuring the same
underlying construct, the scale's differ. And in that case the mean difference or
difference in means won't be helpful because the values
are different or the units are different. And we use standardized mean
difference to measure the association. And here the standardized mean difference
0.28 for the three studies combined. So the way you interpret it is it favors the intervention
by .28 standard deviation. So there's an interpretation problem
if you're trying to explain this to a patient. And we would stick to the main
difference as much as possible and only used standardized main difference
When the instrument are different. There are other types of data. For example, counts. And how would you analyze
counts? Well, you can dichotomize counts,
as any events vs. none, or treated as a continuous data for
common events. as rates, for example, using Posal module. But short scales are not often
encountered in your analysis. If you have that problems come and
talk to us and we will help you figure out what would be the most
appropriate way to analyze the data. And lastly, for the time to event data,
depending on how the authors report it and analyze the results,
you can choose to dichotomise it. For example, breaking the time points
into periods, and this requires the status of all patients in the study
to be known at a fixed time point. So for example, if all patients follow for
at least 12 months and the proportion who have developed
event before 12 months is known for both groups, than a two by
two table can be constructed. So that's what I mean by the dichotomies. If all studies have used a hazard ration
or some survival analysis methods to calculate the hazard or
hazard ratio of relative time. You can always meta-analyze
the hazard ratio. I'm going to conclude by
summarizing which measure to use. Again, which measure to use should
convey the necessary clinical meaningful information. You have to think about the measures that
are useful for making clinical decisions, for example if they are talking about
reducing the blood pressure by 20%, 15%. Then a dichotomous data
might be more appropriate than reporting a blood
pressure as a continuous data. And the choice of the summary measure
should be suitable to the study design and statistically appropriate and
convenient to work with. So, in this section we have
reviewed really different types of data and
measures of a association and measures of effect,
depending on the type of data you have. And I hope this is a quick refresher of
what you have learned in other courses. And there will be issues that
are specific to meta-analysis. You can always come to us and
we will help you to figure them out. Thank you. [MUSIC]
[MUSIC] Hi everyone, welcome back to the
systematic review and meta-analysis class. Today we're going to talk about
statistical methods for meta-analysis. In the goals for today, we're going
to describe the fixed effect and random effects models and
the underlying assumptions. So these two types of models are the
commonly used statistical models for meta-analysis. And then we're going to show you
how to compute the summary effect, the diamond down on the forest plot, using
a fixed effect in a random effects model. So to begin with, let's have a quick
review of what is a meta-analysis. Well, meta-analysis is an optional
component of a systematic review. Which means not every single systematic
review would have a meta-analysis, although every single systematic review
would have the qualitative synthesis. There are classic definitions for
meta-analysis, which is the statistical analysis for a
large collection of analysis results from individual studies for
the purpose of integrating the findings. A better alternative definition
is that meta-analysis is the statistical analysis which combines
the results of several independent studies considered by the analyst
to be combinable. So the second part of this definition, considered by the analyst to be
combinable, is very important, because you as a systematic reviewer has
to make the decision whether the studies are similar enough to be put
together in a meta-analysis. Meta-analysis is not that complicated. Let's demystify what is a meta-analysis. So for metaanalysis,
you're including
a set of individual independent studies. And in each study, those individual
studies, the measures of association or the effect estimate of
the treatment effect, or the association,
is usually summarized as a risk ratio, as an odds ratio, mean difference,
prevalence, or regression coefficient. For example, let's say we're doing a study
comparing vitamin D versus placebo for preventing fracture, and then the fracture
event will be a binary outcome. We can summarize that using risk ratio or
odds ratio. And then you will have a set of studies
to be included into your meta-analysis. Each study will have that summary measure. And now what we're doing
a meta-analysis is to estimate the overall measure
effect as an average. There are different ways to
average a bunch of numbers. Arithmetic mean is one way,
but in meta-analysis, we're taking weighted average. So the weight reflects
the varying importance of each study in your meta-analysis. As you can imagine,
we're going to assign more weight if there's more information
in that particular study. And the more information is primarily
affected by your sample size, as well as the number of events
that occurred in that study. So more information will lead to increased
precision in your individual study, and more precision overall will
lead to increased precision in your meta-analysis results. In today's lecture, we're going to introduce two types
of models for meta-analysis. We're going to start with
the fixed effect model, because that's simpler to describe and
to introduce the concept to you, and then we will move on to
the random effects model. Under the fixed effect model, we assume
that all studies, you're including your meta-analysis are measuring the same,
common (true) effect size. Well, what does it mean? It means that, if it's not for random or sampling error all results in
those
individual studies would be identical. And now we are going to denote the true
(unknown) effect size by theta, okay? So basically, the idea is saying,
we observe some effects from each individual study, and now we
have to come up with the summary effect. And how we're going to do it,
we're going to make some assumptions. And under the fixed effect model, we assume that each individual study
estimated a true common effect size. Here is another way to
look at the assumption. In this example, we have three studies. Each circle on the figure represents
the true effect in each study. And again, under the fixed effect model,
we assume all the circles coincide with each other, and we're trying
to estimate that combined true effect. Well, the problem is, if we already
know the true effect size in each study, we don't have to do any research at all, because we just take where
that circle is and go with it. Again, the true effect we denoted use
theta, and in this particular example, the theta is 0.6. So we don't know the true
effect size in each study. What we have from your data
is the observed effect size. And it usually varies from
one study to the next. And under that model, the fixed effect
model, we assume that there's only one source of variation which is
the random errors inherent in each study. If you look at the figure, the second
figure on the plot, where we have the square, the black square to represent
the observed effect size from each study. So, here, that's what you have
actually from your study. Going back to the example,
let's say we're comparing vitamin D versus placebo to prevent fracture,
and you have three studies. You may observe the effect
size of 0.4 in Study 1, effect size of 0.7 in Study 2, and
effect size of 0.5 in Study 3. That's what you actually
have from your data. And the purpose of doing a meta-analysis
is trying to use those observed effect size to make the best guess of where
the common combined true effect size is. Again, here for example,
the observed effect in Study 2 is circled on the figure, and
it's 0.7 in this example. The observed effects size varies
from one study to the next only because of the random
error inherent in each study. Thus, the assumption for
the fixed effect model meta-analysis. And the sampling error, which is
denoted as epsilon in this example, is -0.2, 0.1, and
-0.1 respectively in Study 1, 2, and 3. Another way to look at this data, so for Study 1, the effect size
you observe is actually 0.4. So Y1 would equal 0.4. And how is that derived? It's using the overall mean theta,
0.6 minus 0.2. That's your Y1. And if you look at the second study, the
observed effect size in that study is 0.7. And it can be written as
using the overall theta, 0.6 plus the 0.1, which is epsilon 2. And you can do the same for
the third study. More generally,
you can write this out, right? The observed effect size Yi for any
study It's given by the population mean, plus the sampling error in that study. So again, in a more general way, you can
write Yi equals theta plus the epsilon Y. Then, Epsilon Y is the random arrow, or
the sampling arrow inherent in that study. If you have three studies
in your meta-analysis, the i would be one, two, or three. Just to summarize,
under a fixed effect model, there is only one source of variance. And that is the random errors
inherent in the study. Here, if you look at this figure, we have the distribution of the random
errors inherent in the study. Each normal curve on the figure reflects
the amount of variance in that study. Comparing Study 1, 2, 3,
Which study has the largest variance? Well it's the first study because the
normal distribution has the widest spread. And the study two is the most
precise because the normal curve has a shorter distribution. And under the fixed effect model. We assume all the circles
on the figure
which are the true effect from each study, but you don't see them,
coincide with each other. Okay? So you don't actually see
the true effect from each study. All you see or all you have from your
data is the observed effect sites, which are represented as
the square on this plot. So meta-analysis is basically you have all
these numbers, you get a point estimate from each study, as well as some
variance estimate from that study reflected as the width of the distribution
of the normal curve on this figure. And now you're saying, basically
you're telling your computer model or software program, Well,
I'm going to assume all the true effects in the studies
lie on the same line together. And we're going to use that model to
try to figure out where that theta is. So that's what you're doing
on their fixed effect model. You're making the assumption,
all the circles, the true effect that the all observes true effect in each
study, lying on top of each other. So they're identical basically. And to do a meta-analysis,
you start with your observed effects and try to estimate the population effect
through computing a weighted mean. Okay. So we say you have that estimate
from each individual study, and now we're going to take a weighted mean to
try to figure out where those circles are. And the weight assigned to each study
in a fixed effect meta-analysis equals the inverse of the Wi
inv Vy study variance. That's the first equation on this slide. Wi equals the inverse of the variance for
that study i. Okay? So, now you get the weight for each study, which is the inverse of
the Wi inv Vy study variance. And the weighted mean can be computed
using the second equation on this slide, which equals the summation of Yi, which
is the effect size from that study i. Times the weight i from the study. And you're going to sum cross all
studies including your meta-analysis. That's your numerator. And the denominator basically
is the summation of all weight. So let's say going back to the previous
example where you have three studies, then you will use the effect
size from study one. Times the weight from study one
plus the effect size from study two times the weight from study two plus
the effect size from the third study, the last study,
times the weight of that study. So that's your numerator. You do the same for your denominator. And that's how you
compute
the weighted mean. Getting a weighted mean is not sufficient
because we also want to know how precise is that weighted mean, That's why
you need to get the variance for the summary effect,
which is the third equation on our slide. So the variance for
the summary effect, the weighted mean, equals one over the summation of weight i. Again, if you have three studies,
then your denominator would be weight
1 plus weight 2 plus weight 3. Once you get the variance, the standard error is basically
the square root of your variance. That's the last equation on the slide. So those are the equations,
all you need to know of how to calculate a meta-analysis result under
a fixed effect model and with those numbers, you can derive
the 95 percent confidence interval. Again, the lower and upper limits for the summary effect would be the mean
minus 1.96 times the standard error or the mean plus 1.96 times the standard
error for the upper limit. And you can use again the mean and
variance or the standard error to test for the null hypothesis that the common
true effects on theta is zero. And from there you can get the P value. So the equations on this slides are
standard equations probably you have seen already from your EPI or biostats classes. Now we're going to plug in some
numbers
to the equations I just showed you and see how these play out. So here we have a data set
where each row is the number or the result from one study. Together we have six studies and for each
trial there are treated participants and untreated participants,
the comparison group. And you will have the number of events and
non-events from each group, denoted as A, B, C, D, in this data set. For example, for the first study,
under the treated group, there are 12 events, and 53 non-events. And under the control group,
there are 16 events and 49 non events. Going back to the vitamin D example,
this could be the number of fractures. There are 12 fractures under
those treated by vitamin D, and 16 fractures under those
who are treated by placebo. And that's the the number you're
going to get from each study and altogether we have six of them. Now, once you have these numbers,
you can calculate the measures for the treatment effect. One of the measure is odds ratio. An odds ratio is defined as the
odds
of the event under one group over the odds of event for
the other group. And, if you remember the equation
from your Biostats class, so here the odds ratio
basically equals AD/BC. And again for the first study,
if you plug in the four numbers, 12, 53, 16, and 49,
you'll get an odds ratio 0.69. So that's the first number
on the second table here. And because this is ratio and
we work on the log scale. So you're going to take
the log of that odds ratio and get a minus 0.37 for the first study. And the variance for odds ratio, again, you can go back
to your biostats
materials to figure out the equation. Again, you plug in those 4 numbers,
you get the variance for that study. And remember from the previous slide,
we said under the fixed effect meta- analysis model the weight
equals one over the variance. So for the first study,
if you use 1 divided by 0.19, you're going to get the weight for
that study, which is 5.4, okay? And once you multiply your
weight from that study with the point estimate from that study,
you get your last column. So weight i, weight one,
is 5.4 and Yi is minus 0.37. So you times those two numbersmultiply
those two numbers you're going to get minus 1.97. So, that's what you get
from your first study. And you do the same for
the second to the sixth studies. You're going to fill in
the numbers to this table following this same
procedure in the equations. You don't have to do this by hand, I mean
the easy way to do it would be to do it in an Excel spreadsheet where
you plug in the equations and it will do it automatically for you. Or you'd use statistical software
we're going to introduce later for example using Stata for the analysis. You don't have to do this by hand. The most
important thing on this
slide is the sum of the numbers. So if you look at the last two columns,
and those are the two columns we have to sum
them across studies, to get the summation. And why we need those two numbers? It's because remember the equation
to get the Pooled odds ratio? So for the meta analysis you have to take
the weighted average of those numbers. And the equation says
the log odds ratio for the meta analysis equals the summation Yi
times Wi divided by the summation of Wi. So that's why those two
sums are important. And if you go back to
the previous slide or if you still remember the two
numbers from the previous slide. They're minus 30.59 divided by 42.25. So that's how you get the log
odds ratio of minus 0.72. And if we exponentiate that number we
get the odds ratio on the natural scale. So that's how you get your
meta-analysis results. Remember we said you not only
need the point estimate, you still need some estimation of
the variability of that estimate. So how precise is that odds ratio? And in order to get the variance and
confidence intervals, the variance of that odds ratio
equals one over the summation of Wi. This is not a new equation, you have
seen it on one of the previous slides. Again, we're just plugging
the number from the example and using 1 divided by 42.25,
you get the variance. Square root of the variance is your
standard arrow, and now again plugging the numbers to the equations you're
pretty familiar with by now. You will get the lower and
upper confidence limit for the alteration. Fortunately, we don't have
to do all this by hand, but I do want to show you how the calculation
was done behind the scenes. If you are going to do this
using statistical software, this is what you will get most likely,
which is a forest plot. Here again, each row represent one study. We have to point estimate the odds
ratio from each individual study, as well as the relative
weight that study's taking. So we calculated odds ratio for
the first study, which is 0.69. And the relative weight basically is the
weight the absolute weight of that study divided by the summation of
weights crossed or studies. So the first study takes about 13% of
all weight in a combined estimate. And each square on the right
hand side basically plots where the odds ratio is and the 95%
confidence interval for that study. You will the notice the size
of square differ and it's reflected by the relative weight. So, the fourth study, the Lane study, which takes about 41%
of the total weight, is the largest. So it dominates other studies. That's why it has the larger square. And then the
summary effect is shown
on the bottom with the diamond, and the length of that diamond is
the confidence interval for the results. So here the summary estimate is .46, and it ranges from 0.3 something
to maybe 0.6 or 0.7. And you can get that number
from our previous slide. So, that's it for
a fixed effect meta-analysis. And I think the most important thing you
should take away from the fixed effect model is the assumption. Basically we're assuming all studies
in analysis share a common true effect. And remember all the circles on
the plot that I showed you early on, we are assuming all the true effects
line up on top of each other. And the meta-analytic result is our
estimate of this common effect. In other sense, all the observed
variation reflects the sampling arrow. So the true effects lie on top of each
other and all the differences you actually see from one study to the next
is only because of the sampling error or the random error of each individual study. And the study weights are assigned
proportional to the inverse of within studies variance. So how precisely the estimates are. So the larger the study,
the smaller the variance. So the larger study will contribute more
information to the meta-analysis results. So it will take up more weight
in your pooled estimate. That's a summary of
the fixed effect model.
We discussed the fixed effect
model in the previous section, and now we're going to talk
about random effects model. We're going to see how random effects
model is different from the fixed effect model. And, we're going to contrast those models
throughout this section of the lecture. A quick review. Under a fixed effect model,
we assume that all of the studies are identical, and the true effect size
is exactly the same in all studies. Well, then the question is,
how plausible is that assumption? Is the assumption underlying
a fixed effect model plausible? What do you think? Well, then, you're going to tell me, we
have to look at the studies in trying to figure out whether the studies
are similar or different. Exactly, that's what you do. For example, the magnitude of the impact
of a educational intervention might vary depending on the class size,
the age, and other factors, which are likely
to vary from study to study. So let's think about this for a moment,. Who on earth are going to do a study that is exactly
the same as a study
that has already been done? You may get two identical studies for those drug trials submitted to
regulatory agency for approval. For example, for a drug to be
licensed in the United States, the drug company have to do two identical
trials and submit the results to the FDA. In that case,
you may get two identical studies. But for most part,
what you're going to have, for your systematic review and meta-analysis, are different studies done by different
investigators in different places. We may or may not know for sure whether these characteristics
are actually related to the effect size. For example, you may argue that vitamin
D studies done in the United States are going to be different from
those studies done in Asia. Because some countries may
be more closer to equator, such that they will
have more sun exposure. So their Vitamin D level, the baseline
level in the participants, are different. We have all this knowledge
to help us to evaluate the factors that may or
may not be the same across studies. But for most time, logic suggests
that such factors do exist and will lead to variations to
the magnitude of effect. So what are we looking at? We're looking at
the characteristics of the study. It could be design characteristics or the
characteristics of the study participants for a study setting, for example. And these characteristics may
be related to the fact size. So, that's all we care about. We care about these characteristics
because they may have effect on the effects size we're
going to see in those studies. Here is an example from one of
the class projects back in 2011. So this study look at the association
between the duration of breastfeeding and the risk of childhood obesity. So the hypothesis is that the duration is
associated with the childhood overweight. And here is what the review group found. Our 21 studies were all cohort
studies,
of which eight were in the US, nine in Europe and four in Asia,
Australia or the Middle East. The studies analyzed breastfeeding
duration ranges from as little as 0-16 weeks to as much as
greater than 12 month. And the sample sizes range from
a little over 300 to over 117,000. And the study dropout rates prior to
follow up ranges is from 5% to 52%. Again, since all of these studies
are brought together in a systematic review of meta-analysis,
we are answering a clinical question. Which is, is there any association
between the duration of breastfeeding and the risk of childhood obesity? But each study is different
from another study. And, we are concerned about whether,
for example, the duration for breastfeeding is associated with the
association you are going to see, okay? Here is another example. This example examines the risk of ischemic
stroke in people with migraine headache. And the authors wrote,
as seen in our descriptive tables, there was substantial variation
in the sample sizes and characteristics of the research
subjects across studies. Subjects were joined from registries,
administrative databases, randomized control trial participants,
hospitals, and the community. These various source populations would
be expected to be associated with different baseline risk of stroke. And the studies also varied in mean age of
subjects, which range from 15 to 97 years. But most focused on the 15
to 50 year old range group. Again, the authors are concerned about
the age distribution from these studies. So the younger population group will,
based on our knowledge, have lower risk of stroke, right,
comparing to the older population. And that's why the association you're
going to see between the migraine headache and stroke might be
different from study to study. Careful qualitative synthesis of the data
usually would indicate that there are diversities, clinically and
methodologically. And they may lead to variations in
the magnitude of the effect size. We call this variation underlying
the effects heterogeneity. So heterogeneity refers to
the clinical methodological diversities among a set of studies. And what can we do about heterogeneity? Well, If the
studies are too different,
let's say, we're really comparing apples and oranges,
we don't have to do a meta-analysis. Remember, the first slide, I said, meta-analysis is only an optional
component of a systematic review. If they don't belong
together in a meta-analysis, you don't put them together. You can report the estimates
from those individual studies. Or we could seek to explain
why the studies are different. So let's say, again,
the vitamin D and placebo example. All of the characteristics
could look very similar, but the difference is in the vitamin D dose. Well then, we can do additional analysis,
trying to figure out if the effect size from vitamin
D is associated with the dose. If there are no good explanations,
where we cannot figure out a reasonable reason why these effects are different, we
could allow for it without explaining it. And the way to allow for it is through
a random effects meta-analysis. Under the random effects model, we assume
there's a distribution of true effects. Remember the assumption under
the fixed effect model? We assume all studies are identical and
there's only one true effect. So here, instead,
we assume there's a distribution of them. So again, the circles represent
the true effect in studies. We're using the same example. There are three studies, so
we have three circles, the true effect. And under the random effects model, these three circles no longer
coincide with each other. Now there is a distribution. So if you look at the little normal
curve underneath the circles, that's the distribution of the true
effects size from those three studies. In contrast, if you still remember
the plot from the fixed effect model, all the three circles coincide or
overlap each other. And again, using the same notation,
the observed effect size in Study 3, that Y3 now differs from the true
effect in Study 3 by that epsilon. So the error term within that study. But because the study,
we're assuming the true effect size follow a distribution, there is one more
source of variability, okay? The variability is denoted as vita 3,
here. And instead of having only
the within study arrow, the true effect in Study 3, the Y3, can now be written as the mu plus
the zeta 3 plus the epsilon 3. Again, the Y3, the 0.4 can be
written out as using the mean of the distribution of true effects among
a population of studies plus the zeta 3, which is the difference from
the true effect in Study 3. From the overall true effects plus
the error term in that study. Here is another way to look at it. Here, the Y3 can be written as
the mu plus zeta 3 plus epsilon 3. Now, the distance between
the overall mean, and the observed effect, in any given study,
consists of two distinct parts. The true variation in effect sizes,
which is the zeta i's. And, the sampling arrow
epsilon i's within the study. More generally, the observed effect Yi for any given study can be written as
a grand mean plus the deviation of the study's true effect from the grand
mean and the sampling error in that study. So now, because we have assumed
all the circles, again, going back to the circles which are the
true effect in each individual studies, there's a distribution, okay? So that there are two sources of variance. The first
source, if we just focus one
study, there's within study variance. So the distance from the theta i,
the circle, to the Yi, the square,
depends on the within study variance. So, the variance is the random
errors within that study. That's the within study variance and
we have that. We have exactly the same within study
variance from our fixed effect model. On top of that, there's another level of variance which
is the between-study variance, okay? So the distance form the mu, the triangle,
to each theta i, the circles, depends on the variance of the distribution
of the true effects across studies. And we call that variance tau square. Because all of the circles,
again, the circles on this plot are the true effects in each study
because they don't line up together, they don't coincide with each other,
there's a distribution. And that distribution is
the between-study variance. So under a random effects model, we have
to capture both sources of variance,. The within study variance,
as well as the between-study variance. So here's a nice contrast, of the fixed
effect model and a random effects model. The different assumptions you are making. Again if there's only one thing you
are
going to take away from this section of the lecture, is this slide. The different assumptions underneath,
two different models for meta-analysis. On the left-hand side, all three figures. You have seen all of them and they are
the
assumptions for the fixed effects model. There's only one assumption and
they're showing you three different ways. Under the fixed effect model, we assume all studies share
a common true effect size. That's why all the three circles,
lying on top of each other, so there's only one identical
common effect size. However, under the random effects model, if you look at the figure on the upper
right-hand corner, here's a distribution. The three circles no
longer align together. There's a distribution of effect size,
okay? And because of that distribution,
we added one more level of variability, which is the between-study variance. And that's why you have to capture them
in your analysis using the two
slightly different equations. Under the fixed effect model, Yi equals
theta plus your error term within study. However, under the random-effects model, the Yi equals the mu of
the grand mean plus the zi. That captures the distance of
each circle from that triangle, plus the epsilon i, the error term. Again, the difference is,
under the fixed effect model, there's one source of variance. If you look at the last set of figures,
right, there's only one source of variance, which is captured
by that normal curve for each study. However, under the random effects model,
we actually have one more layer, which is the variance between studies. That's why you have four normal
curves instead of three, which still have that
within-study variance. But however, underneath the last plot, that little curve shows
you the variability. The distribution of the true effect size. That's your between-study variance. Now let's take a pause
for a moment. Well, what are we trying to do? What are you going to
observe from the study? You have three studies, right? And the data you observe are actually,
what? Always the same. You will get an risk ratio estimate, odds
ratio, plus some variance from that study. So, you observe that the amount of
information data you have in your hand will stay the same, regardless
of which model you are trying to use. And the purpose of doing a meta-analysis. You're trying to use your data you have
in
your hands and trying to guess where that center of the distribution or
where that common effect size is. That's what you are trying
to do in meta-analysis. And, we are saying, there are two different ways to get that
number, get your meta-analytical results. Either assume the studies are identical, they're the same then,
we are going to use a fixed-effect model. If we cannot make that assumption, then
we're going to use a random effects model by assuming, well, the studies
are slightly different from one another. We're going to assume the true
effect sizes are not the same, but there's a distribution. That's what you're doing. You're taking the data you have,
you collected from each individual study, and trying to make a best guess
where the common effect is. And that guess depends on how different or
how similar the studies are. If they are identical, then go ahead,
use the fixed effects model. If you can not make that assumption, then you're better off with
the random effects model. That leads us to the second session
of the random effects model, which we are going to
show you how to do it.
So we talked about the assumptions and then conceptually how it different
from a fixed effect model. Now we're going to show you mathematically how you're going to
actually implement that. Performing a random effects meta-analysis,
your goal for the analysis is, we start with the observed effects and
try to estimate the population effect. So I have repeated that idea over and
over again. The observed effects,
regardless of model, they are the same. That's the data you
collected from the study. And the goal is to use a collection of Yi,
which could be risk ratio, odds ratio,
to estimate the overall mean mu. And how we're going to do it? Through meta-analysis. The overall mean is calculated
as a weighted average. Again, it's just a weighted average. And the tricky part for the random
effects model is figuring out the weight. Alright, remember we said
under a fixed effect model, the weight equals the inverse of the
variance, which we're going to carry over that idea over that idea to
the random effects meta analysis. And now the weight is still equal
to the inverse of the variance. But instead,
the variance has two components. One is the within study variance. One is the between study variance. So that's the
difference. The only difference is how you're
going to weight each study. The weight equals the inverse
of the variance, but the variance is modified by
the between study variance. So, we use the tau squared
to modify the weights used to calculate the summary estimate. That's the difference between
the random effects model and the fixed effect model
when you're doing it. So, let's look at these. Again, they're just a set of equations. They look awfully similar to
the equations you have seen previously. However we have stars
to denote the weight. You start with the observed effects and try to estimate the population effect
through computing a weighted mean. And the weight assigned to each study in a
random effects meta-analysis equals to one over the variance. Now we have a little star here for the weight as well as the
variance, which
is different from the fixed effect model. Because we want to modify that variance. Because now the variance has
the weaning study variance for each study I, plus the estimate of
the between study variance tau squared. So that's the difference. So the red circle on this slide
shows you the difference. From this model to the fixed effect model. So here the variance has two parts,
the study variance, plus that tau square, which is an estimate
of the between study variance. And all the other formulas and
equations are the same. You have seen them, but remember to plug
in the correct weight, which is this star, and that means you have accounted for
the between study variance. So the weighted mean equals
the summation yi times wi, divided by the summation of wi. Exactly the same equation you have seen
previously, but the Wi has been modified. And you can do the variance,
and take a square root of that, you get a standard error. And again, you can use the estimate and
the standard error to get the lower and upper limits for the confidence
of limits for the summary effect. You can do a z test to
test the null hypothesis that the center of
the distribution mu is zero. From there you can get
the two sided p-value. So everything on this slide,
you have seen it previously. And the only difference is a little
star placed in each equation, meaning here you have to account for
the between-study variability. Now let's talk about the Tau-squared. So, the [INAUDIBLE] study variance, you
will have that from each individual study. So let's say you get the odds ratio and the confidence interval from study one,
right? That confidence interval captures
the wingding study variants. And you should know how to get from the
confidence interval to the standard arrow for the log odds ratio. You know you have that
winging study variance. So the tricky part, or the difficult part
is to get the between study variance. How we're going to estimate
that between study variance. There are different ways to do it and one
of the most popular one in the literature is called the DerSimonian Laird Method or
the Method of Moment. A different name but the same thing. And here the tau squared,
the between study variance, equals to Q minus degrees of freedom,
divided by C. And again you will see a set of
equations of how to get your Q and how to get your C. But if you reach the equations carefully,
well we know all those numbers, just a set of Wi and Yi. As long as we have the numbers from
the studies we can get the Q and C. And you can plug in the numbers into
this equation to get the Q and C. And a degrees of freedom basically
equals the number of study minus one. Let's say you have six studies for meta analysis,
then the degrees of freedom would be five. So that's it, you have to figure out a way
to get your between study variance and there similarly in late method is one
of the most popular method to do it and here are the equations to do it. Just a caveat,
if the number of studies is more, then the estimate of the tau-squared
will have poor precision. What do I mean? Well you're trying to guess
the distribution with a set of studies. But you have only three studies. So your guess of that distribution
won't be very precise, right? So that's the idea. Using the same example that we
used under the fixed effect model, here the example you have
seen this data already. Here we have data from six studies. We have the treated group,
the comparison group, and the number of events and
non events for each study. Okay? And based on those numbers,
you can calculate the odds ratio and the log odds ratio, which is the effect
size, on the second table in this slide. So we get the odds ratio. We take the log of that number,
we get the effect size. And we have found
the variance within studies, based on the fixed effect model, right? And here, we can take the inverse
of that number, and get the weight. And you can do all this calculation,again,
not by hand hopefully but using some statistical software or
even excel spreadsheet. Here, again you want to focus on
the summation of those numbers as that's what's required for
your equation, okay? So with all those numbers,
we're going to take those numbers and plot them into the formula to get
our estimate of the tau square. Okay. On the left hand side of this slide, those are the equations I showed
previously to get your tau squared. Tau squared equals Q minus
degrees of freedom divided by C, and there are a bunch of
equations to get to that point. But the point is,
from the previous slide with the number, we can plug in those numbers into these
equations and try to get the tau squared. And, we have those numbers
from the previous slide. So the tau squared for this particular
meta-analysis equals 0.173. What is tau squared? Tau squared is an estimate of
the between study variance. Here we have six studies to
estimate that tau square. Now we have the between study variance,
right, which is 0.173. And there's only one between study
variance cause you're using a bunch of studies to calculate that tau square,
so that's the column for your between study variances,
.173 for every single study. And now you have the variance, which is the column to the left
of the circled column. So you have the winning, and
you have the variance between. And now you can get the total variance. So we have the winning study variance
as well as the between study variance. And the column on the right
of the red circled column is basically the total variance. So that's the modified variance from that
study and from there you get the weight. You can calculate the quantity
using W times your effect size. And get all the numbers on
the rest of this slide. So it's easy. I think as long as you understand
there are two components for your total variance, which is the
variance, and the between study variance. And everything else you can
always look up the equations in figuring out how to do it. Fortunately, you don't
have to do it by hand, and the statistical software will do it for
you. But I want you to understand
how these are derived. And if you have a point of reference
if you want to do it by hand, and you will be able to go
back to the equations. Remember that the variance for each study
is now the sum of the variance within studies plus the variance between studies,
okay? Again I just want to
repeat the most important difference between the random effects
model and the fixed effect model and the weight assigned to study one in
the random effects meta analysis. Now equals the variance within
plus the variance between. So the number from
the previous slide is 0.185. That's the variance within Study one. And the between study variance, tau
squared, the estimate of that, is 0.173. Okay so now you're going to
add up those two numbers, and using 1 divided by that number you
get the weight from the study 1. So the new weight or
the modified weight assigned to study 1 under a random effects model, is 2.793. That's the weight. So, going back to the
slide, that's how
we got the weight for the first study. And you can do the same for the second. All the way to the sixth study. And that's
how you get the modified
weight for each individual study. Okay, now it's easy. You have seen this multiple times by now. And the meta analysis,
the meta analytical result, the pulled estimate,
basically is just a weighted mean. And you're going to take the Yi, and use that time your modified
weight with the star. Remember, the star is the difference
between the two models. And you're going to sum
them up across all studies. And that's how you get the weighted mean. And again, you can plug in the number and
get the variance. Get the standard arrow, and then you can
derive your 95% confidence interval for your weighted mean. So here I did it. You can always go back and try to see if
you can get the same
results by plugging those numbers. The odds ratio under
the random effects model. The point odds ratio is .568 and
the lower confidence limit is .355 and the upper confidence limit is .907. So what is a random effects model? Here are
some of the summary points
that you're going to take away from this lecture. Under the random effects model,
we assume the true effects in the studies have been sampled from
a distribution of true effects. So basically, the idea going back to
the slide, where whether the circles from the studies would coincide or line on top
of each other or there's a distribution. That's the difference
between the two models. When they're identical, lying on top of
each other, that's the fixed effect model. When you're assuming there have been
sampled from a distribution of true effects, then that's
a random effects model. And the summary effect in our estimate of
the mean of all relevant true effects, and we can test a null hypothesis which
is the mean of these effects is 0, for a difference or
whether that's 1 for ratio. And the confidence interval for the random
effects estimate indicates our uncertainty about the location of the center of
the random effects distribution. Not its. So this is a difficult concept to grasp at
a glance, but the idea is the confidence interval from your random effects
model tells you how uncertain or how certain you are about the location
of the center of that mean distribution. And our goal is to estimate
the mean of the distribution, taking into account two sources of
variance, the within, and the between. And here I can trust the output from the fixed effect model
versus the random effects model. So on top of the first figure shows you
the results from the fixed effect model, and the port odds ratio is 0.5485
with a 95% confidence interval. And if you remember the force study, the lane study takes up 41% of the weight
based on a fixed effect model. And if you look a the same study, on the random effects model it
takes up only 25% of the weight. So the random effects model
basically assume all these studies have a distribution of effects so you're adding another source of
variance to each individual study. So the idea is the random effects
model are pulling study or shrinking the estimate together. So the random effects model is giving a
little bit of more weight to smaller study or cut a little bit of weight
from the larger study, so it's basically pulling
them towards the center. So under the random effects model,
the poll estimated. Is 0.568 and you have an estimate
of the confidence interval. And I want to go back to the basic idea. So the basic idea is what? We have a bunch of
studies and
in this example six of them. And you have odds ratio
from each individual study. So if you look at the second
column from both figures those are the odds ratio you
get from individual study. Those are the data you collected as
part of your data abstraction for your systematic review. Regardless of which model
you're going to use, those numbers remain the same, because
that's the data from what you observed. And the only differences here is,
now we have the observed data, we're trying to guess or we're trying
to estimate where that diamond is. So the diamond is pulled estimate. We have two diamonds,
one under the fixed effect model and one under the random effects model. But by using these two different models,
the diamond one lies slightly differently on the plot, and it will have a smaller,
a little bit wider confidence interval. As you can imagine, on the random effects model, You're less
certain about the center of the diamond because now you're saying we
have two source of variation. One is the within and one is between. So your data you have has been partitioned to
estimate those two
sources of variation. That's why it's a little bit wider
than the fixed effects model. So again you're using what you observed
to figure out where the diamond is. And you can do it through making
two different assumptions. One is all the studies are identical. That's why I'm getting a very precise
estimate under the fixed effect. Or you are saying the studies
are slightly different from one another. That's usually the case. That's why you're getting a less precise
estimate under the random effects model. So, I have set these multiple times. Under the fixed effect model, you assume
that the true effect
is the same in each study. And that the only reason for variation in estimates between
studies is sampling error. Going back to the previous example, all the differences in odds ratio
you're seeing from the previous example under a fixed effect model is
because of the sampling error. However, under the random effects model,
the model is trying to estimate a main effect about which it is assumed
that the truest study effect varies. So again, the idea is you assume that all
these studies are estimating an effect but there's a variation between study. We have covered in this lecture
what is a fixed effect model and what is a random effects model. Those are the two models you're
going to see in the literature when people are trying to put
studies into a meta-analysis. We haven't talked about which model
to use, whether a fixed or random. And we haven't really talked about
how to quantify the amount of statistical heterogeneity. We said,
based on your qualitative analysis, you're going to see the studies
are slightly different from one another. How can we quantify that statistically? Can we come up with a number to say
how much different they are different? And we're going to talk about how to
explore the sources of heterogeneity, for example, through using meta regression and
sub group analysis. We will cover these topics
in upcoming lectures. And thank you for today,
we have covered a lot and I hope you have learned a lot and
we will be here to address your questions. [MUSIC]
Okay, so the question is how I got into research synthesis. And there are two things that brought me into this field. The
first is that I used to work in a hospital, and I was involved in the clinical trials. And I realized, at some point, that most
of what we were doing was silly in the sense that basically, we were trying to see if P-values were statistically
significant or not. And we would make decisions about drugs and about treatments on the basis of whether or not the Pvalue
is statistically significant. And in fact, that really doesn't tell us very much at all. If the P-value was statistically
significant, then the only thing that we really know is that the treatment has some effect greater than zero. Well, we
don't really know whether that effect is of any substantive importance or clinical importance or none. In meta-analysis,
people were focusing on the signs of the effect which is really what we care about. When we ask if something is
effective, what we really have in mind is whether or not has the clinically important effect. And meta-analysis, by
focusing on the size of the effect, actually addresses the thing that we care about. And that we thought we were
addressing with P-values but in fact, we weren't. I'm very concerned about the fact that people misunderstand statistics
for heterogeneity, because when people ask whether or not the effects are heterogeneous, what they really have in mind
is how much does the effect size vary from study to study. Is it the case that the treatment has pretty much the same
effect in all populations, or is that the effect that various moderately, or is it the case that the size of the treatment effect
varies where it has only a trivial impact in some cases, and a moderate impact in other cases, and a major impact in
other cases? People seem to think that that's addressed by test of significant with heterogeneity, or that is addressed by isquare.
And in fact, it is not i-square, it does not tell you how much the effect size varies. That's a very common
mistake, but it's a mistake nevertheless. And there are statistics that will tell you that. For example, one of them is the
prediction interval that you can actually say that in some populations, the effect is trivial, and in some, it's moderate, and
in some, it's substantial. But we have to make sure that we use the right statistics and interpret them correctly.
[MUSIC] Thank you for taking the Coursera course
on Introduction to Systematic Reviews and Meta-analysis. You have done a lot and
you have learned a lot. Here are a list of
the topics we have covered. We started by introducing to you what is
a systematic review and meta-analysis. Then you learn how to formulate a research
question, and how to search for studies, how to collect
data from included studies. How to assess the risk or
bias of clinical trials. And then how to synthesize all
the information you have gathered and avoid bias in this process. And lastly,
we talked about meta-analysis, and you should be able to read the results
of a meta-analysis by now. You have achieved a lot and you have
learned that systematic review uses pre-specified and rigorous methods
to identify, appraise, and synthesize all evidence on a given topic. As a systematic reviewer,
you should try to minimize bias and errors at each step of
doing a systematic review. The most important step in
doing a systematic review is to frame an answerable question. We talked about how to use PICO,
the population, intervention, comparative intervention, and outcome to
help you to frame an answerable question. You have listened to the lecture
on searching for studies and that process should be as
comprehensive as possible. Because, you want to draw your conclusions on your question based on
the totality of the evidence. You want to identify
everything that is out there, that meet you eligibility criteria. Risk of bias assessment is also important. It helps
determine how
trustworthy the evidence is. You can always conduct
a qualitative synthesis of the studies created in
your systematic reviews. When appropriate, you could design
it to conduct a meta-analysis, which is the qualitative combination
of the results from individual studies. If you would like to continue your journey
in actually doing a systematic review here is how you can get started. The step one is to gather
your research team. Again you need content expert
as well as methods expert. Then you're going to develop your
protocol for your systematic review. And the protocol will outline your
research question, your eligibility criteria, and the methods you're going
to use for doing your systematic review. And then you're going to collect the data. That means you're going
to search the literature, locate eligible studies,
screen them and collect data. As a step four, you will abstract data and appraise risk of bias in
the individual studies. The next step, is to synthesize
the findings, interpret, and assess the overall body of evidence. You will lastly write a report and
when appropriate, or needed, update your systematic review. Our course covered the basics, or the
introductory steps in doing a systematic reviews, but perhaps you will need more
to actually do a systematic review. And the best of ways to get started is
actually being part of a research team in conducting a systematic review. Yet, we hope you will be able to
read a published systematic review critically after taking our course. And here I am going to show you the
example that we started with in our very first lecture and
we're going to read this review together. The review is on early versus late
initiation of epidural for labor. And it was a review published
in the Cochrane Library. What is the research question for
this review? Well the population or the participants are pregnant term women
requesting epidural analgesia in labor. And the intervention and the comparators are early versus
late initiation of epidural. Here, early is defined as cervical
dilation of less than 4 to 5 centimeters. The primary outcomes of the review
include incidence of C-section, incidence of instrumental birth. Duration of first stage of labor and
duration of the second stage of labor. The authors designed and conducted a comprehensive search for
studies for inclusion. And this flow diagram shows you the
results it got from each database, and how they screened the results, and how many
were excluded or included at each stage. For this review, they included nine
studies in the systematic review for qualitative synthesis as well as for
their meta analysis. The next step in doing a systematic
review is to abstract the data and appraise the risk of bias for
each included study. Here each row is one study and
each column is one of the domain on the risk of bias assessment
that the authors use for their review. For example, the first column
is on random sequence generation The second column allocation concealment. And as you can see most of the domains
and
for most of the studies ar rated as at low risk of bias,
shown as green dots on the plot. And for blinding of participants and personnel, they're all
rated as a high risk bias. After we have done this, you may find
that the studies are similar enough, and then you can combine them
together in your meta-analysis. Here is the meta-analysis for
the primary outcome, and the incidence of C-section, again
the authors included nine studies here. And each row represents
the results from one study. Here we have, let's look at the very first
study, for the early epidural group, there were 13 individuals had the event
out of 74 randomized to that group. And for the late epidural group, there are 14 events out of 75
individuals randomized to that group. And the square in the states represent
the point estimate here relative risk, or risk ratio, in the 95%
confidence interval for that study. And the column to the right
hand inside of the now or of the figures of each study you see
the weight that each study's taking. And if you look at the sample size and
the weight, Wong 2009 study takes about 84% of the total
weight in the meta-analysis. And that's because that study has
the largest sample size with many events. If you look down the force plot
you will see the diamond and the point estimate for
the diamond is relative race is 1.02 with a 95% confidence level that
ranges from .96 to 1.08. Because the ninety five percent confidence
interval covers the now value of one, there is no evidence
showing that early vs. Late epidural has any impact
on the incidence of c-section. On the bottom left of the forest plot, you
will find estimates of the heterogeneity. The heterogeneity tau-squared
is less than point zero one, and the chi-squared value is three point one
eight, with a degrees of freedom, eight. And, i-squared value is
almost zero percent, meaning that all these
studies are very homogeneous. This also shows on the forest plot, because every study, if you look at them,
are almost estimating the same effect. So that's why
the heterogeneity is very low. So for this particular outcome,
which is the incidence of C-Section, the authors did combine
them in a meta-analysis. And the combined result is 1.02. That's how you would read the output
from a meta-analysis, and the heterogeneity statistics for
that meta analysis. As we discussed early not all systematic
review needs to have a meta analysis. And now here is an example
where the authors showed you the results
from individual studies. But did not pull them together
in your meta analysis. It's for the same review however this
is the outcome on the duration of first stage. Again, each line and each stick shows
you the results from one study. Here you don't see any diamond,
because the authors felt that the studies are too heterogeneous to pull
them together in a meta-analysis. So even within the same systematic review,
you could do meta-analysis for certain outcomes and decide not to
do meta-analysis for other outcomes. We look at the systematic review on
a clinical question that we started in the first lecture which is early vs
late initiation of epidural for labor. And we talked about how the authors
formulated the research question, how they looked for studies,
how they assessed the risk of bias, and how they pull the study results together. There are many more advanced
topics on systematic reviews and meta-analysis that we did not cover. For example, we didn't discuss
how to investigate heterogeneity. And there are also advanced statistical
methods for doing meta-analysis. You may have heard of meta-regression,
network meta-analysis, individual patient data meta-analysis,
and multivariate meta analysis. We did not get a chance to talk about
how to conduct a systematic review of observational studies or
diagnostic test accuracy studies. And how to report a systematic review
to follow the reporting guidances. These topics have been covered in a lot
of the textbook on systematic reviews. Here we have for
you a few suggested readings. For example, the standards for systematic
reviews, put together by the Institute of Medicine, the Cochrane Handbook for
Systematic Reviews of Interventions. As well more statistical oriented
textbook such as Introduction to Meta-Analysis Methods for
Meta-Analysis in Medical Research and Systematic Review in Healthcare. Thank you again for taking our course on
Introduction to Systematic Review and Meta-Analysis, you have learned a lot on
this topic but there are more to learn. We hope you will continue this journey and
doing your own systematic reviews and integrating systematic
reviews in your practice. Doesn't matter if it's medicine,
public health, or policy, we hope you will base
your decisions on evidence, on the totality of evidence
summarized in systematic reviews. Thank you very much. [MUSIC]
